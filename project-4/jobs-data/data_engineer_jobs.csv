,company,company_address,employment_type,job_category,job_description,job_requirements,job_title,salary_range,salary_type,seniority,url
0,PEOPLE PROFILERS PTE. LTD.,"SHAW TOWERS, 100 BEACH ROAD 189702",Permanent,Information Technology,"Roles & Responsibilities Join a rapidly expanding and reputable company in FinTech industry  Attractive remuneration package Great benefits (work-life balance, fun work environment, health insurance, dental plan) Junior & senior positions available    Position Overview:  Support our team in developing cutting-edge solutions for the FinTech industry, keeping security standards in mind.    Responsibilities:  Building robust batch and streaming data pipeline for production-grade data products/platforms (tools: Hadoop, Spark) Creating web services or APIs to connect analytical stacks to application layers Building and maintaining both cloud and on-premise data infrastructure Data cleaning, & pre-processing (e.g. with images/text) Analyse requirements and deliver suitable solutions Write code according to best practices, and which meets security standards Keep up to date with new technologies ","RequirementsRequirements:  Preferably at least a Bachelor’s in a quantitative discipline (e.g. Computer Science, Engineering, or Mathematics) 8 years related work experience, in building high scalability, low latency systems Good knowledge & hands-on skills in at least 2 of these: Python/Java/Scala Knowledge of machine learning techniques (e.g. Bayesian methods, regression techniques) Good knowledge of algorithm generation & data structures. Knowledge and interest in data mining, machine learning, natural language processing, or information retrieval Prior exposure to NoSQL databases (development). Familiar with SQL and database technologies. Preferably have knowledge/experience in cloud computing technologies (e.g. AWS/Azure) Familiar with Linux environment. Knowledge in use of Hadoop/Spark  All Successful candidates can expect a very competitive remuneration package and a comprehensive range of benefits. Interested applicants may wish to email your resume in a detailed Word format to ruth.gan@peopleprofilers.com. Please include last drawn and expected salaries and notice period. We regret that only shortlisted candidates will be notified.   Gan Huiru  Recruitment Consultant Tel: +65 6594 9897 Fax +65 6835 7890 Address: 100 Beach Road #33-06 Shaw Tower Singapore 189702 Email: ruth.gan@peopleprofilers.com EA License Number: 02C4944 Registration Number: R1768917",Lead Data Engineer (Established FinTech Co / East),"$7,500to$10,000",Monthly,Senior Executive,https://www.mycareersfuture.sg/job/lead-data-engineer-people-profilers-76998aeacd7bf851e1e784fe69b59afb
1,SANDBOX CONSULTING PTE. LTD.,"TRIVEX, 8 BURN ROAD 369977","Permanent, Contract, Full Time",Information Technology,"Roles & Responsibilities Design, build, launch and maintain efficient and reliable large-scale batch and real-time data pipelines with data processing frameworks  Integrate and collate data silos in a manner which is both scalable and compliant  Collaborate with Project Manager, Frontend Developers, UX Designers and Data Analyst to build scalable data-driven products  Responsible for developing backend APIs & working on databases to support the applications  Working in an Agile Environment that practices Continuous Integration and Delivery  Working closely with fellow developers through pair programming and code review process","Requirements Experience and passion for data engineering in a big data environment using Cloud platforms - AWS, GCP or Azure  Experience with building production-grade data pipelines, ETL/ELT data integration  Interested in being the bridge between engineering and analytics  Knowledgeable about system design, data structure and algorithms  Familiar with data modelling, data access, and data storage infrastructure like Data Mart, Data Lake, and Data Warehouse.",Data Engineer,"$5,500to$6,000",Monthly,Professional,https://www.mycareersfuture.sg/job/data-engineer-sandbox-consulting-25654d51fa59534e3ac5d5e283830ea1
2,Company Undisclosed,Unknown,"Contract, Full Time",Information Technology,"Roles & Responsibilities Design, build, launch and maintain efficient and reliable large-scale batch and real-time data pipelines with data processing frameworks Integrate and collate data silos in a manner which is both scalable and compliant Collaborate with Project Manager, Frontend Developers, UX Designers and Data Analyst to build scalable data-driven products Responsible for developing backend APIs & working on databases to support the applications Working in an Agile Environment that practices Continuous Integration and Delivery Working closely with fellow developers through pair programming and code review process ","Requirements Experience and passion for data engineering in a big data environment using Cloud platforms - AWS, GCP or Azure Experience with building production-grade data pipelines, ETL/ELT data integration Interested in being the bridge between engineering and analytics Knowledgeable about system design, data structure and algorithms Familiar with data modelling, data access, and data storage infrastructure like Data Mart, Data Lake, and Data Warehouse.   ",Data Engineer,"$5,000to$7,000",Monthly,Executive,https://www.mycareersfuture.sg/job/data-engineer-e7c30aceb1ee80d8f68c4ee5f5de9e98
3,UPS ASIA GROUP PTE. LTD.,"UPS HOUSE, 22 CHANGI SOUTH AVENUE 2 486064",Permanent,Logistics / Supply Chain,"Roles & ResponsibilitiesSummary The Data Engineer will be responsible for expanding and optimizing the data and data pipeline architecture, data flow and collection for the Data Science team and creating API’s to integrate the models with production systems. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support the data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. Responsibilities:  Create and maintain optimal data pipeline architecture. Assemble large, complex data sets from multiple data sources that meet functional / nonfunctional business requirements. Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and no SQL technologies. Develop data features that will serve as inputs to AI/Machine Learning/OR techniques. Build analytics tools that utilize the data pipeline to provide actionable insights into key business performance metrics. Develop data design based on exploratory data analysis to meet stated business need. Develop procedures to monitor model and production system performance/integrity. Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. Review and create repeatable solutions through written project documentation, process flowcharts, logs, and commented clean code to produce datasets that can be used in analytics and/or predictive modeling. Act as subject matter expert with investigating and evaluating emerging technologies. Articulate potential competitive market benefits of new technologies to senior management. Maintain broad understanding of implementation, integration, and interconnectivity issues with emerging technologies. ","RequirementsSkills and Qualifications  Possess a Bachelor’s Degree or Master’s Degree in Computer Science, Information Systems or related discipline. Minimum 2 years of relevant experience in similar capacity using software/tools for big data, SQL and NoSQL databases and object oriented/object function scripting languages. Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases. Experience building and optimizing ‘big data’ data pipelines, architectures and data sets. Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. Strong analytic skills related to working with structured and unstructured datasets. A successful history of manipulating, processing and extracting value from large disconnected datasets. Possess solid project management and organizational skills. Prior experience in supporting and working with cross-functional teams in a dynamic environment. ",APAC Data Engineer,"$5,256to$7,008",Monthly,Senior Executive,https://www.mycareersfuture.sg/job/apac-data-engineer-ups-asia-group-f570cad2f586ebfa9a8660768a61ee6c
4,DBS BANK LTD.,Unknown,"Permanent, Full Time",Banking and Finance,"Roles & ResponsibilitiesJob Purpose  The Data Engineer will provide big data engineering support to the Institutional Banking Group (IBG) Business Analytics Team in various data science projects. This role’s primary job responsibility is defining the framework and process for preparing data for analytical uses. The right candidate will be one excited by the prospect of designing data engineering solutions from ground up and will support data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. Responsibilities  Create and maintain optimal data pipeline architecture; Assemble large, complex data sets that meet functional / non-functional business requirements; Identify, design, and implement internal process improvements: automating manual processes, Perform ETL/ELT, Data Modelling, Data Profiling, Data Cleansing, Feature Engineering tasks as part of Data Analytics Life Cycle (DALC); Work with stakeholders (data analysts, data scientists, technology support team) to assist with data-related technical issues and support data infrastructure needs; Build processes supporting data transformation, data structures, dependency and workload management ","Requirements Master’s Degree in software Engineering, Computer Science or related fields with minimum 3 years data engineering work experience in big data analytics environment Strong in data engineering skills with big data stack (Hadoop, Spark, Kafka, etc) Strong in transactional SQL, Enterprise Data Warehouse Experience with Graph Database, NoSQL databases Experience with Feature Engineering Experience with Master Data Management Experience with scripting languages: UNIX/Linux Shell, SQL, Python (Pandas, PySpark etc), Scala, R, etc ","AVP  /  Senior Associate, Data Engineer, IBG Digital, Institutional Banking Group (1800044U)","$5,500to$11,000",Monthly,"Manager, Senior Executive",https://www.mycareersfuture.sg/job/avp-senior-associate-data-engineer-ibg-digital-institutional-banking-group-dbs-bank-633ccd170a9d6b8f860c413b264c49ab
5,DBS BANK LTD.,Unknown,"Permanent, Full Time",Banking and Finance,"Roles & ResponsibilitiesJob Purpose The Data Engineer will provide big data engineering support to the Analytic Center of Excellence.  This role’s primary job responsibility is defining the framework and process for preparing data for analytical uses. The right candidate will be excited by the prospect of designing data engineering solutions from ground up and will support data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects.   Responsibilities  Create and maintain optimal data pipeline architecture; Assemble large, complex data sets that meet functional / non-functional business requirements; Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, designing infrastructure for greater scalability, etc. Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using ‘big data’ technologies; Work with stakeholders (data analysts, data scientists, technology support team) to assist with data-related technical issues and support data infrastructure needs; Build processes supporting data transformation, data structures, dependency and workload management ","Requirements Degree in Computer Science with minimum 3 years data engineering work experience in big data analytics environment Excellent data engineering skills with open source big data stack Experience building and optimizing ‘big data’ data pipelines, architectures and data sets Strong analytic skills related to working with unstructured datasets Experience with big data tools: Hadoop, Spark, Kafka, etc Experience with relational SQL and NoSQL databases Experience with object-oriented/object function scripting languages: Python, Scala, etc Familiar with deployment and optimization of open source big data analytic stack on distributed environment Familiar with compiling, deploying and configuring open source data science tools including Python, R, Spark, etc Familiar with deploying analytic projects and data science products to production Excellent programming skills ","AVP  /  Senior Associate, Data Engineer, Analytic Center of Excellence, Transformation Grp (180003G6)","$5,500to$11,000",Monthly,"Manager, Senior Executive",https://www.mycareersfuture.sg/job/avp-senior-associate-data-engineer-analytic-center-excellence-transformation-grp-dbs-bank-72cb5c42cf6e21851cd857f8129b9c46
6,WORKATO PTE. LTD.,71 AYER RAJAH CRESCENT 139951,"Permanent, Full Time","Engineering, Information Technology","Roles & ResponsibilitiesRole We are seeking a talented, self-directed Data Engineer to design, develop, implement, test, document, and operate large-scale, high-volume, high-performance data structures for our business stakeholders. Implement data structures using best practices in data modeling and ETL/ELT processes. Gather business and functional requirements and translate these requirements into robust, scalable, operable solutions that work well within the overall data architecture. Analyze source data systems and drive best practices in source teams. Participate in the full development life cycle, end-to-end, from design, implementation and testing, to documentation, delivery, support, and maintenance. Produce comprehensive, usable dataset documentation and metadata. Evaluate and make decisions around dataset implementations designed and proposed by peer data engineers. Evaluate and make decisions around the use of new or existing software products and tools. Mentor junior data engineers.  The ideal candidate relishes working with data, enjoys the challenge of highly complex technical contexts, and, above all else, is passionate about data and analytics. He/she is an expert with data modeling, ETL design and business intelligence tools and passionately partners with the business to identify strategic opportunities where improvements in data infrastructure creates outsized business impact. He/she is a self-starter, comfortable with ambiguity, able to think big (while paying careful attention to detail) and enjoys working in a fast-paced team. The ideal candidate needs to possess exceptional technical expertise in large scale data warehouse and BI systems with hands-on knowledge on SQL, Distributed/MPP data storage, and AWS services (S3, Redshift, EMR, RDS).   Responsibilities   Design, implement, and support a platform providing ad hoc access to large datasets   Interface with other technology teams to extract, transform, and load data from a wide variety of data sources using SQL   Implement data structures using best practices in data modeling, ETL/ELT processes, and SQL, and Redshift   Build robust and scalable data integration (ETL) pipelines using SQL, Python and Spark   Build and deliver high quality datasets to support business analysis and customer reporting needs   Interface with business customers, gathering requirements and delivering complete data structures  ","Requirements  Bachelor's degree in Computer Science, Computer Engineering, Business Administration, Mathematics or a related field   3+ years of industry experience as a Data Engineer or related specialty (e.g., Business Intelligence Engineer, Data Scientist)   Experience in data modeling, ETL development, and Data warehousing.   Data Warehousing Experience with Oracle, Redshift, Teradata, etc.   Experience providing technical leadership and mentor other engineers for the best practices on the data engineering space   Experience in continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers   Experience building data products incrementally and integrating and managing datasets from multiple sources   Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets     Bonus Points   Experience leveraging Python, R or Matlab to manipulate data and set up automated processes as per business requirements   Experience with Big Data Technologies (Hadoop, Hive, Hbase, Pig, Spark, etc.)   Experience leading large-scale data warehousing and analytics projects, including using AWS technologies – Redshift, S3, EC2, Data-pipeline and other big data technologies   Strong ability to interact, communicate, present and influence within multiple levels of the organization   Track record of manipulating, processing, and extracting value from large datasets   Excellent communication skills to be able to work with business owners to develop and define key business questions and to build data sets that answer those questions   Master's degree  ",Data Engineer,"$2,000to$10,000",Monthly,Executive,https://www.mycareersfuture.sg/job/data-engineer-workato-ee0013cd49ea74c314c0cb339fdc5047
7,OBSERVATIONAL AND PRAGMATIC RESEARCH INSTITUTE PTE. LTD.,"PAYA LEBAR SQUARE, 60 PAYA LEBAR ROAD 409051","Permanent, Full Time","Engineering, Information Technology","Roles & ResponsibilitiesThe Company OPRI is an academic research institution striving to improve the lives of patients through global research. OPRI has been leading the paradigm shift in real world evidence for the past 12 years, by delivering pragmatic clinical trials, disease registries and database research. The Role We are looking for a Data Engineer to work alongside our research, statistical and database teams in the UK, Singapore and Australia (Brisbane). In this position you will gain invaluable experience within an internationally recognised research organisation involved in analysis and dissemination of data from large-scale observational studies and pragmatic randomised controlled trials. The successful candidate will have high attention to detail, strong time management skills, and most importantly experience in the management and engineering of relational databases. Your responsibilities  Design, construct, install, test and maintain data collection and management systems:     Integrate data management technologies and software engineering tools for custom data collection applications Programming knowledge: Employ a variety of languages and tools (e.g. scripting languages) to combine systems together Ensure seamless integration of data across multiple databases       SQL, queries   Building APIs for data consumption Integrating external or new datasets into existing data pipelines Continuously monitoring and testing the system to ensure optimized performance   Build and maintain data collection platforms for specific organisational projects     Set up automated integration processes for Patient Reported Outcomes into various data collection platforms   EMR/EDC integration with Registry Database     Data collected via Registry EDCs to be uploaded into EMRs Data collected via site specific EMRs/EDCs to be uploaded into Registry EDCs    The role is for a permanent full-time position. Salary is dependent on qualifications and experience. Immediate start is available.","RequirementsQualifications  Bachelor degree in Computer Science, Engineering, Maths or equivalent qualification  Required Experience  Strong working knowledge of SQL (Essential) Experience working with large databases  Preferred Experience  Experience of developing and maintaining data dictionaries for databases Knowledge of statistical analysis tools (e.g. R, STATA, SPSS, SAS) Interest and knowledge of epidemiology, public health and clinical research ",Data Engineer,"$3,000to$6,000",Monthly,Junior Executive,https://www.mycareersfuture.sg/job/data-engineer-observational-pragmatic-research-institute-45f497e183cb1bc7d03f42d025b40f60
8,SCHELLDEN GLOBAL PTE. LTD.,"INTERNATIONAL PLAZA, 10 ANSON ROAD 079903",Full Time,Information Technology,"Roles & ResponsibilitiesMassive data: You will source / examine, analyze, engineer data pipelines for gigabytes/terabytes of structured and unstructured data with our platform to create value for customers.  Pushing the limits: This role will be on the cutting edge of our Data / Machine Learning platform. As we push to solve more of our customer challenges, you will be prototyping new features, tools and ideas. Innovate at a very fast pace to maintain our competitive edge.  Linux hacking: You will be masterfully using the command line, including tools like vi/emacs and understanding beyond basics of grep, bash, awk, sed, etc to aggressively dive into data, systems, and compute platforms to get the results you are seeking.  Production deployment: You will be responsible for integration and deployment of the machine learning pipelines into production where your ideas can come to life.  Coordinate and work with cross functional teams, sometimes located at different geo locations.","RequirementsCommercial software engineering: You have 3+ years of professional software development experience with languages and systems such as Java, Python (PySpark), and version control (git), with good analytical & debugging skills.  Big data: You have extensive experience with data analytics and working knowledge of big data infrastructure such as Hadoop Eco System, HDFS, Spark, Google Cloud, Big Query, Data Flow (nice to have). You've routinely built data pipelines with gigabytes/terabytes of data and understand the challenges of manipulating such large datasets.  Data Modeling: Flair for data, schema, data model, PL/SQL, Star & snow flake schema, how to bring efficiency in data modeling for efficient querying data for analysis, understands criticality TDD and develops data validation techniques.  Real Time Systems: Understands evolution of databases for in-memory, NoSQL & indexing technologies along with experience on real-time & stream processing systems like kafka, Storm.  Project management: You demonstrate excellent project and time management skills, exposure to scrum or other agile practices in JIRA.",Big Data Engineer,"$4,000to$8,000",Monthly,Middle Management,https://www.mycareersfuture.sg/job/big-data-engineer-schellden-global-6fdb5be88c9cb16e84d51c2f07f72b40
9,PERX TECHNOLOGIES PTE. LTD.,"THE RIVERWALK, 20 UPPER CIRCULAR ROAD 058416",Permanent,Information Technology,"Roles & ResponsibilitiesWhat You’ll Do:   As a Data Engineer on the Analytics team, you will be the “source of truth” for Perx’s most fundamental data - such as end-customer engagement and client usage data - along with core metrics such as daily (DAU) and monthly active users (MAU). Alongside designing & implementing the plumbing & infrastructure that will power the Analytics frameworks, you will also help lead the company’s decision to use bleeding-edge data technologies and features, working directly with our infrastructure team to integrate them into the services you design at scale. In doing so, you will help empower the Engineering department, tens of co-workers, thousands of marketing analysts and millions of end customers to dream of new insights and new possibilities.   Who You Are: You are a go-getter & dreamer, wanting to join a community of extremely talented, forward-thinking & diverse engineers in the industry & region. You gain happiness in building & scaling resilient, robust, well performing, and end-to-end tested distributed systems that can power the most business-critical applications. You want to learn, work with, and leverage on cutting-edge open-source technologies. The ideal candidate has experience with and/or history of contributions to Python, Hadoop, Spark, Redshift, Cassandra, PostGREs, Ruby (on Rails) or similar technologies. You have experience in distributed systems, database internals, or performance analysis.","RequirementsMS in computer science or a related field OR BS in computer science and 3 years of experience in software engineering. Backend development experience with a solid foundation in data pipelines, distributed systems, large-scale data processing. Experience with DBs like AWS Redshift, PostGREs, MySQL. Experience with ETL and query language. Proficiency with Python, Scala or Java. Experience with Ruby is a plus. Experience with Linux/Unix systems and AWS / cloud environments. Working knowledge of MapReduce, Hadoop, HDFS. Experience with Spark is a big plus!  ",Data Engineer,"$5,000to$8,000",Monthly,Executive,https://www.mycareersfuture.sg/job/data-engineer-perx-technologies-3ab22f9fa73f55baa27382245369a2e5
10,6ESTATES PTE. LTD.,79 AYER RAJAH CRESCENT 139955,Full Time,Information Technology,"Roles & ResponsibilitiesWe are looking for the right individual who has the passion and desire to crack the code for this very hot field in AI and Big Data. 6Estates engineers build tools and solutions that ensure the delivery of high quality software for our stakeholders. For someone who wants to learn and grow, this role provides you the unique opportunity to work along with all the experts of different fields. As a Big Data Engineer, you will work with a team of talents to design, architect, and develop on the big data analytics platform. You will also touch on researching, introducing modern technologies and integrating your amazing innovations and ideas into our production systems.","Requirements1. Minimum Bachelor’s degree in Computer Science and 3+ years of experience in administration/architecture in the field of big data specific to Hadoop, HBase 2. Excellent implementation skills in Java or Scala. 3. Expert knowledge of Linux/Unix programming. 4. Experience in Apache Hadoop, Spark, Zookeeper, Elasticsearch, etc. 5. Strong knowledge of distributed system architecture and implementation. 6. Knowledge of Information Retrieval / NLP / Data Mining is a plus. 7. Fluent in English and Chinese to liaise with Chinese speaking associates",Big Data Engineer,"$4,000to$8,000",Monthly,Professional,https://www.mycareersfuture.sg/job/big-data-engineer-6estates-a22e4c3b90cea5bc9767d75c5f0bb827
11,DBS BANK LTD.,Unknown,"Permanent, Full Time",Banking and Finance,"Roles & Responsibilities Manage the Avaloq Configuration / Release Management tasks and work on the BAU tasks in non-production environments Env Planning (DB, application), Avaloq ICE Streaming, Release calendars preparation and communicate with all stakeholders Definition of Connect Direct (NDM), IBM MQ, Avaloq Tools Upgrade, TWS Definition and Batch support, and FIX Platform interface setup Projects and Enhancement requests. Plan and manage the end-to-end deployment and delivery of IT infrastructure for Bank’s project from deployment planning, setup & testing, pre-production readiness to production cutover. Collaborate with Architecture and Engineering team, Application teams, Infrastructure teams, and service providers in delivering quality IT solutions and services to meet business objectives. Ensure the project meet schedule and within the allocated budget and resources. Adhere to the bank's Project Management, Deployment and Change management process.  Prepare and submit the necessary change requests and requisition forms for the deployment of infrastructure for the project, such as facility request, IP/DNS/Hostname request, SAN requisition, etc. Conduct proper transition from Project to Operations before project closure. Prepare all project documentation such as SOM and status reports, assure report accuracy and timeliness. e.g. Weekly Project Status report, etc. Work collaboratively with technology team and vendors, provide single point of contact and drive resolution of issues that arise in projects. Maintain business partnership with Line of Business (LOBs) and constantly collect and manage user’s demands and applications’ infra requirements. Coordinate with DBA, Solaris, Linux, Windows, Network, ID Mgmt and other Infra admin teams on the system issue related tasks Co-ordinate with various Infra teams to apply the OS / DB / MQ patches  Apply the innovative thinking to automate the manual tasks, provide infrastructure services effortlessly, improve performance and resilience of the systems. ","Requirements The candidate need to have minimum of 5 years IT experience with Avaloq Release Management and infrastructure project delivery. The candidate should have strong infrastructure technical background with hands on Open Systems platform such as Solaris, Linux, virtualization, network, and storage. Moderate information security knowledge Have a good understanding of ITIL processes and project management processes. Development experience in SQL & PL/SQL, preferably in Oracle 11g / 12c environment Must have hands-on experience in IBM MQ series, Connect Direct and SSH such as file transfer tools Good to have knowledge on the Micro services and cloud services A Bachelor’s degree in Computer Science (or equivalent experience) 5 -8 years of development and delivery experience Able to perform Unix / Linux scripting. Monitor and address issues relating to capacity constraints and performance related items. Analyze and perform database performance tuning. Develop and maintain high-performance, scalable utilities to support technology research and data transformation. Contribute to the establishment and maintenance of distributed computing platform / Messaging services Good leadership skills in working with Application teams and service providers in defining and executing infrastructure deployment plan, coordination of all infrastructure required activities, cutover/migration strategy and test plan. Good in documentation, tracking, form submission, raising change request and project status reports.  Should be an effective communicator with good people management skills to handle diverse groups/teams in the project. Should be a proactive self-starter with strong analytical skill, team-player, independent, pro-active, resourceful.  ","AVP / Senior Associate, Lead Development Engineer, Grp Consumer Banking & Big Data Analytics (180002YV","$5,500to$11,000",Monthly,"Manager, Senior Executive",https://www.mycareersfuture.sg/job/avp-senior-associate-lead-development-engineer-grp-consumer-banking-big-data-analytics-180002yv-dbs-bank-1e4f251f52dccafe737359d25ed7c91a
12,DBS BANK LTD.,Unknown,"Permanent, Full Time",Banking and Finance,"Roles & Responsibilities Develop world-class solution/application for your team Be up to date of the market landscape for solution/application insights, direction, vendors, and methods Provides expertise to identify and translate system requirements into software design artefacts Provide input during the business development life cycle Participate in experimentation to assess new solution/application paths Identify challenges to help the development of formalized solution methodologies Contribute to a repository for solution/application artefacts Interface and coordinate tasks with internal and external technical resources. Collaborate to provision estimates, develop overall implementation solution/application plan, and serve as a lead as required, to implement the installation, customization, and integration efforts Actively contribute to the quality assurance for services within the solution/application area Provide relevant and timely project information to senior management Actively contribute to the change in delivery and deployment strategy for all applications to a total replacement for applications at the end of their technology or functionality lifecycle Maintain and monitor all aspects for the proper running of the application Understand the system process flow of the primary business processes. Provide a clear picture of the functionality map and the applications footprint of various applications across the map ","Requirements Solid experience in Java, JavaScript, IBM MQ & JMS, Spring boot, Hibernate, Eclipse, JUnit, Apache, etc. Hands-on Design, Development, Deployment, Configuration & Support of various Integration platform components 10 + years of experience in the Technology field including hands on in both development and support of SOA or Microservices Proven experience in design and development of APIs using API Gateways including Gateway deployment, configuration, policy development, migration, debugging and troubleshooting Working knowledge of Web API, REST, XML, JSON, Security (OAuth, OpenID Connect)  Ability to work with Linux OS to deploy and configure AXWAY gateway and other components  Solid hands-on design and development experience in TIBCO suite of BW, BPM, BE, EMS, Hawk, and Adapters etc.  Track record in providing application and technical assistance on multiple systems and platforms Understanding of XML file format, hashing and encryption, and transfer protocols (SFTP, NDM, etc.) Experience of Cloud based architecture and development Experience in Software delivery frameworks such as agile, waterfall Experience of CI/CD Core Competencies Solid software engineering experience Strong analytical and problem solving skills Technical depth across a number of technologies (languages, OS, Databases) Excellent written and verbal reasoning and communication skills Ability to lead technical solutions end to end Ability to work across organizational boundaries and build networks to deliver solutions ","VP / AVP, Development Engineer,  Group Consumer Banking & Big Data Analytics Tech, T&O (180001ZC)","$6,500to$13,000",Monthly,Manager,https://www.mycareersfuture.sg/job/vp-avp-development-engineer-group-consumer-banking-big-data-analytics-tech-to-dbs-bank-3754b22e68f3171e65628771aa1eef42
13,DBS BANK LTD.,Unknown,"Permanent, Full Time",Banking and Finance,"Roles & ResponsibilitiesJob Purpose The data analyst will provide the big data analytic support to the Analytic Center of Excellence. He will partner with business and project leader to discover, analyse and process the data to develop analytic and data science solutions. This position allows those with strong data analytic skill and theoretical understanding of advanced analytic algorithms but lack of hands on experience in advanced analytics to learn and prepare for the role of data scientist - advanced analytics in the future. Responsibilities   Identify, profile, analyze and present the data discovery output for analytic projects Develop data ingestion pipeline and create the analytic data assets for analytic projects Work with data engineer to enhance the analytic data infrastructure and develop enterprise analytic data mart Perform data wrangling and feature engineering for machine learning Create helper functions to automate frequently encountered wrangling and feature engineering tasks ","Requirements Bachelors/Masters in Computer Science, Statistics, Mathematics and other highly quantitative fields such as bio-informatics Minimum 3 years of industry experience in data analytics working in a big data environment, preferably in financial services industry Highly proficient with data wrangling, analytic, transformation and feature engineering using programming tools such as Spark, Python or R. Excellent knowledge of SQL. Excellent visualization and communication skills. ","AVP  /  Senior Associate, Data Analyst, Analytic Center of Excellence, Transformation Grp (180003G5)","$5,500to$11,000",Monthly,"Manager, Senior Executive",https://www.mycareersfuture.sg/job/avp-senior-associate-data-analyst-analytic-center-excellence-transformation-grp-dbs-bank-b914690e3a0a71debdbac82d7ff29f2b
14,CHARLES & KEITH (SINGAPORE) PTE. LTD.,"CHARLES & KEITH GRP HEADQUARTERS, 6 TAI SENG LINK 534101",Full Time,Information Technology,"Roles & ResponsibilitiesWe are looking for a savvy Software turned Data Engineer to join our growing Data Engineering team. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. He/She will be responsible for designing, expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. Supporting our software developers, data architect, data analysts and data scientists on organisation wide data initiatives, he/she will ensure optimal data delivery that is consistent throughout ongoing projects. Self-directed and comfortable supporting the data needs of multiple teams, systems and products, he/she will be excited by the prospect of optimizing or even re-designing our organisation’s data architecture with architect to support our next generation of products and data initiatives. Roles & Responsibilities  Create and maintain optimal data pipeline architecture Assemble large, complex data sets that meet functional / non-functional business requirements Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, collaborate with infra team to re-designing infrastructure for greater scalability and stability Collaborate with infrastructure team for provisioning required infrastructure for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and ‘big data’ technologies Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data needs Keep our data separated and secure across national boundaries through multiple data centres and AWS regions Create data tools for analytics and data scientist team members that assist them in building and optimizing models which enables us as an innovative industry leader. Work with data and analytics experts to strive for greater functionality in our data systems Build tools from ground up that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics     ","RequirementsWe are looking for a candidate with 5+ years of experience in a Software Engineer role who wants to move into Data Engineering or is currently working as Data Engineer. He/She should have attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field and also have experience using the following software/tools:  Experience with object-oriented/object function scripting languages: Python and/or Java, C++, Scala, etc. Experience with big data tools: Hadoop, Spark, Kafka, NiFi, sqoop, etc. Experience with relational SQL and NoSQL databases. Experience with data pipeline and workflow management tools: Luigi, Airflow, etc. Experience with AWS cloud services: EC2, EMR, Kenesis, Firehose Experience with stream-processing systems: Storm, Spark-Streaming, etc. ​    Additional Requirements  Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with widely used RDBMS Experience building and optimizing ‘big data’ data pipelines, architectures and data sets Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement Strong analytic skills related to working with unstructured datasets. Build processes supporting data transformation, data structures, metadata, dependency and workload management A successful history of manipulating, processing and extracting value from large disconnected datasets Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores Strong project management and organizational skills. Experience supporting and working with cross-functional teams in a dynamic environment   ",DATA ENGINEER,"$5,000to$6,000",Monthly,Executive,https://www.mycareersfuture.sg/job/data-engineer-charles-keith-9f7e01404a2633eb48701a27a119b430
15,MATCHMOVE PAY PTE. LTD.,137 TELOK AYER STREET 068602,Full Time,Banking and Finance,"Roles & ResponsibilitiesAre you the One?  MatchMove Pay, one of the fastest, award-winning Fin-Tech company, is looking for a couple of experienced Data Engineers to work on in-house data warehouse projects and data modelling.  Job Responsibilities :  Build the data warehouse infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources on AWS technologies. Maintain compliance of the data warehouse with MatchMove data architecture policy. Responsible for data modelling and validation of master data with original data sources. Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability Work with stakeholders including the Executive,  Finance, Product and Engineering teams to assist with data-related technical issues and support their data infrastructure needs. 	  ","Requirements Good programming experience in Python Knowledge of information retrieval using SQL, java or C++ Experience with AWS data technologies such as Redshift, Glue, Spectrum, Quicksight. Well versed with data modelling and data warehousing. Working knowledge of message queuing, stream processing, and highly scalable data stores. Knowledge about Apache Spark or Hadoop would be an advantage. Self-starter and committed to working in fast paced environment 	   Culture in MatchMove :   - To work in a fast-moving startup, fun and yet professional environment that recognizes and rewards individual contributions and also team success.  - To work with highly motivated people who are totally focused on winning by combining great teamwork, rapid execution and an uncompromising approach to quality and customer satisfaction.  - We strongly encourage Innovation, Collaboration, Creativity, and Initiative.  - We work in a collaborative environment where you can talk to the CEO anytime!  - Be A Part of the MatchMove Family! Check us out our Facebook page   Personal Data Protection Act : By submitting your application for this job, you are authorizing MatchMove to: a) Collect and use your personal data, and to disclose such data to any third party with whom MatchMove or any of its related corporation has service arrangements, in each case for all purposes in connection with your job application, and employment with MatchMove; and b) Retain your personal data for 1 year for consideration of future job opportunities (where applicable for relevant unsuccessful job applicants).",Data Engineer (Fin-Tech),"$3,500to$5,000",Monthly,"Manager, Senior Executive",https://www.mycareersfuture.sg/job/data-engineer-matchmove-pay-ee36e0dd15c8ef67e0d43751479bbbbf
16,EIRE SYSTEMS SINGAPORE PTE. LTD.,"KEPPEL TOWERS, 10 HOE CHIANG ROAD 089315",Full Time,Information Technology,"Roles & ResponsibilitiesThe role of an Implementation Services Data Security Engineer is to interpret a customer request and then transform that request into a successful project.  This includes project management, solution design, security solution implementation as well as act as Tier 3 escalation technical support when called upon.  The role is required to act as liaisons to engineers in the Client Architecture and Engineering department in ensuring that global engineering standards and guidelines are adhered to.   Responsibilities include: Requirements verification:  Meeting with Business Clients, Systems Integration, and other IT departments to gather technical requirements, business justification for projects Verify technical requirements for large projects Quickly estimate high-level costs and resource requirements  Solution design:  Evaluating technology options and presenting those that best suit customer requirements. Produce the technical design in the format specified by Client standards.  This requires excellent technical writing skills and proficiency with office automation tools.  Many technical designs will be in excess of 100+ pages and integrate various items including Visio Diagrams, Tables, Charts, and links. Ensure Compliance with all Client Standards for Data and Voice Networks. Assist junior engineers with project costing estimates Generally the first to use new solutions provided by Network Technology engineering team    Project Management  Develop a High-level plan at project initialization that will guide the project through the requirements and pre-sales stage Develop a Detailed project plan that will guide the project through the solution design, procurement, and delivery stages. Hold regular meeting with stake holders to track progress and communicate issues, and delegate tasks Provide Project reporting as required by the Client Minimal Grid Guidelines for Complex and Medium Service Requests  Implementation:  Determining implementation time frames and risks - Coordinating and gaining consensus on change windows with change management and other IT teams and LBM’s Submission and tracking of Change Management tickets for implementation Coordinating the installation of equipment and connections with Move/Add/Change team Testing of network connectivity post implementations Ensuring Compliance of all Changes with Client standards Completing the Acceptance into Service (AIS) Process for all projects. Delivering all Projects and Changes “Right the First Time” as most of our SLA for service uptime are 99.999% and greater.  Failure to deliver right the first time has significant financial penalties. Travel to various customer sites within the Asia Pacific theater is required    General Support:  Provide Tier 3 Technical Support during major incidents and troubleshooting assistance for complex long term issues impacting network performance. Provide Technical consultation as requested by the customer Assist with internal and external audit requests   ",RequirementsExperience in the below Technology The primary technology areas covered by the Data Security Engineer are:  Juniper Netscreen Firewalls and Netscreen Security Manager Palo Alto Firewalls and Panorama CM Cisco Routing and Switching (Cisco Nexus) Multicast EIGRP and BGP Routing Protocols Lucent Vital QIP/Run IP (DNS/DHCP); N3K RunIP F5 Load Balancers Bluecoat Proxy Devices Rebasoft Network Access Control  The preferred candidate should have worked in a System Integrator/ large enterprise managed service environment preferably in the financial industry or have similar experience with the banking sector.,Data Security Network Engineer,"$7,000to$9,000",Monthly,Professional,https://www.mycareersfuture.sg/job/data-security-network-engineer-eire-systems-singapore-5ed1b7c270b5037308ec75adb5edd638
17,ALPHATECH BUSINESS SOLUTIONS PTE. LTD.,Unknown,Permanent,Information Technology,Roles & Responsibilities Evaluate and recommend solutions via data analysis regarding issues related to the improvement of product qua;oty and resolving of customer feedback Apply software and programming abilities to manage and analyse data from a variety of sources ,"Requirements   Must know JAVA8 and SPARK Experience in distributed data architecture Have working knowledge of SQL, Python, Airflow Scala, Hadoop, SPARK Good to know CI/CD Experience (Jenkins Github), AWS, Kubernetes, Docker Preferred to have banking domain experience ",Data Quality Engineer,"$6,000to$8,000",Monthly,Senior Executive,https://www.mycareersfuture.sg/job/data-quality-engineer-alphatech-business-solutions-b75cf372b356ef87217a54dfc56506da
18,DBS BANK LTD.,"MARINA BAY FINANCIAL CENTRE, 12 MARINA BOULEVARD 018982",Full Time,Engineering,"Roles & ResponsibilitiesBusiness Function  Group Technology and Operations (T&O) enables and empowers the bank with an efficient, nimble and resilient infrastructure through a strategic focus on productivity, quality & control, technology, people capability and innovation. In Group T&O, we manage the majority of the Bank's operational processes and inspire to delight our business partners through our multiple banking delivery channels.   Key Accountabilities  Manage the development of the internal engineering productivity tools and environments. Providing DevOps architecture implementation and operational support Architecture and planning for cloud deployments (Private and Public cloud);  Be an innovative and hands-on DevOps engineer capable of looking at both the technology and strategy around the platform. Future-proofing the technical environments and ensuring extreme high levels of automation, availability, scalability and resilience.  Responsibilities   Manage the development of the internal engineering productivity tools and environments. Manage processes, automation, best practices, documentation. Development and operation of continuous integration and deployment pipelines. Monitoring automation to effectively detect/predict/prevent issues in the environment and code base. Ability to conduct research into software issues and products as required Working with the latest tools and techniques  Hands-on coding and mentoring, usually in a pair programming environment  Working in highly collaborative teams and building quality environments. Ability to effectively prioritize and execute tasks in a high-pressure, fast paced, global environment Knowledge in lots of different open source technologies and configurations. ","Requirements More than 3 years of experience as a Developer Excellent problem solving skills Excellent communication skills in order to facilitate workshops Strong knowledge and experience in Devops automation, containerisation and orchestration using tools such as Mesos Chef, Ansible, Docker, Jenkins, SonarQube Kubernetes etc. Experience with highly scalable distributed systems Hands on in depth experience in some of the following technologies: Jenkins/Maven/Git/SonarQube/Fortify/Confluence/Jira/Artifactory Cloud Foundry, OpenShift or other PaaS technologies. Public clouds such as AWS, Google Cloud or Azure. Dockers, Garden, Kubernetes, Mesos. Strong understanding of virtualization and networking. Strong understanding of Linux. Familiarity with relational databases, preferably MySQL, NoSQL, MariaDB, PostgreSQL. Experience working with, or an interest in Agile Methodologies, such as Extreme Programming (XP) and Scrum  Knowledge of software best practices, like Test-Driven Development (TDD). ","VP, Lead DevOps Engineer, Group Consumer Banking and Big Data Analytics Technology, T&O (180001X5)","$9,500to$15,000",Monthly,Senior Management,https://www.mycareersfuture.sg/job/vp-lead-devops-engineer-group-consumer-banking-big-data-analytics-technology-to-dbs-bank-2f9b305f1df117c8ae8ee02746fd5a62
19,LAZADA SERVICES SOUTH EAST ASIA PTE. LTD.,"AXA TOWER, 8 SHENTON WAY 068811",Permanent,"Information Technology, Logistics / Supply Chain","Roles & ResponsibilitiesStory of Lazada Group: Launched in 2012, Lazada has grown rapidly to include over 4.900 full-time employees in the region, with eCommerce operations in Indonesia, Malaysia, Philippines, Singapore, Thailand, Vietnam and a sourcing center in Hong Kong that drives cross-border marketplace activities as well as an R&D TechHub in Russia. Revolutionizing the way customers shop in Southeast Asia and perform online transactions across the region, Lazada has reached an online footprint of approximately 9 million unique daily visits to its websites, and the largest Facebook community in Southeast Asia with over 16.5 million fans. Lazada Group owns the biggest and the most efficient technology driven logistics and fulfilment ecosystem in the region – Lazada eLogistics. With 11 own warehouses, 5 sorting centers, 78 last-mile hubs we are ensuring 48 hours delivery of more than 6 million orders every month. Our warehouses cover more than 115 thousands of square miles and it takes less than 2 hours to process every order even during massive sales campaigns. Having our own cross-border operator helps us connect more than 100 million of customers and businesses from all over Asia. Our transportation is driven by our own LEL Express delivery fleet which, together with more than 80 third-party logistics, guarantees high quality 48 hours delivery. All of that would be impossible without sophisticated IT systems, which are being developed and expanded in-house by one of the most experienced and agile tech teams in Southeast Asia! As a Lead Data Engineer in Lazada eLogistics Tech Team, you'll be part of an extremely motivated and experienced group of people. You'll help drive LEL business and be a key contributor. You will also become a mentor for other developers and business members. Does the real-time challenge of dealing with massive datasets (billions of transactions a day) get you excited? If yes, then we would like to speak with you. Lazada eLogistics is using a mix of cutting edge and proven technologies to build new data products that aim to change the E-Logistics landscape. You will be the tech leader of a data engineering team that primarily focuses on productionalizing data pipelines that drive our most critical applications. The tech lead position is the a critical layer that makes sure projects get done. Your daily duties will be:   Drive development of real-time data ingestion pipelines and batch data ingestion pipelines for analysis, machine learning, dashboards, alerts and visualizations.   Drive development of new systems and tools to enable data scientists to consume and analyse data faster and more efficiently.   Design data warehouse and data pipelines ensuring data integrity between systems are maintained.   Architect, build, and launch new data models.   Execute code review of data engineers.   Mentor data engineers in the team.   Convert specs into to working code.   Work and tune data warehousing and data ingest environments.   Script programs and APIs in Python/Go.   Create, monitor and manage low latency ETL and Data pipelines.   Your future benefits will be:    Class ""A” office with the best view on business district of Singapore.   Official employment and relocation coverage.   Medical insurance from the first day.   Comfortable working hours in the office.   Caring and respectful HR team.   Powerful workstations and various software licenses (Mac / Winbook + HD displays to your liking).   Daily snacks, chill-out on Friday and of course high quality coffee.   Personal development system for both specialists and managers.   Choice of hackathons, meetups and other entertainment activities.   Opportunity to become public speaker in technology and take part in industry conferences – for top performers.   Exciting international business travels.   No dress code.  ","Requirements  Minimum 5 years of data engineering experience.   Experience in leading more than 2 people in team.   Expert/Advanced level experience with Python.   Expert Level experience with PostgreSQL, Hadoop.   Deep experience in SQL tuning, tuning ETL solutions, physical optimization of databases.   Experience or understanding of Big Data Platforms.   eCommerce industry knowledge is a plus.   Great experience building production applications in a heterogeneous environment.   Experience with Multithreaded and Concurrent programming.   Creative and nimble with ability to overcome obstacles to solve the hardest problems   Ability to write well-abstracted, reusable code components (TDD / Git / Jenkins / Ansible as plus).   Experience in cloud computing services, relational, and non relational databases for business intelligence and analytics.  ","Vice President, Data Engineering","$13,000to$17,000",Monthly,"Senior Management, Middle Management",https://www.mycareersfuture.sg/job/vice-president-data-engineering-lazada-services-south-east-asia-ebc128c9a6c0b3cbc735091d6728c537
20,Company Undisclosed,Unknown,Full Time,"Engineering, Information Technology, Others","Roles & Responsibilities Being part of the Data Engineering Team to conceptualise, build and maintain the Data Infrastructure for relational as well as Big Data Infrastructure in order to deliver a high performanc Data Environment to process Retail and Shopper Data from various partners. Being responsible for Data Warehousing Design, Data Integration Processes, Database Ressource Planning, Infrastructure Performance, Data Governance and Security Management as well as design, architecture, implementation and documentation of new, scalable ETL processes, pipelines, pathways and dimensional data models. Audit and QA data and processes to ensure data quality and integrity throughout the data ecosystem Work with Data Scientist, Business Consultant and Frontend Developer to build scalable Data Analytics Apps using advanced analytics.     ","Requirements You have 10+ years hands-on experience and a passion for data engineering in a big data envrionment, ETL Processes and related services as well as experience and a very good understanding of AWS Big Data Cloud Infrastructure. You have very good experience in relational Database Technologies (ideally MS SQL) and Big Data Technologies and Database Performance Tuning . You are a highly motivated, challenge-taking Personality with a positive attitude. Team-oriented with an international mindset. You like to work in a dynamic Start-Up Environment and build exciting new Data Products within a motivated team. Fluent in English written and oral and ready to travel. You feel comfortable in developing on your own but also in working with a team of developers   ",Data Engineer (Big Data),"$6,000to$8,000",Monthly,Executive,https://www.mycareersfuture.sg/job/data-engineer-154eb447d815b4ca348b1540b6893368
21,ZALORA SOUTH EAST ASIA PTE. LTD.,"KEPPEL TOWERS, 10 HOE CHIANG ROAD 089315",Full Time,Engineering,"Roles & ResponsibilitiesWe are looking for a Big Data Engineer happy to design, build, maintain and automate big data environments (datalake, etc…) and the associated data to enable the teams to make use of the high volume of data available from our e-commerce activities. You should be proficient in: - Technical background:  Linux Big Data technologies (Redshift, BigQuery, Spark, Glue, Parquet…) Industrialization (Ansible..), Orchestration (Kubernetes…), containers in cloud (Docker, AWS…) Strong experience in resilient architecture (high availability, scalability) Data management: you have experience in integrating and managing large volumes of Data while taking into account performance issues Coding skills: skills in one or more scripting languages (Perl, Ruby…) as well as one or more development languages (Python, Java…)  Soft skills: while being a tech automation enthusiast with a passion for building tools to make developers' lives easier, you also want and know how to share your expertise with other people to empower them. Agile and DevOps approach, with an operational experience as an Ops in a demanding environment. You know what it’s like to manage in production critical systems and you have experience in sharing this knowledge to the teams to enable a “you build it / you run it” mindset.","Requirements BS in Computer Science or related technical discipline or equivalent practical experience 3-4+ years of experience with high-traffic, high volume, high scalable distributed systems and client-server architectures (clustering, partitioning, sharding, etc) Some experience working with Data Scientists and finding solutions for them to work efficiently while manipulating high volume of data and be able to work with them and the teams to bring their algorithms at scale Strong operational experience with AWS, container approaches. ",Data Engineer,"$4,500to$6,000",Monthly,Professional,https://www.mycareersfuture.sg/job/data-engineer-zalora-south-east-asia-149994bf19ff2eef0bba260147f76d2d
22,SMARTSOFT PTE. LTD.,"INTERNATIONAL PLAZA, 10 ANSON ROAD 079903",Full Time,Information Technology,"Roles & Responsibilities Massive data: You will source / examine, analyze, engineer data pipelines for gigabytes/terabytes of structured and unstructured data with our platform to create value for customers. Pushing the limits: This role will be on the cutting edge of our Data / Machine Learning platform. As we push to solve more of our customer challenges, you will be prototyping new features, tools and ideas. Innovate at a very fast pace to maintain our competitive edge. Linux hacking: You will be masterfully using the command line, including tools like vi/emacs and understanding beyond basics of grep, bash, awk, sed, etc to aggressively dive into data, systems, and compute platforms to get the results you are seeking. Production deployment: You will be responsible for integration and deployment of the machine learning pipelines into production where your ideas can come to life. Coordinate and work with cross functional teams, sometimes located at different geo locations. ","Requirements Commercial software engineering: You have 3+ years of professional software development experience with languages and systems such as Java, Python (PySpark), and version control (git), with good analytical & debugging skills. Big data: You have extensive experience with data analytics and working knowledge of big data infrastructure such as Hadoop Eco System, HDFS, Spark, Google Cloud, Big Query, Data Flow (nice to have). You've routinely built data pipelines with gigabytes/terabytes of data and understand the challenges of manipulating such large datasets. Data Modeling: Flair for data, schema, data model, PL/SQL, Star & snow flake schema, how to bring efficiency in data modeling for efficient querying data for analysis, understands criticality TDD and develops data validation techniques. Real Time Systems: Understands evolution of databases for in-memory, NoSQL & indexing technologies along with experience on real-time & stream processing systems like kafka, Storm. Project management: You demonstrate excellent project and time management skills, exposure to scrum or other agile practices in JIRA. ",senior Big data Engineer,"$4,000to$8,000",Monthly,Senior Executive,https://www.mycareersfuture.sg/job/senior-big-data-engineer-smartsoft-89a30bf67f8015341e9fea060c98f0c0
23,DENODO TECHNOLOGIES PTE. LTD.,Unknown,Full Time,Engineering,"Roles & ResponsibilitiesYour Opportunity Denodo is always looking for technical, passionate people to join our Services Engineering team. We want a professional who will travel, consult, develop, train and troubleshoot to enhance our clients’ journey around Data Virtualization. Your mission: to help people realize their full potential through accelerated adoption and productive use of Denodo solutions. In this role you will successfully employ a combination of high technical expertise and client management skills to conduct on-site and off-site consulting, product implementation and solutions development in either short or long-term engagements being critical point of contact for getting things done among Denodo, partners and client teams. Duties & Responsibilities  Obtain and maintain strong knowledge of the Denodo Platform, be able to deliver a superb technical pitch, including overview of our key and advanced features and benefits, services offerings, differentiation, and competitive positioning. Constantly learn new things and maintain an overview of modern technologies. Be able to address a majority of technical questions concerning customization, integration, enterprise architecture and general feature / functionality of our product. Capable of building and/or leading the development of custom deployments based and beyond client’s requirements. Provide timely, prioritized and complete customer-based feedback to Product Management, Sales, Support and/or Development regarding client’s business cases, requirements and issues. Train and engage clients in the product architecture, configuration, and use of the Denodo Platform. Promote knowledge and best practices while managing deliverables and client expectations. Manage client expectations, establish credibility at all levels within the client and build problem-solving partnerships with the client, partners and colleagues. Provide technical consulting, training and support. Develop white papers, presentations, training materials or documentation on related topics ","RequirementsQualifications Required Skills  BS or higher degree in Computer Science. Solid understanding of SQL and good grasp of relational and analytical database management theory and practice. Knowledge in Java software development, especially in the database field. Good knowledge of JDBC, XML and Web Services APIs. Excellent verbal and written communication skills to be able to interact with technical and business counterparts. Active listener. Strong analytical and problem solving abilities. Lots of curiosity. You never stop learning new things. Creativity. We love to be surprised with innovative solutions. Willingness to travel around 50%. Be a team worker with positive attitude.  We Value  Experience working with Java frameworks. Experience working with GIT or other version control systems. Experience working with BigData and/or noSQL environments like Hadoop, mongoDB, ... Experience working with caching approaches and technologies such as JCS. Experience in Windows & Linux (and UNIX) operating systems in server environments. Business software implementation and integration projects (e.g. ETL/Data Warehouse architectures, CEP, BPM). Integration with packaged applications (e.g. relational databases, SAP, Siebel, Oracle Financials, Business Intelligence tools, …). Industry experience in supporting mission critical software components. Experience in attending customer meetings and writing technical documentation. Foreign language skills are a plus.  Additional Information Employment Practices  We are committed to equal employment opportunity. We respect, value and welcome diversity in our workforce. We do not accept resumes from headhunters or suppliers that have not signed a formal fee agreement. Therefore, any resume received from an unapproved supplier will be considered unsolicited, and we will not be obligated to pay a referral fee. ",Data Engineer - SQL  /  Big Data  /  Java,Salary undisclosed,Unknown,Professional,https://www.mycareersfuture.sg/job/data-engineer-sql-big-data-java-denodo-technologies-cfe51cc03bee7d249ecf83fde7a21e8d
24,PALO IT SINGAPORE PTE. LTD.,51B CIRCULAR ROAD 049406,"Permanent, Full Time",Information Technology,"Roles & ResponsibilitiesYour profile & role on the project YOU:  Thrive on challenge. When was the last time you failed? Are curious & always learning. What are you up to right now? Can deal with constant change. When were you last surprised? Have mastered at least one skill of your trade but you’re not defined by it. What can you teach us? Can you wear many hats?  YOU AGAIN: The DevOps Architect will install, maintain, and support an on-premises cloud infrastructure and apply DevOps practices and solutions. The person will also implement cloud-related and DevOps technologies such as AWS/Puppet/Chef/Elk/Azure/Openstack. Other infrastructure related activities such as maintaining the company internal server infrastructure and respond to consultant requests when required will be expected.  Install, maintain, and support on-premises and off-premises cloud stack. Configure, maintain, and support the cloud-related infrastructures. Act as a system administrator on different OSes (e.g. RHEL, Opensolaris, Ubuntu, etc.) and help teams deploy their application and automate their development and releases on the cloud. Ability to develop solutions and self-learn new tools and technologies. Document, and share knowledge on developed DevOps solutions.  STILL YOU:  Unix / Linux / Bash knowledge Very good understanding of cloud computing (e.g. Technologies, Deployment, costing, HA/DR, etc.) Good understanding of DevOps principles (e.g. testing automation, BDD, TDD, Release automation, CI/CD, etc.) 2 years experience with cloud deployment (e.g. Openstack, VMWare, AWS, Azure, Terraform, etc.) 1 year experience with testing automation (e.g. Maven, Selenium, HP QC, LoadRunner) 1 year experience with release automation process (e.g. CA-RA, Jenkins, etc.) 1 year experience with Configuration Management (e.g. Ansible, SaltStack, Puppet/Chef, etc.) 1 year experience with monitoring tools (e.g. ELK, Prometheus, Grafana and Splunk) Experience with developing and implementing processes to handle releases from Development to Operations while respecting internal rules, and offering solutions for rollback) Experience with designing an architecture to implement development-to-production workflows. Knowledge of SRE, Containers, Kubernetes, Openshift is a plus. Good understanding of microservice architecture and DevOps practices that support. Strong RDMS and NoSQL skill in deploying and fine tuning such as MySQL, Oracle, Elasticsearch.  Your role at PALO IT You will be invited to take part in R&D works done within our Practices. You will have the chance to assist or be a speaker at must-attend international IT conferences. You will have the opportunity to write articles for our Blog or specialized press. Genuine ambassador of PALO IT, you will present our offers and take an active role in the development of the company.  Your technical environment # Cloud and DevOps based technologies (AWS/Puppet/Chef/Elk/Azure/Opencloud) # DevOps practices # Linux OS, Shell Scripting, SQL # Agile and scrum environment","Requirements✔     You hold a Bachelor, Master or PhD degree in IT, Information Management and/or Computer Science ✔     You are just graduated or have less than 3 years of working experience ✔     Good knowledge of big data technology landscape and concepts related to distributed storage / computing ✔     Experience with big data frameworks (e.g. Hadoop, Spark) and distributions (Cloudera, Hortonworks, MapR) ✔     Experience with batch & ETL jobs to ingest and process data from multiple data sources ✔     Experience with NoSQL databases (e.g. Cassandra, MongoDB, Neo4J, ElasticSearch) ✔     Experience with querying tools (e.g Hive, Spark SQL, Impala) ✔     Experience or willingness to go in real-time stream processing, using solutions such as Kafka, Flume and/or Spark Streaming ✔     You are passionate about technology and continuous learning comes naturally to you  ",Senior Database Consultant - Big Data Engineer,"$6,000to$12,000",Monthly,Professional,https://www.mycareersfuture.sg/job/senior-database-consultant-big-data-engineer-palo-singapore-c1782f4017dc58c1308f3816b938ef0b
25,AVIVA ASIA PTE LTD,"SUNTEC TOWER FOUR, 6 TEMASEK BOULEVARD 038986",Permanent,Information Technology,"Roles & ResponsibilitiesPURPOSE AND CONTEXT OF THE ROLE Taking leadership in:  Design and develop architecture for data services ecosystem spanning Relational and Big Data technologies Design data models for mission critical and high volume data management, real-time and distributed data process aligning with the business requirements. Work with business units on their analytics initiatives, supply and/or source analytics expertise and resources. Ensure the appropriate technology resourcing and support for the Data Engineering and analytics team.    Lead projects involving high level of coordination among departments and business areas.   OUTCOMES  Produce optimal solutions in the receipt and delivery of data sets to desired destinations Alignment to Data Strategy, Digital Strategy, IT Strategy, Architecture and Transformation roadmap Maintain IT Applications Development Excellence amongst IT Development community through detailed development and training and the conduct of Knowledge transfer for completed tasks Produce Optimal solutions, through detailed Impact Analysis, Technical Solutions, Technical Specifications and provide Leadership in Projects, Change Initiatives and Solution Delivery. Supervise IT teams to produce and complete assigned tasks for Strategic, Mandatory and Tactical Change Request, within budget and within projected Scope of work.   ","RequirementsQUALIFICATIONS  Bachelor degree or above Masters (preferred)  KNOWLEDGE/EXPERIENCE ·A deep understanding of Data Modelling best practices  Good understanding of  tools and use in a commercial environment Preferably of 2-5 years of experience working in Hadoop/ Big Data related field Must possess working experience on Hive, Spark, HBase, Sqoop, Impala, Kafka, Flume, Oozie, MapReduce etc Working experience on ETL tools like Oracle PLSQL, Informatica etc Deep understanding of the Hadoop ecosystem and strong conceptual knowledge in Hadoop architecture components Possess data management, data visualization and statistical analysis experience Self-starter who works with minimal supervision. Ability to work in a team of diverse skill sets Experience working in Agile development process and has good understanding of various phases of Software Development Life Cycle Good interpersonal with excellent communication skills -written and spoken English    SKILLS Required  Data Warehouse Technologies Hadoop Proficiency Proficient in Hive, Spark, HBase, Sqoop, Impala, Kafka, Flume, Oozie, MapReduce etc. Working experience on ETL tools like Oracle PLSQL, Informatica etc data management, data visualization and statistical analysis skills    Desirable  Experience in working in Cloud environment   ","Manager, Data Engineer","$10,000to$12,000",Monthly,Manager,https://www.mycareersfuture.sg/job/manager-data-engineer-aviva-asia-a5f03c8482421ffd8fcfa6757d53a231
26,FINSURGE PTE. LTD.,Unknown,Full Time,Information Technology,"Roles & ResponsibilitiesWe are looking for a Senior Data Engineer who will join the Digital Technology Team as we are at an early developmental stage and planning for considerable growth over the next 12 months. We need an experienced data engineer to design and develop data infrastructure. As we are looking to build the data pipeline from scratch, you will have autonomy and the technical backing from our engineering team in designing,developing and maintaining this infrastructure. Contribute to key data pipeline architecture decisions and lead the implementation of major initiatives Translating business requirements into technical specifications and documentation Developing code standards, ETL architecture standards, and naming conventions Designing, executing and documenting ETL testing plans Optimizing performance of ETL jobs Develop the team’s data capabilities - share knowledge, enforce best practices and encourage data-driven decisions.","RequirementsTechnical background in computer science, data science, machine learning, artificial intelligence, statistics or other quantitative and computational science 5+ years of experience with a proven track record of building scalable and performant data infrastructure, implementing data warehousing and business intelligence projects: MS SQL Server, Oracle, MySQL, PostgreSQL Delivering production ETL jobs using Informatica stack including: Power Center, PowerExchange, Informatica Data Explorer Experience with the following tools is a bonus: Big Data Management, Big Data Streaming, Enterprise Data Integration, Enterprise Data Lake, Enterprise Data Catalog, Customer 360 Experience with insert other Temasek ETL tools e.g. SSIS is a bonus Experience implementing Data Quality (DQ) framework is a bonus: Data profiling DQ validation rules Automatic cleansing 3+ years of experience with Hadoop architecture and components including: Hive Spark Kafka Nifi Sqoop HDFS architecture HBase MapReduce Experience with NoSQL databases is a bonus: MongoDB, Cassandra, etc. Experience building APIs is a bonus Experience with the following languages: SQL Bash HiveQL Scala, SparkSQL, PySpark Python Java",senior data engineer,"$7,000to$8,500",Monthly,Senior Management,https://www.mycareersfuture.sg/job/senior-data-engineer-finsurge-2de8441eb1decb498fb94af1ad4645de
27,LAZADA SOUTH EAST ASIA PTE. LTD.,"AXA TOWER, 8 SHENTON WAY 068811",Permanent,Information Technology,"Roles & ResponsibilitiesTeam Introduction Lazada is the number one online shopping & selling destination in Southeast Asia – present in Indonesia, Malaysia, the Philippines, Singapore, Thailand and Vietnam. Lazada helps more than 80,000 local and international sellers as well as 2,500 brands serve the 560 million consumers in the region through its marketplace platform, supported by a wide range of tailored marketing, data, and service solutions. Lazada offers an excellent customer experience through a wide network of logistics partners and its own first- and last-mile delivery arm. Roles & Responsibilities  Implementing working data projects based on given technical specifications Develop real-time and batch data ingesting and processing pipelines to be used for analysis, machine learning, dashboards, alerts and visualizations. Work closely with partner teams to establish the optimal technical solution to business problems Monitor & manage data pipelines, ensuring accuracy and stability ",Requirements Educational background in Computer Science / Electrical Engineering or similar Understanding of database concepts and distributed processing systems Experience with programming and understanding of basic algorithms Strong command of SQL and database concepts Preferred previous knowledge of Hadoop or NOSQL databases Excellent communication & problem solving skills  ,"Senior Manager, Data Engineer","$7,000to$10,500",Monthly,"Middle Management, Manager",https://www.mycareersfuture.sg/job/senior-manager-data-engineer-lazada-south-east-asia-6ec839622a88378203648025875ad61a
28,ABAKUS (ASIA PACIFIC) PTE. LTD.,Unknown,Permanent,"Banking and Finance, Information Technology","Roles & ResponsibilitiesTo reinvent an industry, you need to build an all-star team. Join Wecash if you want to leverage upon the power of big data and machine learning to develop and promote products that can provide businesses with better credit profiles of customers and underwrite loans between funding sources and consumers. Founded in 2014, we are the first Chinese startup using big data and machine learning to evaluate consumer credit and detect fraud. Our company has raised more than US$200 million in financing over 4 rounds, acquired over 130 million users and have underwritten over Billions of USD loans to transform the lifestyle and credit worthiness of individuals over the past 4 years. We are looking for a stellar technologist to drive our expansion in South Asia. If you can understand complex technology, navigate the fintech industry and thrive under ambiguous objectives, join us as our Senior/Lead Data Engineer for Southeast Asia, and help us grow.","Requirements Experience working on Big Data technologies such as Spark, Redshift etc Good working experience with Kubernetes or containers in cloud such as Docker or AWS Strong experience in resilient architecture (high availability, scalability) Experience in integrating and managing large volumes of Data while taking into account performance issues Skilled in Python, and experienced with packages related to machine learning and data science (e.g. pandas, numpy, matplotlib, scikit-learn) Strong foundation in computer science and software engineering, and ability to deliver and test production level code Bachelor in Computer Science or related technical discipline or equivalent practical experience At least 5 years of experience with highly scalable distributed systems and client-server architectures (clustering, partitioning, sharding, etc) Experience working with Data Scientists and finding solutions for them to work efficiently while manipulating high volume of data and be able to work with them and the teams to bring their algorithms at scale Passion in building tools, empower developers practicing Agile and DevOps approach, with operational experience in a fast pace and demanding environment.  We're building an amazing team and are constantly on the lookout for like-minded and motivated individuals to join our fast-paced and challenging environment, so as to contribute to our mission to improve financial inclusion and change the lives of Southeast consumers.",Data Engineer,"$4,000to$6,500",Monthly,"Executive, Senior Executive",https://www.mycareersfuture.sg/job/data-engineer-abakus-04089871f96f4fc0f3c88cdbe4930430
29,HELIX LEISURE PTE. LTD.,"YELLOW PAGES BUILDING, 1 LORONG 2 TOA PAYOH 319637",Permanent,Information Technology,"Roles & ResponsibilitiesHelix Leisure is a leading global supplier to the Out of Home Entertainment industry – locations outside the home people visit for entertainment and recreation. Across our core brands – Embed (revenue management systems, e-commerce), Booking Boss (Tours, Attractions and Activities), LAI Games (arcade games), The Locker Network (operating electronic lockers) and Matahari Leisure (equipment manufacturing) we service over 2,500 locations around the globe. Helix operates full service offices in Singapore, Perth, Sydney, Dallas, Dubai and Jakarta. The group enables our customers to create rich experiences for their visitors and guests through both technology and service. As we embark on building the next generation platform for our software – a core consumer, supplier and distributor facing application, we are looking for highly motivated professionals who enjoy working in a fast paced, agile development environment. You will be working closely with product owners and UX designers to create and develop best-in-class data service solutions with the ability to use the latest in web development technology. Responsibilities:  Design, develop, test, deploy, maintain and improve software Manage individual project priorities, deadlines and deliverables. ","RequirementsSkills & Qualifications:  BS degree in Computer Science preferred, similar technical field of study or equivalent practical experience will be considered Strong Core Java Development Experience Experience working with three or more from the following: web application development, Unix/Linux environments, distributed and parallel systems, machine learning, information retrieval, natural language processing, networking, developing large software systems, and/or security software development Working proficiency and communication skills in verbal and written English Strong TSQL knowledge, able to optimise queries and understand unoptimised query plans Real Time, Multithreaded experience Experience with ORM tools such as hibernate Experience building high-volume file processing systems Experience with payments/transactions Experience in an Agile development environment. DBA experience Effective teamwork and good communication skills with the ability to mentor peers and provide peer code-reviews Ability to work effectively with software engineers to enhance test plans and automated testing framework ","Software Engineer, Data Services","$5,000to$10,000",Monthly,Senior Executive,https://www.mycareersfuture.sg/job/software-engineer-data-services-helix-leisure-877f6be3b659dddce49f867141e49e8c
30,A*STAR RESEARCH ENTITIES,"UOB PLAZA, 80 RAFFLES PLACE 048624","Contract, Full Time",Sciences / Laboratory / R&D,"Roles & ResponsibilitiesAbout the Institute for Infocomm Research (I²R) The Institute for Infocomm Research (I²R) is a member of the Agency for Science, Technology and Research (A*STAR) family and is Singapore’s largest ICT research institute. Our strategic thrusts are in the spheres of intelligence, communications and media and our research capabilities are in shared sensor networks, public-public/public-private data-sharing platform, big data analytics and visualization solutions. For more information about I2R, please visit www.i2r.a-star.edu.sg We are looking for highly-motivated and skilled data engineer to work on a new initiative on developing advanced automatic data pre-processing techniques for facilitating key data analytics applications in various industry domains, including advanced manufacturing and engineering, financial services, healthcare, and urban development. Successful candidate will work with a team of data scientists and data engineers to develop novel methodology for automatic data integrity check, data imputation, and segmentation for structured data and time-series data commonly generated by industry companies. Successful candidate will have the opportunity to tap into a large pool of industrial data from different disciplines and to interact with the companies to understand their real data needs. Successful candidate will also engage in project scoping with industry companies to develop solution to address the data analytics needs.  ","Requirements Minimum Bachelor degree in the field of computer science, computer engineering, mathematics and statistics, electrical engineering, or other data science intensive program. With expertise in at least one of the following areas: data mining and management, machine learning, statistical learning, time-series analytics Possess minimum 1 year of relevant work experience Ability to work independently to translate research ideas into programs with efficient coding Basic knowledge on data analytics, machine learning, data mining Proficient in Python, R, C++ or Java Prior industry experience with engineering, financial services, healthcare, or urban development is a plus Able to deliver under tight schedule Good team player with both research and engineering ethics Good interpersonal and communication skills  The above eligibility criteria are not exhaustive. A*STAR may include additional selection criteria based on its prevailing recruitment policies. These policies may be amended from time to time without notice. We regret that only shortlisted candidates will be notified.",Research Engineer (Data Analytics)  /  I2R (A*STAR),"$2,500to$5,000",Monthly,Professional,https://www.mycareersfuture.sg/job/research-engineer-i2r-astar-research-entities-0a23bad0c7261de1a5faa7d8e5196ad9
31,A*STAR RESEARCH ENTITIES,"UOB PLAZA, 80 RAFFLES PLACE 048624","Contract, Full Time",Sciences / Laboratory / R&D,"Roles & ResponsibilitiesAbout the Institute for Infocomm Research (I²R) The Institute for Infocomm Research (I²R) is a member of the Agency for Science, Technology and Research (A*STAR) family and is Singapore’s largest ICT research institute. Our strategic thrusts are in the spheres of intelligence, communications and media and our research capabilities are in shared sensor networks, public-public/public-private data-sharing platform, big data analytics and visualization solutions. For more information about I2R, please visit www.i2r.a-star.edu.sg We are looking for highly-motivated and skilled data engineer to work on a new initiative on developing advanced automatic data pre-processing techniques for facilitating key data analytics applications in various industry domains, including advanced manufacturing and engineering, financial services, healthcare, and urban development. Successful candidate will work with a team of data scientists and data engineers to develop novel methodology for automatic data integrity check, data imputation, and segmentation for structured data and time-series data commonly generated by industry companies. Successful candidate will have the opportunity to tap into a large pool of industrial data from different disciplines and to interact with the companies to understand their real data needs. Successful candidate will also engage in project scoping with industry companies to develop solution to address the data analytics needs.  ","Requirements Minimum Bachelor degree in the field of computer science, computer engineering, mathematics and statistics, electrical engineering, or other data science intensive program. With expertise in at least one of the following areas: data mining and management, machine learning, statistical learning, time-series analytics Possess minimum 1 year of relevant work experience Ability to work independently to translate research ideas into programs with efficient coding Basic knowledge on data analytics, machine learning, data mining Proficient in Python, R, C++ or Java Prior industry experience with engineering, financial services, healthcare, or urban development is a plus Able to deliver under tight schedule Good team player with both research and engineering ethics Good interpersonal and communication skills  The above eligibility criteria are not exhaustive. A*STAR may include additional selection criteria based on its prevailing recruitment policies. These policies may be amended from time to time without notice. We regret that only shortlisted candidates will be notified.",Research Engineer (Data Analytics)  /  I2R (A*STAR),"$2,500to$5,000",Monthly,Professional,https://www.mycareersfuture.sg/job/research-engineer-i2r-astar-research-entities-890ee3257c7013bc35dfd3c0affdd907
32,A*STAR RESEARCH ENTITIES,"UOB PLAZA, 80 RAFFLES PLACE 048624","Contract, Full Time",Sciences / Laboratory / R&D,"Roles & ResponsibilitiesAbout the Institute for Infocomm Research (I²R) The Institute for Infocomm Research (I²R) is a member of the Agency for Science, Technology and Research (A*STAR) family and is Singapore’s largest ICT research institute. Our strategic thrusts are in the spheres of intelligence, communications and media and our research capabilities are in shared sensor networks, public-public/public-private data-sharing platform, big data analytics and visualization solutions. For more information about I2R, please visit www.i2r.a-star.edu.sg We are looking for highly-motivated and skilled data engineer to work on a new initiative on developing advanced automatic data pre-processing techniques for facilitating key data analytics applications in various industry domains, including advanced manufacturing and engineering, financial services, healthcare, and urban development. Successful candidate will work with a team of data scientists and data engineers to develop novel methodology for automatic data integrity check, data imputation, and segmentation for structured data and time-series data commonly generated by industry companies. Successful candidate will have the opportunity to tap into a large pool of industrial data from different disciplines and to interact with the companies to understand their real data needs. Successful candidate will also engage in project scoping with industry companies to develop solution to address the data analytics needs.  ","Requirements Minimum Bachelor degree in the field of computer science, computer engineering, mathematics and statistics, electrical engineering, or other data science intensive program. With expertise in at least one of the following areas: data mining and management, machine learning, statistical learning, time-series analytics Possess minimum 1 year of relevant work experience Ability to work independently to translate research ideas into programs with efficient coding Basic knowledge on data analytics, machine learning, data mining Proficient in Python, R, C++ or Java Prior industry experience with engineering, financial services, healthcare, or urban development is a plus Able to deliver under tight schedule Good team player with both research and engineering ethics Good interpersonal and communication skills  The above eligibility criteria are not exhaustive. A*STAR may include additional selection criteria based on its prevailing recruitment policies. These policies may be amended from time to time without notice. We regret that only shortlisted candidates will be notified.",Research Engineer (Data Analytics)  /  I2R (A*STAR),"$2,500to$5,000",Monthly,Professional,https://www.mycareersfuture.sg/job/research-engineer-i2r-astar-research-entities-7498a25a005b0c536731e67949a82865
33,A*STAR RESEARCH ENTITIES,"UOB PLAZA, 80 RAFFLES PLACE 048624","Contract, Full Time",Sciences / Laboratory / R&D,"Roles & ResponsibilitiesAbout the Institute for Infocomm Research (I²R) The Institute for Infocomm Research (I²R) is a member of the Agency for Science, Technology and Research (A*STAR) family and is Singapore’s largest ICT research institute. Our strategic thrusts are in the spheres of intelligence, communications and media and our research capabilities are in shared sensor networks, public-public/public-private data-sharing platform, big data analytics and visualization solutions. For more information about I2R, please visit www.i2r.a-star.edu.sg We are looking for highly-motivated and skilled data engineer to work on a new initiative on developing advanced automatic data pre-processing techniques for facilitating key data analytics applications in various industry domains, including advanced manufacturing and engineering, financial services, healthcare, and urban development. Successful candidate will work with a team of data scientists and data engineers to develop novel methodology for automatic data integrity check, data imputation, and segmentation for structured data and time-series data commonly generated by industry companies. Successful candidate will have the opportunity to tap into a large pool of industrial data from different disciplines and to interact with the companies to understand their real data needs. Successful candidate will also engage in project scoping with industry companies to develop solution to address the data analytics needs.  ","Requirements Minimum Bachelor degree in the field of computer science, computer engineering, mathematics and statistics, electrical engineering, or other data science intensive program. With expertise in at least one of the following areas: data mining and management, machine learning, statistical learning, time-series analytics Possess minimum 1 year of relevant work experience Ability to work independently to translate research ideas into programs with efficient coding Basic knowledge on data analytics, machine learning, data mining Proficient in Python, R, C++ or Java Prior industry experience with engineering, financial services, healthcare, or urban development is a plus Able to deliver under tight schedule Good team player with both research and engineering ethics Good interpersonal and communication skills  The above eligibility criteria are not exhaustive. A*STAR may include additional selection criteria based on its prevailing recruitment policies. These policies may be amended from time to time without notice. We regret that only shortlisted candidates will be notified.",Research Engineer (Data Analytics)  /  I2R (A*STAR),"$2,500to$5,000",Monthly,Professional,https://www.mycareersfuture.sg/job/research-engineer-i2r-astar-research-entities-8baf748caed9144a3fb5a41ad6b522e2
34,A*STAR RESEARCH ENTITIES,"UOB PLAZA, 80 RAFFLES PLACE 048624","Contract, Full Time",Sciences / Laboratory / R&D,"Roles & ResponsibilitiesAbout the Institute for Infocomm Research (I²R) The Institute for Infocomm Research (I²R) is a member of the Agency for Science, Technology and Research (A*STAR) family and is Singapore’s largest ICT research institute. Our strategic thrusts are in the spheres of intelligence, communications and media and our research capabilities are in shared sensor networks, public-public/public-private data-sharing platform, big data analytics and visualization solutions. For more information about I2R, please visit www.i2r.a-star.edu.sg The project focuses on data management systems, data engineering solutions and deep learning systems for radiology applications. Candidate should have demonstrated interests or experience in: 1. Experience in the execution of translational projects focused on development, testing and deployment  2. Big data analytics and data engineering 3. Experience with biomedical datasets, in particular medical images 4. Business analysis skills and/or past work with/in clinical partner institutions  The position entails working in a multi-disciplinary business analytics translation group alongside machine learning and deep learning teams that are closely collaborating with clinical and industry partners on impactful projects that will translate research to deployed technology.","Requirements Minimum bachelor degree in computer science, computer engineering, mathematics and statistics, data science intensive programs, with expertise in one or more of the following areas: data mining and management, machine learning, time-series data analytics, etc. Minimum 2 years post completion of last degree Experience in clinical research environments is a plus Excellent knowledge of a programming language such as Node.js., java or C++  Proficient in Python Good knowledge on data analytics/ machine learning/ data mining and experience in solving real-world data science problems Able to deliver under tight schedules Good team player with both research and engineering ethics Good interpersonal and communication skills Prior experience with NLP is a big plus Prior experience with medical image processing, clinical informatics systems and software platforms is a big plus  The above eligibility criteria are not exhaustive. A*STAR may include additional selection criteria based on its prevailing recruitment policies. These policies may be amended from time to time without notice. We regret that only shortlisted candidates will be notified.",Senior Research Engineer (Data Analytics)  /  I2R (A*STAR),"$3,400to$6,800",Monthly,Professional,https://www.mycareersfuture.sg/job/senior-research-engineer-i2r-astar-research-entities-b4659907b132bbf61179fb31803ecf39
35,A*STAR RESEARCH ENTITIES,"UOB PLAZA, 80 RAFFLES PLACE 048624","Contract, Full Time",Sciences / Laboratory / R&D,"Roles & ResponsibilitiesAbout the Institute for Infocomm Research (I²R)  The Institute for Infocomm Research (I²R) is a member of the Agency for Science, Technology and Research (A*STAR) family and is Singapore’s largest ICT research institute. Our strategic thrusts are in the spheres of intelligence, communications and media and our research capabilities are in shared sensor networks, public-public/public-private data-sharing platform, big data analytics and visualisation solutions. For more information about I²R, please visit www.i2r.a-star.edu.sg We are looking for a capable and responsible engineer to work on and make contributions to a major big data R&D project on fraud risk prediction. The work scope involves design and implementation of big data management, analytics and web applications. Successful candidate will work with other research team members to do part of system implementation, data pre-processing, analysis and visualization. Successful candidate will also have opportunities to be involved in other industry projects and/or research projects.  ","Requirements Minimum Bachelor Degree in computer science or other related fields  Minimum 2 years experience in data analytics related projects   Well-versed in programming (Python, R, Java, J2EE, or C/C++), database (MySQL, NoSQL,MongoDB)   Experience in system development lifecycle   Familiarity with big data analytics toolkits/frameworks such as Spark/Hadoop/Cassandra/Mahout   Good team player, able to multitask and work independently   Good interpersonal and communication skills   The above eligibility criteria are not exhaustive. A*STAR may include additional selection criteria based on its prevailing recruitment policies. These policies may be amended from time to time without notice. We regret that only shortlisted candidates will be notified.",Research Engineer (Data Analytics)  /  I2R (A*STAR),"$2,500to$5,000",Monthly,Professional,https://www.mycareersfuture.sg/job/research-engineer-i2r-astar-research-entities-5084cd98811850f9c05c81be702834ea
36,A*STAR RESEARCH ENTITIES,"UOB PLAZA, 80 RAFFLES PLACE 048624","Contract, Full Time",Sciences / Laboratory / R&D,"Roles & ResponsibilitiesAbout the Institute for Infocomm Research (I²R)  The Institute for Infocomm Research (I²R) is a member of the Agency for Science, Technology and Research (A*STAR) family and is Singapore’s largest ICT research institute. Our strategic thrusts are in the spheres of intelligence, communications and media and our research capabilities are in shared sensor networks, public-public/public-private data-sharing platform, big data analytics and visualisation solutions. For more information about I²R, please visit www.i2r.a-star.edu.sg The project focuses on development of machine learning, deep learning and artificial intelligence algorithms for applicants in precision medicine. Candidates should have demonstrated interests or experience in one or more of the following:  Analysis of large scale heterogeneous biomedical datastreams (genomics, EMR, imaging, lifestyle) Projects involving biomarker identification, knowledge discovery, predictive analytics for patient outcomes, and/or clinical application. R&D for advanced algorithms.  Core responsibilities include preprocessing of raw disparate biomedical datasets, development of automated knowledge extraction and feature engineering pipelines, and design of pilot studies/demos. The position entails working in a multi-disciplinary machine learning and deep learning team in close collaboration with bioinformatics experts, biologists, clinicians, as well as other leading academic and industry partners on impactful projects that have the potential to transform patient-care and deliver improved health outcomes.  ","Requirements Minimum Bachelor Degree with knowledge or exposure to Computer Science, Bioinformatics, Computational Biology, Statistics, or other Data Science intensive fields Candidates should have particular experience in one or more of the following areas:      Large-scale data handling and/or databases Experience with biomedical data analysis Genomics/ Computational and Systems Biology Medical Imaging Biomedical informatics/ Healthcare Data Analytics Knowledge extraction and feature engineering Natural Language Processing/Text Mining Exposure to machine learning and deep learning methods is highly encouraged   Experience in corporate or application oriented environments is a plus. Ability to work independently and as part of a multidisciplinary team  Quick learner, willing to acquire the necessary domain knowledge  Good communication skills for proposals, reports, and publications Proficiency in spoken and written English. Experience with biomedical and healthcare datasets (EMR, medical imaging, genomics) is a plus. Strong programming abilities (Eg: Python, R, MATLAB, C/C++, Java, Perl, Bash) Familiarity with data preprocessing, data science and data visualization tools (Eg: SAS, Tableau, Knime, WEKA, Jupyter notebooks, deep learning, machine learning and visualization libraries)  Bioinformatics applicants looking to switch into fields related to analytics and intelligent systems with significant domain expertise and strong programming skills will also be considered.    The above eligibility criteria are not exhaustive. A*STAR may include additional selection criteria based on its prevailing recruitment policies. These policies may be amended from time to time without notice. We regret that only shortlisted candidates will be notified.",Research Engineer (Data Analytics)  /  I2R (A*STAR),"$2,500to$5,000",Monthly,Professional,https://www.mycareersfuture.sg/job/research-engineer-i2r-astar-research-entities-f0e72022c7d6b0386e377ce08bfba62d
37,NTUC ENTERPRISE NEXUS CO-OPERATIVE LIMITED,Unknown,Full Time,"Engineering, Information Technology","Roles & ResponsibilitiesNTUC Enterprise Co-operative Limited is the holding entity and single largest shareholder of the NTUC group of Social Enterprises. We aim to create a greater social force to do good by harnessing the capabilities of the social enterprises to meet pressing social needs in areas like health and eldercare, childcare, daily essentials, cooked food and financial services. Serving over two million customers, NTUC Enterprise wants to enable and empower all in Singapore to live better and more meaningful lives. The NTUC Enterprise Centre of Excellence for Data, Digitalisation and Technology leads the transformation of the NTUC Social Enterprises by leveraging digital technologies to become more nimble, adaptable and innovative in today’s digital age. he NTUC Enterprise Centre of Excellence for Data, Digitalisation and Technology has been registered as NTUC Enterprise Nexus, a wholly owned subsidiary of NTUC Enterprise. The rapid adoption of technology and mobile devices have contributed to vast new flows of information which are larger in volume, faster in velocity, diverse in variety, and requires veracity of the information for use. This new type of information composed of structured and unstructured data, broadly known as big data (and combined with tools and platforms), if utilized well, could radically improve business performance.  As the organization embarks to become a data-driven organization, significant decisions and value generation will be based on the data that we capture and deploy. The 7 SE’s range in a broad scope of data from FairPrice (retail), Income (insurance), Unity (healthcare), FoodFare (F&B), LearningHub (training), First Campus(ECE), Link (membership). The leaders of these groups are keen to utilize the data to drive growth, deliver customer service, and create personalized experiences. The Data Architecture & Information Management Team will manage and govern the overall datasets of the organization and drive the execution of how the data will be collected, stored, processed and applied across these social enterprises (SEs). In this role you will work with various industries and most diverse datasets in Singapore.    Responsibilities: ·       As a data engineer, you will be creating, writing and maintaining data transfer process and protocols for the data platform.  ·       Work closely with each SE tech heads and their external vendors in mapping out data fields and data transfer process.  ·       Design, build, support and optimize new and existing data models and ETL processes. ·       Develop and support the data pipeline to integrate new data from various data sources with emerging data technologies. ·       Develop and manage the various dashboards for management decision and data visualizations. ·       Define and manage SLA for all data processes and own data quality issues.  ","Requirements  Preferred qualification and skills:  ·       Advanced degree in computer science, computer engineering, or other technical fields. ·       6-10 years’ experience having developed data engineering capabilities for large and complex franchises. ·       Strong data modeling, schema design and SQL development skills  ·       ETL/ELT implementation and data integration ·       Modern open source data visualization tools, eg. D3js, superset, plotly, leaflet,etc.  ·       Big data platform development (Hadoop/Hive/Hbase/Spark, etc.) ·       REST/Web API development and management ·       Hands-on experience in any modern programming language (Python or Java preferred) ·       Design pattern, 12-factor app principle and modern cloud architecture ·       Self-motivated and proactive, willing to learn new things ·       Good communication skills and strong team player          ",Data Engineer,"$5,000to$10,000",Monthly,Professional,https://www.mycareersfuture.sg/job/data-engineer-ntuc-enterprise-nexus-co-operative-625ef348b4ebce0c98cf5e41294f8533
38,NTUC LINK PRIVATE LIMITED,Unknown,Full Time,"Engineering, Information Technology","Roles & ResponsibilitiesThe rapid adoption of technology and mobile devices have contributed to vast new flows of information which are larger in volume, faster in velocity, diverse in variety, and requires veracity of the information for use. This new type of information composed of structured and unstructured data, broadly known as big data (and combined with tools and platforms), if utilized well, could radically improve business performance.    As the organization embarks to become a data-driven organization, significant decisions and value generation will be based on the data that we capture and deploy. The 7 SE’s range in a broad scope of data from FairPrice (retail), Income (insurance), Unity (healthcare), FoodFare (F&B), LearningHub (training), First Campus(ECE), Link (membership). The leaders of these groups are keen to utilize the data to drive growth, deliver customer service, and create personalized experiences. The Data Architecture & Information Management Team will manage and govern the overall datasets of the organization and drive the execution of how the data will be collected, stored, processed and applied across these social enterprises (SEs). In this role you will work with various industries and most diverse datasets in Singapore.    Responsibility: ·       As a data engineer, you will be creating, writing and maintaining data transfer process and protocols for the data platform.  ·       Work closely with each SE tech heads and their external vendors in mapping out data fields and data transfer process.  ·       Design, build, support and optimize new and existing data models and ETL processes. ·       Develop and support the data pipeline to integrate new data from various data sources with emerging data technologies. ·       Develop and manage the various dashboards for management decision and data visualizations. ·       Define and manage SLA for all data processes and own data quality issues.  ","RequirementsPreferred qualification and skills:  ·       Advanced degree in computer science, computer engineering, or other technical fields. ·       6-10 years’ experience having developed data engineering capabilities for large and complex franchises. ·       Strong data modeling, schema design and SQL development skills  ·       ETL/ELT implementation and data integration ·       Modern open source data visualization tools, eg. D3js, superset, plotly, leaflet,etc.  ·       Big data platform development (Hadoop/Hive/Hbase/Spark, etc.) ·       REST/Web API development and management ·       Hands-on experience in any modern programming language (Python or Java preferred) ·       Design pattern, 12-factor app principle and modern cloud architecture ·       Self-motivated and proactive, willing to learn new things ·       Good communication skills and strong team player",Data Engineer,"$5,000to$10,000",Monthly,Professional,https://www.mycareersfuture.sg/job/data-engineer-ntuc-link-a0a7237670675de712423b8209df8127
39,JEWEL PAYMENTECH PTE. LTD.,"TECHLINK, 31 KAKI BUKIT ROAD 3 417818","Permanent, Full Time",Information Technology,"Roles & ResponsibilitiesAs a Data Engineer you will get to work on a wide range of problems using the cutting-edge technologies in Big Data and Data Science. You are required to translate Data Science and Machine learning based solutions into scalable code, and to develop innovative solutions to collect/cleanse/store/process data. In case you are very passionate about building high throughput, low latency, fault tolerant software then this position is for you.  To be successful in this role, you will need to:  Capture/Analyse requirements and lead the design/architecture of solutions to meet requirements.  Write code by using best software development practices/security standards. Lead projects end-to-end from conceptualisation to deployment. Write clear & concise documentation for solutions/code. Contribute ideas within team to build better code.  Continuously improve knowledge on new technologies. Excellent in English, both written and spoken.","RequirementsRequired Qualifications:  10+ years of experience building highly scalable, low latency, fault tolerant systems. B.Sc., Masters, or equivalent experience in a quantitative field (Computer Science, Mathematics, Engineering, Artificial Intelligence, etc.). Hands on knowledge of at least 2 programming languages of Python/Java/Scala. In depth knowledge of at least 2 of Hadoop/Spark/Storm/Flink/Kafka. In depth knowledge of at least two NoSQL database (HBase/Cassandra/DynamoDB/Neo4j/Mongo/MemcacheDB) Good knowledge of at least some machine learning algorithms like logistic regression/ SVM/ Random Forests.  Knowledge of advanced data structures and algorithms. Preferred Qualifications: Knowledge of large scale ML systems like Tensorflow/pytorch. Knowledge of advanced machine learning algorithms like CNN/RNN Knowledge of Cloud environments like AWS/GCP. Knowledge of indexing systems like Elastic search/Solr/Lucene.  Proficient in using CI/CD and knowledge of Jenkins/SonarQube/Ansible. You’re a perfect fit us if you are   A master problem solver, and able to use own initiative to develop suitable solutions.  A strong communicator with the ability to convey information to others in a simple and unambiguous way.  An innovative, original thinker approach to job responsibilities, methods and processes.  An energetic person who can be trusted to get a job done.    ",Lead Data Engineer,"$8,000to$10,000",Monthly,Professional,https://www.mycareersfuture.sg/job/lead-data-engineer-jewel-paymentech-5ce040f4ff138ac4658c44ded16e1542
40,HOOQ DIGITAL PTE. LTD.,"COMCENTRE, 31 EXETER ROAD 239732",Permanent,"Engineering, Information Technology","Roles & ResponsibilitiesWe are looking for a Data Engineer to join our rapidly expanding Data & Analytics team. You will help shape how we build and grow our service in the region. We look for self-starters who demand the best. Key Responsibilities:   Develop ETL/ELT jobs to integrate new data into the data warehouse or build new reporting schemas   Develop data pipelines, both bath and realtime from various platforms into the data lake   Manage various data platforms and seek out new technologies to improve efficiency   Develop advanced analytical models that help the business identify trends within customer base and behavior   Work closely with the business to understand data needs and create data sets to enable reporting and dashboards to monitor business performance   Ensure the data warehouse load jobs run as per schedule and data availability to the business is uninterrupted   Continuously improve the information management platforms of the company to leverage benefits ","RequirementsDesired Skills and Experience  Minimum 3 years of solid development experience within a data warehouse/information management team with strong understanding of programming languages like Java, Python, JavaScript and SQL.   Hands on experience in full-stack development, design and architecture. Experience in creating a REST API that can handle a production load (code + deploy).   Familiarity with AWS (DynamoDB, Redshift, S3, EC2, RDS, Lambda) (Will be an advantage but not mandatory)   Minimum 3 years development experience with ETL/ELT tools (preferably Pentaho DI, Informatica, Datastage or Talend)   Proven experience with Data Warehousing and Big Data technologies   Working knowledge of big Data Technologies like Hadoop, Hive, Spark and streaming/messaging services like Kafka,Spark streaming.   Solid understanding of some BI tools such as Cognos, QlikView or Tableau.   Comfortable working in dynamic fast paced environment with competing priorities. Self-starter and willing and able to learn on your own   Work well within a team environment and willing to accommodate task and duties that maybe outside of your JD for limited time periods ",Data Engineer,Salary undisclosed,Unknown,Manager,https://www.mycareersfuture.sg/job/data-engineer-hooq-digital-951d3deac4969252c3ee86cb1284d396
41,SENSORFLOW PTE. LTD.,"OXLEY BIZHUB, 61 UBI ROAD 1 408727",Permanent,Information Technology,"Roles & ResponsibilitiesAt SensorFlow we are planning for considerable growth over the next 12 months and need a data engineer to design and develop SensorFlow’s data infrastructure. As we are looking to build the data pipeline from scratch, you will have full autonomy and the technical backing from our engineering team in designing, developing and maintaining this infrastructure. Job Roles & Responsibilities   Design, develop and maintain SensorFlow’s infrastructure for streaming, processing and storage of data. Build tools for effective maintenance and monitoring of the data infrastructure.   Contribute to key data pipeline architecture decisions and lead the implementation of major initiatives.   Work closely with stakeholders to develop scalable and performant solutions for their data requirements, including extraction, transformation and loading of data from a range of data sources.   Develop the team’s data capabilities – share knowledge, enforce best practices and encourage data-driven decisions.  ","RequirementsSkill Requirements   Solid Computer Science fundamentals, excellent problem-solving skills and a strong understanding of distributed computing principles.   At least 2 years of experience in a similar role, with a proven track record of building scalable and performant data infrastructure.   Expert SQL knowledge and deep experience working with relational and NoSQL databases (e.g. HBase, Cassandra).   Advanced knowledge of Apache Kafka and demonstrated proficiency in Hadoop v2, HDFS, MapReduce.   Experience with stream-processing systems (e.g. Storm, Spark Streaming), big data querying tools (e.g. Pig, Hive) and data serialization frameworks (e.g. Protobuf, Thrift, Avro).   Tech Stack  Data storage: Amazon DynamoDB Service Layer: Amazon Lambda, Amazon API Gateway Service backend: JavaScript/TypeScript, Node.js Web frontend: Angular Mobile: Ionic  For Interested candidates, please email us your full resume to: jobs@sensorflow.org.  We regret only shortlisted candidate will be notified.",Data Engineer,"$6,000to$8,000",Monthly,Senior Executive,https://www.mycareersfuture.sg/job/data-engineer-sensorflow-84b3983ff61718861b98b5db0dfea38f
42,GUMI ASIA PTE. LTD.,"CONNEXIS, 1 FUSIONOPOLIS WAY 138632",Full Time,Information Technology,"Roles & Responsibilities Design, develop, implement, and evolve data pipelines powering core data sets and key business and performance metrics Identify, troubleshoot, and resolve any performance, system or data related issues, and work to ensure data consistency and integrity Work with Product and Marketing teams on data requirements.  Work with various game teams on data set and data flow to ensure that data requirements are met. Ensure the quality, accuracy, and timeliness of analytical data ","Requirements Min. 5 years working in a large analytical data ecosystem Strong technical understanding of data modelling, design, architecture principles, and techniques to take business requirements from concept to implementation Strong knowledge of relational databases and SQL. Knowledge of Python, PHP, Java, Linux architecture and scripting Extensive background extracting and transforming complex data sets. i.e. ETL  Experience with database design and star schema data warehouse theory ",Data Engineer,"$5,000to$7,000",Monthly,"Professional, Senior Executive",https://www.mycareersfuture.sg/job/data-engineer-gumi-asia-bf8174edbdaeab1602d40f6fa7afd0e6
43,TRAVELOKA SERVICES PTE. LTD.,"ROBINSON 77, 77 ROBINSON ROAD 068896",Permanent,Information Technology,"Roles & ResponsibilitiesData Engineers at Traveloka are passionate on designing, building, and maintaining our growing big data platform. We collect millions of events everyday to our data lake for insights, and to our real time pipeline for enabling many data-driven features such as personalized user experience. What you do will be mixed of software engineering, system architecture design, and operation:  You will be designing, building, supporting and scaling our data infrastructure. Including monitoring, alerting and debugging infrastructure that is streaming millions of events per hour with thousands of pipelines running Collaborate and coordinate with other departments (product, etc) to solve their use case using data technology; state of the art big data stack such as Kafka, Pubsub, Spark, DataFlow, BigQuery, Airflow, etc. on hundreds of terabytes data. Explore/learn new technologies that can complement or replace our current stack to improve it.   ","Requirements Passion in big data, software engineering, and systems. Excellent analysis and reasoning of system behaviors 8+ years hands-on experience Uphold best practices and principle around clean code, testing, continuous integration Strong team player and collaborator Having high level of responsibility and resilience in dealing with issues Preferably familiar with big data infrastructure (such as Kafka, Spark, etc.), cloud based infrastructure, varying databases, security concerns would be an advantage. Familiar with Java/JVM. Python is added advantage  Experience working with data infrastructure will be valuable, but your desire to learn data infrastructure is more important. Having systems operational experience is a bonus, but not required. 	  ",Senior Level Data Engineer,"$8,300to$15,000",Monthly,Senior Executive,https://www.mycareersfuture.sg/job/senior-level-data-engineer-traveloka-services-b0cb80230dcbaa180a9009d484df9ae5
44,TRAVELOKA SERVICES PTE. LTD.,"ROBINSON 77, 77 ROBINSON ROAD 068896",Permanent,Information Technology,"Roles & ResponsibilitiesData Engineers at Traveloka are passionate on designing, building, and maintaining our growing big data platform. We collect millions of events everyday to our data lake for insights, and to our real time pipeline for enabling many data-driven features such as personalized user experience. What you do will be mixed of software engineering, system architecture design, and operation:  You will be designing, building, supporting and scaling our data infrastructure. Including monitoring, alerting and debugging infrastructure that is streaming millions of events per hour with thousands of pipelines running Collaborate and coordinate with other departments (product, etc) to solve their use case using data technology; state of the art big data stack such as Kafka, Pubsub, Spark, DataFlow, BigQuery, Airflow, etc. on hundreds of terabytes data. Explore/learn new technologies that can complement or replace our current stack to improve it.   ","Requirements Passion in big data, software engineering, and systems. Excellent analysis and reasoning of system behaviors 6+ years hands-on experience Uphold best practices and principle around clean code, testing, continuous integration Strong team player and collaborator Having high level of responsibility and resilience in dealing with issues Preferably familiar with big data infrastructure (such as Kafka, Spark, etc.), cloud based infrastructure, varying databases, security concerns would be an advantage. Familiar with Java/JVM. Python is added advantage  Experience working with data infrastructure will be valuable, but your desire to learn data infrastructure is more important. Having systems operational experience is a bonus, but not required. 	  ",Mid -  Senior Level Data Engineer,"$6,100to$10,700",Monthly,Executive,https://www.mycareersfuture.sg/job/mid-senior-level-data-engineer-traveloka-services-9dadd0b7466285dd52264503a3893a77
45,TRAVELOKA SERVICES PTE. LTD.,"ROBINSON 77, 77 ROBINSON ROAD 068896",Permanent,Information Technology,"Roles & ResponsibilitiesData Engineers at Traveloka are passionate on designing, building, and maintaining our growing big data platform. We collect millions of events everyday to our data lake for insights, and to our real time pipeline for enabling many data-driven features such as personalized user experience. What you do will be mixed of software engineering, system architecture design, and operation:  You will be designing, building, supporting and scaling our data infrastructure. Including monitoring, alerting and debugging infrastructure that is streaming millions of events per hour with thousands of pipelines running Collaborate and coordinate with other departments (product, etc) to solve their use case using data technology; state of the art big data stack such as Kafka, Pubsub, Spark, DataFlow, BigQuery, Airflow, etc. on hundreds of terabytes data. Explore/learn new technologies that can complement or replace our current stack to improve it. 	  ","Requirements Passion in big data, software engineering, and systems. Excellent analysis and reasoning of system behaviors 3+ years hands-on experience Uphold best practices and principle around clean code, testing, continuous integration Strong team player and collaborator Having high level of responsibility and resilience in dealing with issues Preferably familiar with big data infrastructure (such as Kafka, Spark, etc.), cloud based infrastructure, varying databases, security concerns would be an advantage. Familiar with Java/JVM. Python is added advantage  Experience working with data infrastructure will be valuable, but your desire to learn data infrastructure is more important. Having systems operational experience is a bonus, but not required. 	  ",Mid Level Data Engineer,"$4,300to$7,600",Monthly,Executive,https://www.mycareersfuture.sg/job/mid-level-data-engineer-traveloka-services-25d8051597693057fd4fc7eb55143515
46,TRAVELOKA SERVICES PTE. LTD.,"ROBINSON 77, 77 ROBINSON ROAD 068896",Permanent,Information Technology,"Roles & ResponsibilitiesData Engineers at Traveloka are passionate on designing, building, and maintaining our growing big data platform. We collect millions of events everyday to our data lake for insights, and to our real time pipeline for enabling many data-driven features such as personalized user experience. What you do will be mixed of software engineering, system architecture design, and operation:  You will be designing, building, supporting and scaling our data infrastructure. Including monitoring, alerting and debugging infrastructure that is streaming millions of events per hour with thousands of pipelines running Collaborate and coordinate with other departments (product, etc) to solve their use case using data technology; state of the art big data stack such as Kafka, Pubsub, Spark, DataFlow, BigQuery, Airflow, etc. on hundreds of terabytes data. Explore/learn new technologies that can complement or replace our current stack to improve it. 	  ","Requirements Passion in big data, software engineering, and systems. Excellent analysis and reasoning of system behaviors Fresh Graduate to 4 years hands-on experience Uphold best practices and principle around clean code, testing, continuous integration Strong team player and collaborator Having high level of responsibility and resilience in dealing with issues Preferably familiar with big data infrastructure (such as Kafka, Spark, etc.), cloud based infrastructure, varying databases, security concerns would be an advantage. Familiar with Java/JVM. Python is added advantage  Experience working with data infrastructure will be valuable, but your desire to learn data infrastructure is more important. Having systems operational experience is a bonus, but not required. 	  ",Junior - Mid Level Data Engineer,"$3,000to$6,000",Monthly,Fresh/entry level,https://www.mycareersfuture.sg/job/junior-mid-level-data-engineer-traveloka-services-32e223ac87c140e00e2bec31f5d7eb29
47,JEWEL PAYMENTECH PTE. LTD.,Unknown,"Permanent, Full Time",Information Technology,"Roles & ResponsibilitiesAs a Data Engineer you will get to work on a wide range of problems using the cutting-edge technologies in Big Data and Data Science. You are required to translate Data Science and Machine learning based solutions into scalable code, and to develop innovative solutions to collect/cleanse/store/process data. In case you are very passionate about building high throughput, low latency, fault tolerant software then this position is for you. To be successful in this role, you will need to:  Analyze requirements and deliver solutions that meet requirements. Write code by using best software development practices. Produce code that meets security standards. Estimate timelines and deliver solutions within agreed timeline. Write clear & concise documentation for solutions/code. Contribute ideas within team to build better code. Continuously improve knowledge on new technologies. Excellent in English, both written and spoken. ","Requirements B.Sc., Masters, or equivalent experience in a quantitative field (Computer Science, Mathematics, Engineering, Artificial Intelligence, etc.). Knowledge in the use and application of Python to develop complex software. General machine learning techniques and technologies (e.g., Bayesian classifiers, regression techniques, graphical models, working with unbalanced data-sets) as well as applications (e.g., predictive analytics). NoSQL Database Programming/Development. Manipulation of various types of data; data cleaning, filtering, and pre-processing for example with text/images. Knowledge and experience in the use of cloud computing platforms (AWS/Azure/GCP/etc). SQL familiarity and database technologies (e.g., row versus column stores, in-memory DB, DB clustering, HA for DB). Familiarity and experience with Linux environments. Understanding batch (e.g., Apache Hadoop / Map Reduce) and stream processing approaches / frameworks (e.g., Apache Spark).    You’re a perfect fit us if you are  A master problem solver, and able to use own initiative to develop suitable solutions. A strong communicator with the ability to convey information to others in a simple and unambiguous way. An innovative, original thinker approach to job responsibilities, methods and processes. An energetic person who can be trusted to get a job done. ",Data Engineer,"$4,000to$5,000",Monthly,Professional,https://www.mycareersfuture.sg/job/data-engineer-jewel-paymentech-b5ed42b992f59a193bacf860079ba10f
48,THE SUPREME HR ADVISORY PTE. LTD.,"AZ @ PAYA LEBAR, 140 PAYA LEBAR ROAD 409015",Full Time,Information Technology,Roles & Responsibilitieslocation: Scotts Road   ,"RequirementsMAIN ROLES AND RESPONSIBILITIES • OS Backup & Server’s Data Backup • Network, VOIP & Server Administration • PC’s Installation & Configuration • OS & Software Installation & Configuration • Troubleshooting PC’s, Notebook’s, Printer’s & Network Equipment • Troubleshooting OS & Software issues • Create/ Manage computer & notebook Asset List",L1 Desktop Engineer - Scotts Road - OS Backup & Server’s Data Backup (A1),"$1,800to$2,100",Monthly,Senior Executive,https://www.mycareersfuture.sg/job/l1-desktop-engineer-scotts-road-os-backup-server%E2%80%99s-data-backup-supreme-hr-advisory-a13f8fd321d4437ec06552b3f6e51849
49,EYEOTA PTE. LTD.,12A UPPER CIRCULAR ROAD 058410,Full Time,Information Technology,"Roles & ResponsibilitiesEyeota is looking for an exceptional Data Engineer who can contribute to building a world-class big data engineering stack that will be used to fuel our Machine Learning product pipeline. This person will be contributing to the architecture, operation, and enhancement of:   Our petabyte-scale data platform with a key focus on finding solutions that can support the Machine Learning product roadmap. This platform ingests terabytes of data daily which need to be made available to a variety of Machine Learning use cases. Our bespoke Machine Learning pipelines. This will also provide opportunities to contribute to the prototyping, building, and deployment of Machine Learning models.  The candidate should have significant experience in developing and operating a modern data pipeline platform and should have a keen interest in Machine Learning and Data Science","RequirementsYou:  Minimum 4 years Experience working with petabyte-scale data Experience architecting, developing, and operating data warehouses, big data analytics platforms, and high-velocity data pipelines Exposure to modern Big Data tech: Cassandra/Scylla, Kafka, Ceph, the Hadoop Stack, Spark, Flume, Hive, Druid etc… while at the same time understanding that certain problems may require completely novel solutions Exposure to one or more modern ML tech stacks: Spark ML-Lib, Tensorflow, Keras, GCP ML Stack, AWS Sagemaker Deep technical understanding of Golang and/or Java Production experience with Python is a plus Exposure to configuration management tools such as Ansible or Salt Exposure to IAAS platforms such as AWS, GCP, Azure… Strong buyer of Agile/Lean values Experience with supporting and troubleshooting large systems ","Data Engineer, Modeling & Onboarding","$8,000to$15,000",Monthly,Middle Management,https://www.mycareersfuture.sg/job/data-engineer-modeling-onboarding-eyeota-1bff565c5baa88783a0f179db8b9058a
50,PEOPLE PROFILERS PTE. LTD.,"SHAW TOWERS, 100 BEACH ROAD 189702",Permanent,Information Technology,"Roles & Responsibilities Join a fast-expanding and highly-specialised tech company Good exposure in data analytics projects Central location Great company culture & working environment    Requirements  Make enhancements to data collection, structure & delivery Use automated methods for data compilation Build robust batch & streaming pipelines Write clear and high-quality code (including in Python) Work well in a team ","RequirementsRequirements  Degree in a technical/quantitative discipline e.g. Mathematics/Computer Science Ideally at least 1 year of relevant experience Experience in Python programming, web crawling Knowledge of AWS platforms     Helpful:  -Familiarity with ETL design & database management -Familiarity with NoSQL database -Knowledge of machine learning techniques   Successful candidates can expect a very competitive salary package with comprehensive benefits. Interested applicants may wish to email your resume in a detailed Word format to ruth.gan@peopleprofilers.com. Please include last drawn and expected salaries and notice period. We regret that only shortlisted candidates will be notified.   Gan Huiru  Recruitment Consultant  Tel: +65 6594 9897 Fax +65 6835 7890   Address: 100 Beach Road #33-06 Shaw Tower Singapore 189702 Email: ruth.gan@peopleprofilers.com EA License Number: 02C4944 Registration Number: R1768917",Data Engineer (Python / Central),"$4,000to$6,000",Monthly,Junior Executive,https://www.mycareersfuture.sg/job/data-engineer-people-profilers-c751566bb80319a39400f1f21ffd396b
51,INFINEON TECHNOLOGIES ASIA PACIFIC PTE LTD,"INFINEON, 8 KALLANG SECTOR 349282","Permanent, Full Time","Engineering, Information Technology, Manufacturing","Roles & ResponsibilitiesYou are responsible to support the automated / productivity projects which are relevant to Automated Test Equipment (ATE), and improve data handling towards automation. In your new role you will:  Support automated / productivity projects relevant with Automated Test Equipment (ATE) & data handling Improve data clarity with scripting towards yield improvement Drive & improve data relevant KPI (data quality, process time) towards data automation Responsible for production of automated computer hardware and software for test equipment Install and configure, investigate, diagnose and solve ATE computer software and hardware issues ","RequirementsYou are best equipped for this task if you have:  Bachelor's Degree in Engineering / Information Technology Good knowledge on computer hardware, and standard application support Good communication & analytical problem-solving skills Previous experience in the semiconductor testing field will be advantageous Candidate must be comfortable working on both Windows and Linux environment Good Understanding of Programming / Query languages / Visualisation software such as Python,Perl, Javascript, C / C++, SQL,Tableau  Please apply via https://www.infineon.com/cms/en/careers/jobsearch/jobsearch/32237-Senior-Engineer---Engineer-Data-Management/ ",Senior Engineer  /  Engineer - Data Management,"$3,000to$6,000",Monthly,Executive,https://www.mycareersfuture.sg/job/senior-engineer-engineer-data-management-infineon-technologies-asia-pacific-3403f6d4103cc4c5d37c0675be210fbc
52,PM ASIA PROJECT SERVICES PTE. LTD.,"THE SYNERGY, 1 INTERNATIONAL BUSINESS PARK 609917","Permanent, Contract",Engineering,"Roles & ResponsibilitiesOverview:  Lead the Electrical Design of large scale, high end industrial facilities.  Have knowledge and experience in designing Electrical systems for Data Centres, including Medium Voltage, Low Voltage Power Systems, Emergency Power Systems, ELV’s (CCTV, Interlocks, Fire Alarm, Power Monitoring System) Work in a multi-disciplinary design office environment for global clients. Display a personal commitment to safety, hold safety as a core value and provide safety leadership in the performance of all work activities Be quality focussed, producing well engineered designs to the highest standards in an ISO9000 Quality System environment   ","Requirements Degree in Electrical Engineering, Chartered Engineer preferred At least 10 years’ of experience of relevant experience  Experience within the Data Centre/ Pharmaceutical / Food & Nutriton industry preferred ",Lead Electrical Engineer (Data Centre),"$7,500to$11,000",Monthly,Senior Executive,https://www.mycareersfuture.sg/job/lead-electrical-engineer-pm-asia-project-services-670e2d8df5923f0e81e3b39995f7038f
53,SIEMENS PTE. LTD.,"THE SIEMENS CENTRE, 60 MACPHERSON ROAD 348615","Permanent, Full Time",Information Technology,"Roles & ResponsibilitiesWhat are my responsibilities?  Responsible for the integration of large, structured and unstructured data volumes into the existing cloud platforms Development of scalable end-to-end data pipelines for batch and stream processing Execution of the data integration activities (ETL /ELT) for populating the data lake and integrating diverse data sources Execution an further development of the physical implementation of the logical data model into a physical implementation in the data lake Implementation of solutions for reference data and master data management within the context of the mobility data business Execution of data quality measurements and implementation of data quality improvement ","RequirementsWhat do I need to qualify for this job?  University degree in an appropriate area (e.g.informatics) At least 2 years of relevant work experience Experience with modern big data technologies like Hadoop, MapReduce, Kafka, Hive, Presto, Spark, etc. Experience with cloud solutions like AWS Experience with programming languages like SQL, Scala, Python, Java Experience with enterprise application integration ",Big Data Engineer (287102),"$4,000to$7,000",Monthly,"Professional, Senior Executive",https://www.mycareersfuture.sg/job/big-data-engineer-siemens-b60dbaf7939bcf609cb9866708a184ac
54,NIOMETRICS (PTE.) LTD.,"PARKVIEW SQUARE, 600 NORTH BRIDGE ROAD 188778",Permanent,Information Technology,"Roles & ResponsibilitiesWHAT WE DO We invite you to be part of our ambitious, close-knit team creating systems for large customers who need to crunch through Tbps of data in real-time. Our approach is relentless performance-oriented software engineering vs. server sprawl in our customers' datacentres. You will use the latest high-end hardware and continuously devise ways to push the envelope of software performance. We build in-house systems if we must. We had to for indexing 1M 60-column rows/s, for aggregating high throughput event streams over hundreds of combinations of dimensions, and for pattern matching 5M patterns at 100Gb/s per 2RU. We use these to solve real customer problems. You will experiment wildly. For example we implemented network monitoring using a GPU, and we tested 4-socket machines with 2T RAM. Our current favourite platform is a 2-socket system with E5-2699v4 CPUs (88 lcores in total), 4x40Gbps NICs and 1T RAM, which we use to process 160Gbps. You will help us build a successful software platform for the long run. We invest a lot in flexibility, such as with our extensible rule engine and declarative aggregation system that empowers our analysts and helps us minimise the C code we have to write for supporting disparate use-cases. We know the devil is in the details. You will improve performance through better memory allocation systems and better data structures, all while ensuring that they are integrated with Address Sanitizer and fully tested using unit tests and end-to-end regression tests. We work end-to-end. You will implement data engineering solutions that are both efficient and secure for handling events from 500 million users, and to extract insight without leaking individual information. We want to show off. To attract the best programmers we plan to showcase our technology. You can be part of our effort to open-source interesting pieces of our technology stack.   YOUR ROLE AS HIGH-PERFORMANCE DATA ENGINEER As a High-Performance Data Engineer, you will create and maintain tools, mainly in C, for crunching large amounts of data in files or streams. You will have to think both big, in terms of overall architecture, and small, in terms of low-level optimisations, to deliver solutions that are reusable, and match the performance of the best hardware. Every capability you add directly translates to new offerings made possible. Every percent of performance improvement directly translates to large cost savings. At the same time, the correctness and reliability of your work will be the cornerstone to our customers’ trust.",RequirementsWHAT WE VALUE  Bachelor’s or Higher Degree in Computer Science or equivalent Software craftsmanship Attention to reliability and successful delivery Experience with large C code bases and high-performance C programming Familiarity with shared memory data structures and parallel algorithms Proficiency with Linux system & development tools ,High-Performance Data Engineer,"$5,500to$11,000",Monthly,Professional,https://www.mycareersfuture.sg/job/high-performance-data-engineer-niometrics-66d8658e3e2ccf132df50b9b94f43931
55,CAPITA PTE. LTD.,"ASIA SQUARE TOWER 1, 8 MARINA VIEW 018960","Contract, Full Time",Information Technology,"Roles & Responsibilities​Responsibility   Monitoring of ESX, OS Batch Monitoring Service Request Execution Coordinate with Appointed vendor for Offsite tape archival Restoration of VM/s Batch execution and monitoring, batch output & event handling, batch incident identification, escalation & reporting Tape backups, media handling, tape library operations and off-site storage Server health checks Start-up, shutdown, reboot/restart systems & services Generate daily/ weekly/ monthly/ yearly/ ad-hoc reports and dispatch reports to users & customers Adhere to all operational & physical security procedures Provide operational support during DR Exercises at DR sites Work towards acceptable audit rating ","RequirementsRequirements:  Minimum 1 year of experience in Data Center Experience in monitoring of servers and network Tape management, facility checks  Interested candidates, please click the Äpply Now"" below Only shortlisted applicants will be notified by our consultants.   CAPITA PTE LTD | EA License No : 08C2893 Tan Chin Yin | REG No : R1762272",Data Center (System) Engineer,"$2,500to$3,000",Monthly,Junior Executive,https://www.mycareersfuture.sg/job/data-center-engineer-capita-55edf1efb48a58087f2beab5fe483a24
56,Company Undisclosed,Unknown,Full Time,Banking and Finance,"Roles & ResponsibilitiesWorldQuant develops and deploys systematic financial strategies across a variety of asset classes and global markets. We seek to produce high-quality predictive signals (Alphas) through our proprietary research platform to employ financial strategies focused on exploiting market inefficiencies. Our teams work collaboratively to drive the production of Alphas and financial strategies – the foundation of a sustainable, global investment platform.   Technologists at WorldQuant research, design, code, test and deploy projects while working collaboratively with researchers and portfolio managers. Our environment is relaxed yet intellectually intense. Our teams are lean and agile, which means rapid prototyping of products with immediate user feedback. We seek people who think in code, aspire to solve undiscovered computer science challenges and are motivated by being around like-minded people. In fact, of the 600 employees globally, approximately 500 of them code on a daily basis.   WorldQuant’s success is built on a culture that pairs academic sensibility with accountability for results. Employees are encouraged to think openly about problems, balancing intellectualism and practicality. Great ideas come from anyone, anywhere. Employees are encouraged to challenge conventional thinking and possess a mindset of continuous improvement. That’s a key ingredient in remaining a leader in any industry.      Our goal is to hire the best and the brightest. We value intellectual horsepower first and foremost, and people who demonstrate an exceptional talent. There is no roadmap to future success, so we need people who can help us create it. Our collective intelligence will drive us there.   The Role: In this role, candidate will implement and maintain software towards creation of new datasets. Data sets will be consumed internally by researchers and utilized by internal quantitative models. Candidate will design efficient algorithms for collection, analysis, processing and filtering of data.    Work with the global team in designing and implementing data retrieval software for various data sets Implement the rules and procedures that ensure integrity in data sets Collect and analyze statistics on market data applications and devise approaches to improve the relevant processes Develop and enhance monitoring tools to detect various types of errors in data ","RequirementsWhat You’ll Bring:    Background in Computer Science, Engineering, Math or Physics, with minimum Bachelor’s degree. Proof of good academic record (such as GPA and other relevant test scores) Effective problem-solving skills both independently and as member of a team Good communication skills: must be fluent in English, spoken and written Experience working under Linux environment, familiar with vi or emacs for editing files Interested in applying technology to real world situation, comfortable working in fast paced work environment, detail oriented and capable performing tasks under time pressure Experience with programming in C/C++, familiar with common algorithms and data structures (binary tree, sorting, etc), Object Oriented programming and design patterns. Familiarity with compilers, debuggers under Linux (gcc, g++, gdb). Experience with scripting languages, such as Perl, Python, and shell scripting Knowledge of basic statistics/probability, familiar with concepts such as correlation, standard deviation and how to compute Familiarity with databases (such as MySQL) ",Data Development Engineer,"$6,000to$8,000",Monthly,Professional,https://www.mycareersfuture.sg/job/data-development-engineer-38aad66c1a08506f4012e0c235d5b40c
57,Company Undisclosed,Unknown,"Permanent, Full Time","Engineering, Manufacturing","Roles & ResponsibilitiesDo you have a broad theoretical and practical understanding of data engineering and data science? Can you wrangle large scale multidimensional data effectively? Are you always curious to learn something new? Do you love to solve engineering puzzles and optimize complex systems? Can you translate an idea in to an algorithm and make it into a product with quality and scalability in mind? Are you looking for window to the world ?   If so, you may be a great candidate for an Manufacturing Central Team (MCT) Data Engineer position at company, a global, Fortune 500 leader in the semiconductor industry. This position will be based in Singapore.   As an MCT Data Engineer at company, you will:   * Work with an international team of data scientists, data engineers, software engineers, process and equipment engineers, process integration engineers, yield enhancement engineers, R&D, etc. in a collaborative manner to develop new data science solutions that improve quality, improve yield, reduce deviations, improve manufacturing cycle time, reduce cost, extend manufacturing capabilities, etc. * Draw from a broad background of data-mining techniques in mathematics, statistics, information technology, machine learning, data engineering, design of experiments (DOE), visualization, etc. to discover insightful patterns in semiconductor manufacturing data * Work on projects and develop solutions that would be of high impact to various areas at all manufacturing fabs * Deliver polished presentations of data acquisition, data flow, data preparation and data presentation layer to internal customers and leaders to inform business strategy, streamline operations, and execute to revenue goals * In short, be a full-stack data engineer who can take an idea, access and prepare necessary data, work with data scientists to create machine learning models, develop it to an application with intuitive user interface, integrate with any pre-existing systems, demonstrate successful use cases and wins, etc.   Responsibilities and Tasks include, but not limited to:   * Understanding business needs and strategy to develop data science solutions * Collaborating with other data engineers and business process experts to access existing data in data warehouse and big data environments * Developing new or enhancing prior data acquisition and ETL pipelines from various sources into big data ecosystem. * Preparing data for machine learning using appropriate steps and methods, which may include data cleaning, transformation, augmentation, enrichment, sampling, etc. * Working with various scientific data such as equipment sensor data and logs, image and various types of signals, manufacturing process data, etc. to extract meaningful information for analytics * Creating intuitive user interface for interactive data visualization to explain insights from data * Preparing and delivering powerful presentations with rich data visualizations and meaningful business conclusions * Documenting the train of thoughts used to design and implement solutions along with managed source code * Traveling and participating in various internal forums for strategy building and to build solutions in collaboration with various manufacturing sites","RequirementsQualifications and Experience: * B.S degree or M.S. degree with 2 years’ experience in Computer Engineering, Industrial Engineering, or any other discipline with extensive programming or machine learning work  * Minimum 2 years of experience working in big data and data science projects and teams * Extensive experience with Java, Scala, Python in Hadoop ecosystem (Spark, Hive, HBase, etc.) is a must * Extensive experience with at least one relational databases (MS SQL, Oracle, MySQL, Teradata, etc.) is a must * Experience with building analytical web applications and data visualization technologies (Django, Javascript, Bootstrap, D3, etc.) is a plus * Good grasp of statistical and scientific programming packages in Python, R, etc. * Good grasp of data science concepts with emphasis on machine learning techniques is a plus * Experience with image processing (OpenCV, Python PIL, scikit-image, etc.) is a plus * Proficiency with collaborative source code management and documentation tools. (GIT, JIRA, Confluence, etc.) * Strong communication skills (written, verbal and presentation) * Willing to do international travel",MCT Big Data Engineer,"$3,400to$6,800",Monthly,Non-executive,https://www.mycareersfuture.sg/job/mct-big-data-engineer-d7f534d77330aac893279fc384dca2bd
58,Company Undisclosed,Unknown,"Permanent, Full Time","Engineering, Manufacturing","Roles & ResponsibilitiesResponsibilities: Be part of a DevOps team that design, build and maintain innovative Smart Manufacturing solutions and Big Data platform.  Participate in Agile development lifecycle for software & solution related to Smart Manufacturing and Big Data platform.   Work with Data Science within company to develop, automate and maintain reliable data analytic and mining solutions for Smart Manufacturing and Big Data platform.  Ability to assess current IT environments and make recommendations to increase capacity needs.  Communicate, collaborate and coordinate on Smart Manufacturing and Big Data related activities to various level of stakeholders and senior management.","RequirementsRequirements: Bachelor’s or Master’s degree Computer Science, Electrical & Electronics/Computer/Software Engineering, Information Systems or related fields.   Fresh graduates are welcome to apply and for those with good understanding and hands-on experience in the following areas will be advantageous.    Hadoop based technologies such as HDFS, MapReduce, Hive, MongoDB, HBase, Spark etc.  Data warehousing solutions and latest (NoSQL) database technologies.  Programming or scripting languages like Java, Linux, Matlab, C#/C++, Python, Perl and/or R on Linux/Windows platforms.  Big Data visualization and reporting software like Tableau.  ETL/BI solutions using Microsoft SSIS, Informatica or having DB programming experience (TSQL, PLSQL).    Effective oral and written communication with strong analytical, problem solving, multitasking and project management skills are essential on the job.",IT - BIG DATA ENGINEER,"$3,400to$6,800",Monthly,Non-executive,https://www.mycareersfuture.sg/job/-big-data-engineer-8e55a4d0f381865a40d9d1930e36d367
59,Company Undisclosed,Unknown,"Permanent, Full Time","Engineering, Manufacturing","Roles & ResponsibilitiesDo you have a broad theoretical and practical understanding of data engineering and data science? Can you wrangle large scale multidimensional data effectively? Are you always curious to learn something new? Do you love to solve engineering puzzles and optimize complex systems? Can you translate an idea in to an algorithm and make it into a product with quality and scalability in mind? Are you looking for window to the world ?   If so, you may be a great candidate for an Manufacturing Central Team (MCT) Data Engineer position at company, a global, Fortune 500 leader in the semiconductor industry. This position will be based in Singapore.   As an MCT Data Engineer at company, you will:   * Work with an international team of data scientists, data engineers, software engineers, process and equipment engineers, process integration engineers, yield enhancement engineers, R&D, etc. in a collaborative manner to develop new data science solutions that improve quality, improve yield, reduce deviations, improve manufacturing cycle time, reduce cost, extend manufacturing capabilities, etc. * Draw from a broad background of data-mining techniques in mathematics, statistics, information technology, machine learning, data engineering, design of experiments (DOE), visualization, etc. to discover insightful patterns in semiconductor manufacturing data * Work on projects and develop solutions that would be of high impact to various areas at all manufacturing fabs * Deliver polished presentations of data acquisition, data flow, data preparation and data presentation layer to internal customers and leaders to inform business strategy, streamline operations, and execute to revenue goals * In short, be a full-stack data engineer who can take an idea, access and prepare necessary data, work with data scientists to create machine learning models, develop it to an application with intuitive user interface, integrate with any pre-existing systems, demonstrate successful use cases and wins, etc.   Responsibilities and Tasks include, but not limited to:   * Understanding business needs and strategy to develop data science solutions * Collaborating with other data engineers and business process experts to access existing data in data warehouse and big data environments * Developing new or enhancing prior data acquisition and ETL pipelines from various sources into big data ecosystem. * Preparing data for machine learning using appropriate steps and methods, which may include data cleaning, transformation, augmentation, enrichment, sampling, etc. * Working with various scientific data such as equipment sensor data and logs, image and various types of signals, manufacturing process data, etc. to extract meaningful information for analytics * Creating intuitive user interface for interactive data visualization to explain insights from data * Preparing and delivering powerful presentations with rich data visualizations and meaningful business conclusions * Documenting the train of thoughts used to design and implement solutions along with managed source code * Traveling and participating in various internal forums for strategy building and to build solutions in collaboration with various manufacturing sites","RequirementsQualifications and Experience: * B.S degree or M.S. degree with 2 years’ experience in Computer Engineering, Industrial Engineering, or any other discipline with extensive programming or machine learning work  * Minimum 2 years of experience working in big data and data science projects and teams * Extensive experience with Java, Scala, Python in Hadoop ecosystem (Spark, Hive, HBase, etc.) is a must * Extensive experience with at least one relational databases (MS SQL, Oracle, MySQL, Teradata, etc.) is a must * Experience with building analytical web applications and data visualization technologies (Django, Javascript, Bootstrap, D3, etc.) is a plus * Good grasp of statistical and scientific programming packages in Python, R, etc. * Good grasp of data science concepts with emphasis on machine learning techniques is a plus * Experience with image processing (OpenCV, Python PIL, scikit-image, etc.) is a plus * Proficiency with collaborative source code management and documentation tools. (GIT, JIRA, Confluence, etc.) * Strong communication skills (written, verbal and presentation) * Willing to do international travel",MCT Big Data Senior Engineer,"$5,000to$10,000",Monthly,Non-executive,https://www.mycareersfuture.sg/job/mct-big-data-senior-engineer-a6442261f905654e89c7c81a0633461f
60,Company Undisclosed,Unknown,"Permanent, Full Time","Engineering, Manufacturing","Roles & ResponsibilitiesResponsibilities •As a Manufacturing Central Team Quality Engineer at company, you will be a member of the worldwide Quality Team responsible for QDRs, reducing variation and deviations for all company's Fabs. •Specific to your role as Data Specialist, your work will encompass the responsibilities of an architect, a designer and a developer/administrator. You need to have an in-depth understanding of database and structure and use those skills and knowledge to maintain their stability and reliability. •You will work with SMEs to identify their needs and to design and implement reporting dashboard and data structure to meet their requirements for quality improvement; your work will also extend to recommend improvements to meet the demands of swiftly changing interface technology; you will also be in charge of performing backups procedures to protect the data.","RequirementsRequirements: •A Computer Science degree or related disciplines. •A solid and wide-reaching foundation in programming and database structures is required. (SQL, Perl, C++, PHP) •A good knowledge of Tableau or related software that able to present data in a meaningful manner to enable SMEs to perform analysis. •In addition to possessing technical know-how and being communication savvy, data specialists must also be creative problem solvers",MCT QE Engineer (Data Specialist),"$4,000to$8,000",Monthly,Non-executive,https://www.mycareersfuture.sg/job/mct-qe-engineer-8904f86cd01b51bb85ec6a26fa44e23b
61,KPLER PTE. LTD.,Unknown,Full Time,Information Technology,"Roles & ResponsibilitiesWho are we ?   Kpler is an intelligence company providing transparency solutions in energy markets. We develop proprietary technologies that systematically aggregate data from hundreds of sources ranging from logistical and commercial, to governmental and shipping databases. By connecting the dots across fragmented information landscapes, we are able to provide our clients with unique, real-time market coverage.   We rely on intelligent people to build intelligent software. Our team is composed of individuals of various backgrounds, with diversified skill sets and international experiences. Our clients are players across the energy market spectrum, with offices from Houston to Singapore.   Role Purpose   We are looking for a Senior Data Engineer to join our software engineering team in Singapore to work on our data pipelines (collect, manage, data lake storage), data algorithms (based on either business rules, constraint programming, ML, etc.) and be a technical referent in Python and/or Scala. Our future team member will have a good understanding of data collection and management of complex B2B business rules. Also, you will take ownership of large features from technical design through completion. At Kpler, the Senior Data Engineer is at the centre of our research delivery and will have a key role in defining architecture, helping identify and implement areas for improvement within our data methodologies and technologies used. Based in Singapore, you will also interact with the Paris engineering team; being able to communicate efficiently in English (mandatory, we have more than 15 nationalities at Kpler!) and work with remote team members is key.   Also, you will:  Coach and work on code review with more junior engineers Ensure integrity of data through creative, robust and sustainable quality control methods Participate in operations/support of the real-time platforms Participate in defining coding standards, specifications and development processes Translating technical concepts to/from non-technical language ","RequirementsKnowledge & Experience   Must Have    At least 5 years of experience in similar roles working Python and/or Scala and cloud infrastructure (Amazon AWS) on linux servers Experience with SQL (PostgreSQL or equivalent) Proficiency developing automated unit and integration tests and continuous integration  Ability to learn quickly and deliver high quality code in a fast-paced, dynamic team environment     Nice to have    Knowledge of SqlAlchemy, Scala Play2, ElasticSearch, Heroku Experience of data management lifecycles (collection, cataloguing, ETL design) Experience with geographic information systems (PostGIS for instance) Experience of LEAN methodologies and approaches to process optimisation ",Senior Data Engineer,"$7,000to$10,000",Monthly,Senior Executive,https://www.mycareersfuture.sg/job/senior-data-engineer-kpler-dedd2bb8800198d1af9afdc64ca90870
62,"JPMORGAN CHASE BANK, N.A.",Unknown,"Permanent, Full Time",Information Technology,"Roles & ResponsibilitiesAs a Data Center Operations Engineer I, your mission is to support the day-to-day technology operations of JPMorgan Chase mission critical data centers. The purpose of this role is to maintain operational stability and handle customer requests while working on shifts and on calls to support the 24x7 operation. You will be responsible for installing and configuring enterprise class technology hardware, troubleshooting hardware and network issues, maintain change control process in the data center, and support 3rd party vendor activities. This position is full-time, working with team members on a rotating basis.  ","RequirementsThis role requires a wide variety of strengths and capabilities, including:    Understanding of information technology concepts in a working or academic environment General knowledge of a physical IT infrastructure (server, networking, storage) Some understanding of network concepts (switching, routing, perimeter security) Some understanding of operating systems (Windows, Linux, AIX) A mindset that challenges rather than simply understands and accepts Passionate about technology, innovation and continuous learning Ability to integrate well into a team, connect and collaborate effectively with the wider organization Being obsessed about our customers’ experience Flexibility to work non-business hours that may include weekends and/or holidays Ability to support frequent standing, walking, pushing, pulling, bending, reaching, and lifting up to 50lbs Willingness for occasional travel between sites          Our Global Technology Infrastructure group is a team of innovators rewarded with innovators who love technology as much as you do. Together, you will use a disciplined, innovative and a business focused approach to develop a wide variety of high-quality products and solutions. You will work in a stable, resilient and secure operating environment where you—and the products you deliver—will thrive.   When you work at JPMorgan Chase & Co., you are not just working at a global financial institution. You are an integral part of one of the world’s biggest tech companies. In 14 technology hubs worldwide, our team of 40,000+ technologists design, build and deploy everything from enterprise technology initiatives to big data and mobile solutions, as well as innovations in electronic payments, cybersecurity, machine learning, and cloud development. Our $9.5B+ annual investment in technology enables us to hire people to create innovative solutions that will not only transform the financial services industry, but also change the world.    At JPMorgan Chase & Co. we value the unique skills of every employee, and we’re building a technology organization that thrives on diversity.  We encourage professional growth and career development, and offer competitive benefits and compensation.  If you are looking to build your career as part of a global technology team, tackling big challenges that impact the lives of people and companies all around the world, we want to meet you.      ",Global Technology Infrastructure Data Center Operations Engineer I - Analyst,"$4,000to$8,000",Monthly,Professional,https://www.mycareersfuture.sg/job/global-technology-infrastructure-data-center-operations-engineer-analyst-jpmorgan-chase-bank-na-bed3f247755e2133360f5f735799f692
63,ALLEGIS GROUP SINGAPORE PRIVATE LIMITED,"SUNTEC TOWER FIVE, 5 TEMASEK BOULEVARD 038985",Contract,Information Technology,"Roles & ResponsibilitiesThis is a new initiative of the bank within a pioneer team to bring data quality into operations. The primary work comprises of ETL tool programming, data quality implementation, data governance and change & release management.  ","RequirementsData quality implementation experience 2) Preferably with Infomatica Data Quality tool OR other DQ tools like Trilium, IBM QualityStage or Microsoft data quality services(DQS) 3) Change and release management experience Experience with data warehousing and data mart development will be a good to have. Skills & Competencies: (free text) Technical skills refer to job-specific knowledge, skills and abilities (e.g. programming, 3D modelling, marketing, etc), Generic skills refer to skills that can be transferred from one job to another (e.g. communication, problem-solving). 1) Data quality implementation experience; 2) Preferably with Infomatica Data Quality tooling experience OR other DQ tools like Trilium, IBM QualityStage or Microsoft data quality services(DQS) 3) Strong change and release management experience Experience with data warehousing and data mart development will be a good to have.",Data Quality Engineer,"$9,000to$12,000",Monthly,Executive,https://www.mycareersfuture.sg/job/data-quality-engineer-allegis-group-singapore-0314d2030cb171e97a3b626d602a8f4b
64,ALLEGIS GROUP SINGAPORE PRIVATE LIMITED,"SUNTEC TOWER FIVE, 5 TEMASEK BOULEVARD 038985",Contract,Information Technology,"Roles & ResponsibilitiesThis is a new initiative of the bank within a pioneer team to bring data quality into operations. The primary work comprises of ETL tool programming, data quality implementation, data governance and change & release management.  ","RequirementsData quality implementation experience 2) Preferably with Infomatica Data Quality tool OR other DQ tools like Trilium, IBM QualityStage or Microsoft data quality services(DQS) 3) Change and release management experience Experience with data warehousing and data mart development will be a good to have. Skills & Competencies: (free text) Technical skills refer to job-specific knowledge, skills and abilities (e.g. programming, 3D modelling, marketing, etc), Generic skills refer to skills that can be transferred from one job to another (e.g. communication, problem-solving). 1) Data quality implementation experience; 2) Preferably with Infomatica Data Quality tooling experience OR other DQ tools like Trilium, IBM QualityStage or Microsoft data quality services(DQS) 3) Strong change and release management experience Experience with data warehousing and data mart development will be a good to have.",Data Quality Engineer,"$5,000to$9,000",Monthly,Executive,https://www.mycareersfuture.sg/job/data-quality-engineer-allegis-group-singapore-da13bc3e09061498bdcc9263e243ea86
65,DBS BANK LTD.,Unknown,"Permanent, Full Time",Banking and Finance,"Roles & ResponsibilitiesJob Purpose  The role forms part of the Insurance and Investment Platform technology team. Develop and deliver technology solutions relating to web and mobile applications across wealth customer segments.   Key Accountabilities  Engineer CI/CT/CD pipeline that is optimized to run within minutes Enforce best practices in code quality and release/deployment process to achieve near zero production incidents    Responsibilities  Architect, build and maintain continuous integration, testing and deployment (CI/CT/CD) pipeline for web and mobile apps Collaborate with architects, development engineers and system administrators to provision and maintain the platform infrastructure both on premise as well as cloud (for development, test and production environments) Build and maintain system and application health check and house-keeping jobs Troubleshoot system and connectivity errors and follow up with administrators, vendors or other teams for timely resolution Develop, maintain and document best practices in source control management and infrastructure as code  Track, maintain and renew infrastructure, web and mobile application key-stores and profiles Track, enforce and maintain code quality, security and performance reports Identify improvement areas and engage the required stakeholders to successful implement the changes Keep track of evolving technologies and perform proof of concept integrations for successful platform integrations as per roadmap Maintain platform collaboration tools such as JIRA and Confluence ","Requirements Experience and exposure to other DevOps related infrastructure software (such as ZABBIX, Puppet, Graphite, ELK stack, Kickstart, tcp wrappers, iptables, yum/apt-get) for classic sys admin functions (building hosts, monitoring, alerting, account management, releasing software, and security. Oracle, or MySQL 5.X, IBM DB2 (Any), SqLite (for Mobile devices), mariadb Build Tool: Ant, Maven, Gradle (Any) Version Control: CVS, SVN, GIT WebSphere Administration WebSphere MQ administration. DevOps – Jenkins/Bamboo, SonarCube, HP Fortify. Atlasian tools: Bitbuket, JIRA, Confluence, Bamboo and SharePoint Mobile application tools – Kony IDE, XCode, Android Studio, Ant, Maven, Gradle  AIX/Unix Administration Tomcat/IBM HTTP Server administration ","Senior Associate, DevOps Engineer, Group Consumer Banking and Big Data Analytics Tech (180004DY)","$5,000to$10,000",Monthly,Senior Executive,https://www.mycareersfuture.sg/job/senior-associate-devops-engineer-group-consumer-banking-big-data-analytics-tech-dbs-bank-4a3a66a54557b6de2aa9a5978bb99f4e
66,GOVERNMENT TECHNOLOGY AGENCY,"MAPLETREE BUSINESS CITY, 10 PASIR PANJANG ROAD 117438",Permanent,"Information Technology, Public / Civil Service","Roles & ResponsibilitiesThe Government Digital Services team is seeking an accomplished Data Engineer. We are a team in GovTech that aims to design and develop software applications that help government agencies to better serve the needs of Singaporeans. We adopt an Agile development approach and work towards adopting tech best practices and cutting edge tools.
We are looking for a savvy Data Engineer to join our growing team of analytics experts. The hire will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up.
The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.
What To Expect:

Create and maintain optimal data pipeline architecture.
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and ‘big data’ technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Keep our data separated and secure through multiple data centers.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems.

How To Succeed:

Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Experience supporting and working with cross-functional teams in a dynamic environment.
We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. Preferably with the experience of using the following software/tools:

Experience with big data tools: Hadoop, Spark, Kafka, RabbitMQ etc.
Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.
Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Experience with either of these languages: Python, Java.


",Unknown,Data Engineer,Salary undisclosed,Unknown,Unknown,https://www.mycareersfuture.sg/job/data-engineer-government-technology-agency-8580bddf71991ce3f59d7459c8c0451f
67,DBS BANK LTD.,Unknown,"Permanent, Full Time",Banking and Finance,"Roles & ResponsibilitiesJob Purpose  The Data Engineer will provide big data engineering support to the Institutional Banking Group (IBG) Business Analytics Team in various data science projects. This role’s primary job responsibility is defining the framework and process for preparing data for analytical uses. The right candidate will be one excited by the prospect of designing data engineering solutions from ground up and will support data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. Responsibilities  Create and maintain optimal data pipeline architecture; Assemble large, complex data sets that meet functional / non-functional business requirements; Identify, design, and implement internal process improvements: automating manual processes, Perform ETL/ELT, Data Modelling, Data Profiling, Data Cleansing, Feature Engineering tasks as part of Data Analytics Life Cycle (DALC); Work with stakeholders (data analysts, data scientists, technology support team) to assist with data-related technical issues and support data infrastructure needs; Build processes supporting data transformation, data structures, dependency and workload management ","Requirements Master’s Degree in software Engineering, Computer Science or related fields with minimum 3 years data engineering work experience in big data analytics environment Strong in data engineering skills with big data stack (Hadoop, Spark, Kafka, etc) Strong in transactional SQL, Enterprise Data Warehouse Experience with Graph Database, NoSQL databases Experience with Feature Engineering Experience with Master Data Management Experience with scripting languages: UNIX/Linux Shell, SQL, Python (Pandas, PySpark etc), Scala, R, etc ","AVP  /  Senior Associate, Data Engineer, IBG Digital, Institutional Banking Group (1800044U)","$5,500to$11,000",Monthly,"Manager, Senior Executive",https://www.mycareersfuture.sg/job/avp-senior-associate-data-engineer-ibg-digital-institutional-banking-group-dbs-bank-54bb338074df251516356bf7fb7992ca
68,MOKA TECHNOLOGY SOLUTIONS PTE. LTD.,Unknown,"Permanent, Full Time",Engineering,"Roles & ResponsibilitiesDo you have a passion for data? Are you looking to push the frontiers of innovation and build the Next Big Data Product? We are looking for excellent Data Engineers who are keen to help us manage the end-to-end data pipeline and drive big data solutions.  You will:  Design, implement and manage end to end data pipelines (ETL, data streaming and warehousing) so as to make data easily accessible for analysis. Integrate with third party APIs for accessing external data. Create and maintain data warehouses for reporting or analysis. Consult and partner with engineering and product teams to execute data-related product initiatives. Ability to quickly resolve performance and systems incidents. Evaluate the latest monitoring and automation tools. ","RequirementsYou have:  BS (MS preferred) in Computer Science or Computer Engineering. Excellent software engineering skills and proven track record (4+ years experience) in building automated, scalable and robust data processing systems. Proficiency in SQL, bash scripts and Python (or similar languages). Intermediate understanding of database technologies. Experience with data warehouse systems (e.g. Redshift) and batch/semi-online building blocks (e.g. MapReduce, Spark etc). Demonstrated expertise in working with large scale quantitative data. Excellent attention to detail and team player. ",Data Engineer,"$5,000to$7,500",Monthly,Professional,https://www.mycareersfuture.sg/job/data-engineer-moka-technology-solutions-9a2882ca74c6e52d5a8f33fbc7a5a572
69,NTT DATA SINGAPORE PTE. LTD.,"KEPPEL TOWERS, 10 HOE CHIANG ROAD 089315",Contract,Information Technology,"Roles & Responsibilities Own, develop and enhance the prototype cost allocation model for Singapore     Taking over the existing model and making further modifications / enhancements as required   Customise the prototype as required for rolling out the new cost allocation design to other countries     Implementing modifications / enhancements to the prototype as required for each country   ","RequirementsWe are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Information Systems. The candidate should also have experience using the following software/tools:  Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases. Experience building and optimizing 'big data' pipelines, data architectures and data sets. Experience with big data tools: Hadoop, Spark. Experience with object-oriented function scripting languages: Python, C++ Experience with statistical computer languages (Python, SQL, R) to manipulate data and draw insights from large data sets. Experience with common data science toolkits, such as NumPy, Pandas. R, etc. Knowledge of machine learning techniques and algorithms, such as K-NN, Naive Bayes, SVM, Decision Forests, etc. Experience with data visualisation tools, such as D3.js, GGplot, QlikView, etc. ",Data Engineer (Python developer),"$5,000to$7,500",Monthly,"Executive, Junior Executive, Senior Executive",https://www.mycareersfuture.sg/job/data-engineer-ntt-data-singapore-f652dac9e4485a097f8ae664ee69eb15
70,CHANDLER MACLEOD GROUP PTE. LTD.,Unknown,Contract,Information Technology,"Roles & ResponsibilitiesA well-established global brand is looking for a high calibre Mobile Network and Data Support Engineer in Singapore. This role is responsible providing technical support to improve customer’s experience with front-line support team. Ideally you should have experience helping users to troubleshoot network connectivity issues, experience with NOC and highly knowledgeable with IP networking.   Responsibilities:  Deliver technical support inline with a set of departmental service levels through a scaled ticketing systems Perform product integrations with mobile operators remotely, involving traffic zero rating Work with the mobile infrastructure engineering team, finding or escalating issues (TCP/IP, VPN, SMPP, SS7) Analysing protocol logs, add new IP address and network traces Troubleshooting networking and service issues on Linux based operating systems Liaise with carrier / operator partners on technical issues around connectivity ","RequirementsRequirements:  Bachelor degree in Telecommunications or related technical field   2+ to 5 years’ relevant experience in network engineering or telecommunications support environment Experience working in NOC environment Vast experience in IP networking, mobile networking, traffic analysis and troubleshooting    Ability to communicate effectively with a demonstrated ability to build valuable relationships  If this sounds like your next challenge get in touch -for a confidential discussion, please contact Naveen.Vasudevan@chandlermacleod.com Interested parties please click ""Apply Now"" or contact Naveen Vasudevan (EA Reg. No. R1330844) on +65 6429 3218 for more information. Chandler Macleod Group Pte Ltd, EA Licence: 11C3837",Mobile Network & Data Support Engineer,"$4,000to$8,000",Monthly,Senior Executive,https://www.mycareersfuture.sg/job/mobile-network-data-support-engineer-chandler-macleod-group-4aa227ac50f5f52ac797165b5726a72b
71,Company Undisclosed,Unknown,Permanent,Engineering,"Roles & ResponsibilitiesAs a Data Center Engineer in the Technical Operations group, the ideal candidate will have the knowledge and experience to work in a fast-paced Data Center environment. They will be responsible for the day-to-day operations and projects to ensure the continued success.   Conduct all day-to-day operations for the Data Center   Install all racks and enclosures for equipment   Prepare all equipment to be installed in racks and other enclosures   Update cable the management system as changes are made to the data center cable plant   Update asset management system as equipment is added and removed from the Datacenter   Define tasks for each project and work with all TechOps teams to meet project timelines   Provide status to team members and management on the completion of all tasks   Provide all information to assist with Data Center capacity planning for space and power   Manage all vendor resources to complete tasks as defined in SOWs   Research new Data Center infrastructure equipment advancements and recommend changes as needed   Conduct all audits as required by company policies   Coordinate with the warehouse the delivery and shipping of all equipment    ","RequirementsBasic Requirements:     3+ years in Data Centers managing hardware assets   Familiar with Linux   Demonstrable project management experience with the ability to manage tasks under tight deadlines   Extensive experience as a data center engineer working within large heterogeneous environments   Extensive experience with rack and stack of server and network equipment installations   Solid day-to-day experience working data centers and co-location environments   Experienced working fast paced engineering data center environments   Solid experience managing teams of onsite infrastructure build-out engineers and vendors   Experience with a large variety of different vendor hardware such as servers, storage, and network equipment.   Familiar with the processes of pulling, terminating and testing copper and fiber network cabling   Experience in contributing to detailed project plans for programs involving cross-functional teams and challenging requirements   Ability to multitask and manage multiple projects   Demonstrated ability to coordinate issue resolution across departmental teams   Coordinating vendors, scheduling RMA's and on-site repairs as needed   Experience with Asset Management Systems   Experience in managing inventory of all cable, power cords and other infrastructure items.   Solid understanding of disaster recovery and business continuance methodology   Strong sense of methodology, process, and metrics     Preferred Requirements:   Bachelor's degree or equivalent work experience preferred   Proven track record of success and delivering results   Strong interpersonal and relationship building skills conducive to team development   Excellent communication skills, both verbal and written   Physical work Ability to lift at least 50-70 lbs on a regular basis  ",DATA CENTER ENGINEER,Salary undisclosed,Unknown,Senior Executive,https://www.mycareersfuture.sg/job/data-center-engineer-1c61c2fecf9b58213b4cdb9d273848ff
72,DBS BANK LTD.,Unknown,"Permanent, Full Time",Banking and Finance,"Roles & Responsibilities Design and implement key components for highly scalable, distributed data collection and analysis system built for handling petabytes of data in the cloud.  Work with architects from other divisions contributing to this analytics system and mentor team members on best practices in backend infrastructure and distributed computing topics.  Analyze source data and data flows, working with structured and unstructured data. Manipulate high-volume, high-dimensionality data from varying sources to highlight patterns, anomalies, relationships and trends Analyze and visualize diverse sources of data, interpret results in the business context and report results clearly and concisely. Apply data mining, NLP, and machine learning (both supervised and unsupervised) to improve relevance and personalization algorithms. Work side-by-side with product managers, software engineers, and designers in designing experiments and minimum viable products. Build and optimize classifiers using machine learning techniques and enhance data collection procedures that is relevant for building analytic systems. Discover data sources, get access to them, import them, clean them up, and make them “model-ready”. You need to be willing and able to do your own ETL. Create and refine features from the underlying data. You’ll enjoy developing just enough subject matter expertise to have an intuition about what features might make your model perform better, and then you’ll lather, rinse and repeat. Run regular A/B tests, gather data, perform statistical analysis, draw conclusions on the impact of your optimizations and communicate results to peers and leaders. ","Requirements Experience in big data and machine learning The ability to work with loosely defined requirements and exercise your analytical skills to clarify questions, share your approach and build/test elegant solutions in weekly sprint/release cycles. Development experience in Java/Scala and pride in producing clean, maintainable code Practical experience in clustering high dimensionality data using a variety of approaches Real world experience in solving business problems by deploying one or more machine learning techniques Experience creating pipelines to analyze data, extracted features and updated models in production. Independence and self-reliance while being a pro-active team player with excellent communication skills. Hands-on development with key technologies including Scala, Spark, and other relevant distributed computing languages, frameworks, and libraries.  Experience with distributed databases, such as Cassandra, and the key issues affecting their performance and reliability.  Experience using high-throughput, distributed message queueing systems such as Kafka. Familiarity with operational technologies, including Docker (required), Chef, Puppet, ZooKeeper, Terraform, and Ansible (preferred).  An ability to periodically deploy systems to on-prem environments.  Mastery of key development tools such as GIT, and familiarity with collaboration tools such as Jira and Confluence or similar tools.  Experience with Teradata SQL, Exadata SQL, T-SQL Strong experience in graph and stream processing Experience in migrating SQL from traditional RDBMS to Spark and BigData technologies Experience in building language parsers using ANTLR, query optimizers and automatic code generation In-depth knowledge of database internals and Spark SQL Catalyst engine ","VP / AVP, Senior Data Engineer, Group Consumer Banking and Big Data Analytics Technology (180003L2)","$7,000to$14,000",Monthly,"Middle Management, Manager",https://www.mycareersfuture.sg/job/vp-avp-senior-data-engineer-group-consumer-banking-big-data-analytics-technology-dbs-bank-41369aada853ab1d604a4bd218031943
73,PALO IT SINGAPORE PTE. LTD.,51B CIRCULAR ROAD 049406,"Permanent, Full Time",Information Technology,"Roles & ResponsibilitiesYour profile & role on the project YOU:  Thrive on challenge. When was the last time you failed? Are curious & always learning. What are you up to right now? Can deal with constant change. When were you last surprised? Have mastered at least one skill of your trade but you’re not defined by it. What can you teach us? Can you wear many hats?  YOU AGAIN: The DevOps Architect will install, maintain, and support an on-premises cloud infrastructure and apply DevOps practices and solutions. The person will also implement cloud-related and DevOps technologies such as AWS/Puppet/Chef/Elk/Azure/Openstack. Other infrastructure related activities such as maintaining the company internal server infrastructure and respond to consultant requests when required will be expected.  Install, maintain, and support on-premises and off-premises cloud stack. Configure, maintain, and support the cloud-related infrastructures. Act as a system administrator on different OSes (e.g. RHEL, Opensolaris, Ubuntu, etc.) and help teams deploy their application and automate their development and releases on the cloud. Ability to develop solutions and self-learn new tools and technologies. Document, and share knowledge on developed DevOps solutions.  STILL YOU:  Unix / Linux / Bash knowledge Very good understanding of cloud computing (e.g. Technologies, Deployment, costing, HA/DR, etc.) Good understanding of DevOps principles (e.g. testing automation, BDD, TDD, Release automation, CI/CD, etc.) 2 years experience with cloud deployment (e.g. Openstack, VMWare, AWS, Azure, Terraform, etc.) 1 year experience with testing automation (e.g. Maven, Selenium, HP QC, LoadRunner) 1 year experience with release automation process (e.g. CA-RA, Jenkins, etc.) 1 year experience with Configuration Management (e.g. Ansible, SaltStack, Puppet/Chef, etc.) 1 year experience with monitoring tools (e.g. ELK, Prometheus, Grafana and Splunk) Experience with developing and implementing processes to handle releases from Development to Operations while respecting internal rules, and offering solutions for rollback) Experience with designing an architecture to implement development-to-production workflows. Knowledge of SRE, Containers, Kubernetes, Openshift is a plus. Good understanding of microservice architecture and DevOps practices that support. Strong RDMS and NoSQL skill in deploying and fine tuning such as MySQL, Oracle, Elasticsearch.  Your role at PALO IT You will be invited to take part in R&D works done within our Practices. You will have the chance to assist or be a speaker at must-attend international IT conferences. You will have the opportunity to write articles for our Blog or specialized press. Genuine ambassador of PALO IT, you will present our offers and take an active role in the development of the company.  Your technical environment # Cloud and DevOps based technologies (AWS/Puppet/Chef/Elk/Azure/Opencloud) # DevOps practices # Linux OS, Shell Scripting, SQL # Agile and scrum environment","Requirements✔     You hold a Bachelor, Master or PhD degree in IT, Information Management and/or Computer Science ✔     You are just graduated or have less than 3 years of working experience ✔     Good knowledge of big data technology landscape and concepts related to distributed storage / computing ✔     Experience with big data frameworks (e.g. Hadoop, Spark) and distributions (Cloudera, Hortonworks, MapR) ✔     Experience with batch & ETL jobs to ingest and process data from multiple data sources ✔     Experience with NoSQL databases (e.g. Cassandra, MongoDB, Neo4J, ElasticSearch) ✔     Experience with querying tools (e.g Hive, Spark SQL, Impala) ✔     Experience or willingness to go in real-time stream processing, using solutions such as Kafka, Flume and/or Spark Streaming ✔     You are passionate about technology and continuous learning comes naturally to you  ",Senior Database Consultant - Big Data Engineer,"$6,000to$12,000",Monthly,Professional,https://www.mycareersfuture.sg/job/senior-database-consultant-big-data-engineer-palo-singapore-0967c08bb2ea80dd96344490bdbe3da3
74,DBS BANK LTD.,Unknown,"Permanent, Full Time",Banking and Finance,"Roles & ResponsibilitiesJob Purpose    Build and improve machine learning and analytics platform. Work with data scientists to create, optimize and productionize of machine learning models for various business units within the organization. Keep innovating and optimizing data and machine learning workflow to enable data-driven business activities at large scale.    Responsibilities   Build and improve machine learning and analytics platform.       Apply cutting edge technologies and tool chain in big data and machine learning to build machine learning and analytics platform. Keep innovating and optimizing the machine learning workflow, from data exploration, model experimentation/prototyping to production. Provide engineering solution and framework to support machine learning and data-driven business activities at large scale. Perform R&D on new technologies and solutions to improve accessibility, scalability, efficiency and us abilities of machine learning and analytics platform.     Work with data scientists to build end-to-end machine learning and analytics solution to solve business challenges.       Turn advanced machine learning models created by data scientists into end-to-end production grade system. Build analytics platform components to support data collection, exploratory, and integration from various sources being data API, RDBMS, or big data platform. Optimize efficiency of machine learning algorithm by applying state-of-the-art technologies, i.e. distributed computing, concurrent programming, or GPU parallel computing.      Establish, apply and maintain best practices and principles of machine learning engineering.       Study and evaluate the state of the art technologies, tools, and frameworks of machine learning engineering. Contribute in creation of blueprint and reference architecture for various machine learning use cases. Support the organization in transformation towards a data driven business culture.     Work Relationships  Internal        Work closely with data scientists, business team, and project managers to provide machine learning and data-driven business solution.  Collaborate with other technology teams to build platform and framework to enable machine learning and data analytics activities at large scale     External        Maintain engineering principles and best practices of machine learning framework and technologies.   ","Requirements PhD/Masters/Bachelors in Computer Science, Computer Engineering, Statistics, Applied Mathematics, or related disciplines.  Excellent understanding of software engineering principles and design patterns. Excellent programming skills in either Python, Scala, or Java. In-depth understanding of data science and machine learning technologies and methodologies. Good working knowledge of high performance computing, parallel data processing, and big data stack, e.g. Spark and Hadoop/Yarn. Experience to one or more commercial / open source data warehouses or data analytics systems, e.g. Teradata, is a big plus. Experience to one or more NoSQL databases is a big plus. Hands-on experience in Cloud platforms, e.g. AWS, or containerization/ virtualization platforms, e.g. Docker/Kubernetes, is a big plus. Experience to any data science or machine learning platform, e.g. IBM Data Science Experience or Cloudera Data Science Workbench, is a big plus. Exposure to mainframe system is a plus. Passion about machine learning and data-driven intelligence system. Excellent communication and presentation skills in English. Team player, self-starter, ability to work on multiple projects in parallel is necessary. 2+ years of experience in machine learning system or data science research 5+ years of experience in software engineering or DevOps automation or data engineering Experience working in multi-cultural environments ","VP / AVP, Machine Learning Engineer, Group Consumer Banking and Big Data Analytics Tech (180003YE)","$7,000to$14,000",Monthly,"Middle Management, Manager",https://www.mycareersfuture.sg/job/vp-avp-machine-learning-engineer-group-consumer-banking-big-data-analytics-tech-dbs-bank-b3fa00eab03bc7ee01a600fd70041e2d
75,ENGIE ITS  PTE. LTD.,"ENTREPRENEUR BUSINESS CENTRE, 18 KAKI BUKIT ROAD 3 415978",Permanent,Engineering,"Roles & ResponsibilitiesResponsibilities: Manage data centre operations and facilities Plan and implement predictive and preventive programmes Manage sub-contractors to carry out maintenance works Project manage facility development Provide engineering/technical expertise and value engineering to customers Respond to service call and ensure resolution of the problems Prepare weekly reports, incident reports and O&M procedures Ensure critical system availability to meet SLA.","RequirementsRequirements:  Diploma / ITC Cert in Information Technology, Building Services, Electrical or Mechanical Engineering At least 1 years experience with Facility Management or M&E facilities maintenance, preferably in data centre environment Service-oriented with strong analytical and problem-solving skills Resourceful, dynamic, highly motivated and able to work independently Good interpersonal skills with ability to interact with people at different levels Willing to work overtime including weekends and public holidays when necessary  Personal Attributes  Meticulous and have an eye for details with positive working attitude Organized and details-oriented, with a strong focus on accuracy Able to work under pressure and tight deadline  Other Information:  Job Type: Full Time, Permanent Work week: 5 days Work location : Various location ",Data Centre Facilities Management Engineer,"$2,300to$4,000",Monthly,Junior Executive,https://www.mycareersfuture.sg/job/data-centre-facilities-management-engineer-engie-2f3848b3d78deee74eb96b79156e9b45
76,Company Undisclosed,"PENINSULA PLAZA, 111 NORTH BRIDGE ROAD 179098",Contract,Information Technology,"Roles & Responsibilities• Evaluate and renew implemented big data architecture solutions to ensure their relevance and effectiveness in supporting business needs and growth. • Design, develop and maintain data pipelines, with a focus on writing scalable, clean, and fault-tolerant code to handle disparate data sources, process large volume of structured / unstructured data from various sources. • Understand business requirements and solution designs to develop and implement solutions that adhere to big data architectural guidelines and address business requirements • Support and maintain previously implemented big data projects, as well as provide guidance and consultation on other projects in active development as needed • Drive optimization, testing and tooling to improve data quality • Document and communicate technical complexities completely and clearly to team members and other key stakeholders","RequirementsRequired to work European time zone (4pm-1am) • Degree qualified in Business management, IT, Computer Systems, software or computer engineering fields or equivalent. • Minimum 6 years of experience in data warehousing / big data environments. • Experience with big data processing  • Experience in designing and developing data models, integrating data from multiple sources, building ETL pipelines, and other data wrangling tools in big data environments • Understanding of structured and unstructured data design/modeling • Experience using software engineering best practices in programming, testing, version control, agile development, etc.     Technical competency: • Hadoop / Big Data knowledge and experience • Design & Development based on Hadoop platform and it’s components • Big Data Platform based on Cloudera on Hadoop • Python / Spark / Scala / Java • HIVE / HBase / Impala / Parquet • Sqoop, Kafka, Flume • SQL • Relational Database Management System (RDBMS) • NOSQL database • Data warehouse platforms or equivalent   Essential skill set: • Highly organized, self-motivated, pro-active, and able to plan. • Ability to analyze and understand complex problems. • Ability to explain technical information in business terms. • Ability to communicate clearly and effectively, both verbally and in writing. • Strong in User Requirements Gathering, Maintenance and Support. • Agile experience in a Scrum setting • Data Architecture, Data Modeling of BI Applications / Data Warehouse / Big Data",Big Data Engineer,"$6,500to$9,000",Monthly,Professional,https://www.mycareersfuture.sg/job/big-data-engineer-4cdf4a619b8c129fd5fa853a8f405a61
77,INTELLECT MINDS PTE. LTD.,"ANSON CENTRE, 51 ANSON ROAD 079904",Full Time,Information Technology,"Roles & ResponsibilitiesCompany Overview Intellect Minds is a Singapore-based company since 2008, specializing in talent acquisition, application development, and training. We serve BIG MNCs and well-known clients in talent acquisition, application development, and training needs for Singapore, Malaysia, Brunei, Vietnam and Thailand. Our client is an establish company a, leader within their industry, is now looking for a Data Engineer to join their esteemed organization. Job Descriptions: Responsibilities • Create and maintain optimal data pipeline architecture. • Assemble large, complex data sets that meet functional / non-functional business requirements. • Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Cassandra, Hadoop and other Big Data Technologies. • Build data pipeline on premise and on Google Cloud Platform. • Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics. • Work with stakeholders including the Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. • Work with data and analytics experts to strive for greater functionality in our data systems.","RequirementsQualifications • Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases. • Experience building and optimizing ‘big data’ data pipelines, architectures and data sets. • Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. • Strong analytic skills related to working with unstructured datasets. • Build processes supporting data transformation, data structures, metadata, dependency and workload management. • A successful history of manipulating, processing and extracting value from large disconnected datasets. • Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores. • Experience supporting and working with cross-functional teams in a dynamic environment. • We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have experience using the following software/tools: • Experience with big data tools: Hadoop, Spark, Kafka, etc. • Experience with Google Cloud Platform esp. Google Pub-Sub, Big Query, Data Proc, Data Flow, Cloud Storage. • Experience with IoT & Time series data. • Experience with relational SQL and NoSQL databases, including Postgres and Cassandra. • Experience with stream-processing systems: Storm, Spark-Streaming, etc. • Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc. All successful candidates can expect a very competitive remuneration package and a comprehensive range of benefits. Interested Candidates please submit your detailed resume online. To your success! The Recruitment Team Intellect Minds Pte Ltd (Singapore)",Data Engineer,"$5,000to$7,000",Monthly,Executive,https://www.mycareersfuture.sg/job/data-engineer-intellect-minds-5aefdcde0bb0f359967a5fdc27f414dd
78,AIA SINGAPORE PRIVATE LIMITED,"AIA TOWER, 1 ROBINSON ROAD 048542",Full Time,Information Technology,"Roles & Responsibilities You will be designing, developing and testing ETL Mappings, Mapplets, Workflows and Worklets You will be developing data pipelines to extract, transform and aggregate data that can scale to petabytes, elastically, with low latency and high availability. ","Requirements A background in Computer Science, Engineering, Mathematics or Statistics. 1 to 2 years of experience building complex data pipelines for data integration from enterprise wide applications/systems into centralised big data lakes/warehouses Open to fresh graduates who possess strong programming skill in java / python, SQL and Shell scripting ",Data Engineer,"$3,000to$6,000",Monthly,Executive,https://www.mycareersfuture.sg/job/data-engineer-aia-singapore-3e1790961b746514efe3771041fa1aba
79,SILOT PTE. LTD.,"VISION EXCHANGE, 2 VENTURE DRIVE 608526",Full Time,Sciences / Laboratory / R&D,"Roles & ResponsibilitiesRole   As data engineer at Silot, you design and implement distributed backend services that processes data in real-time, with focus on scalability, data quality and integration of machine learning.   You feel natural at extracting information out of heterogeneous data from many sources. You can plan, build and maintain distributed, service-oriented and event-driven data platform for real-time processing. You like delivering accurate data components that people rely on. You optimize architecture and processes for performance and stability.   To visualize what this position is like, think ""building systems,"" not ""processing data"" (even though your day will involve aspects of both).","RequirementsSkills & Qualifications   Requirements:  Excellent computer science knowledge Working experience in designing and building big data platforms (distributed filesystems, distributed databases, batch processing frameworks, stream processing frameworks, message queues, job scheduling) Expert knowledge of scalable architecture for real-time processing Experience ensuring data quality and compliance Experience with DevOps and continuous integration Experience in agile practices, test-driven development Fluent written and spoken English  Good to have:  Good documentation skills Experience in Fintech Experience with machine learning ",Data Platform Engineer,"$4,000to$7,000",Monthly,Professional,https://www.mycareersfuture.sg/job/data-platform-engineer-silot-11009083bc1a531502b38288c19fbe5a
80,TITANSOFT PTE. LTD.,90 EU TONG SEN STREET 059811,Permanent,Professional Services,"Roles & ResponsibilitiesIf you believe data makes the world go round, we believe we have found the one we are looking for. Our data and research team are the ultimate magicians of data. They throw data in the system, wave their hands around the keyboard, and pull out a never-ending stream of business value. Our team have opportunities to build efficient and reliable data pipelines that move data across systems. Our team are part mathematician, part computer scientist, and part interpreters- magicians of data. If you are interested to work some magic with our data, drop us an owl. What a Senior Data Engineer does in Titansoft  Partner with internal stakeholders to understand business requirements Work with cross-functional data and product teams to build efficient and scalable data solutions Design, build, optimize, launch and support new and existing data models in production Build scalable solutions of real-time data streaming and static analysis Setup network for deploy cluster and troubleshooting Write Linux script programming to assist in auto deploy and system health monitoring Design and build reliable Hadoop system ","RequirementsWhat we are looking for in a Senior Data Engineer Qualifications  BA / BS in Computer Science, Electronics or Electrical Engineering, Information Technology or other relevant fields  Experience  3+ years of experience in Unix / Linux operation systems (e.g. file systems, inodes, system calls) or networking (e.g. TCP / IP, routing, network topologies and hardware, SND) 2+ years of hands-on experience in the data warehouse space, custom ETL design, implementation and maintenance 3+ years of hands-on experience in SQL or similar languages and development experience in at least one scripting language (Python preferred) Experience with large data sets, Hadoop, and data visualization tools  Skills  Strong data architecture, data modeling, schema design Effective project management skills in leading data driven projects from definition to interpretation and execution Ability to initiate and drive projects, and communicate data warehouse plans to internal clients / stakeholders  What makes a (Super!) Senior Data Engineer in Titansoft  Expertise in designing and analyzing large-scale distributed systems (e.g. Hadoop, Kafka, Hive) Systematic problem-solving approach Ability to debug and optimize code and automate routine tasks ",Senior Data Engineer,"$4,000to$8,000",Monthly,Executive,https://www.mycareersfuture.sg/job/senior-data-engineer-titansoft-7ce2e0a5c4ed7a640e372253970f72e5
81,WOODPECKER ASIA TECH PTE. LTD.,137 TELOK AYER STREET 068602,Permanent,Information Technology,"Roles & ResponsibilitiesOur Data Engineer (in the analytics team) is the person responsible for enhancing our analytics and performance management framework. You’ll be building our data infrastructure - like databases and large-scale data processing tools -  on the Google Cloud Platform.  You’ll be a perfect fit in this role if you’re an eager learner, have prior experience in quantitative domains, and if you’re keen to be a team player in a dynamic start-up. You’ll get to:  Design, construct, install, test and maintain highly scalable data management systems Employ a variety scripting languages and tools to marry systems together Make sure our systems meet business requirements and industry practices Research opportunities for data acquisition and new uses for existing data Develop data set processes for data modelling, mining and production Integrate new data management technologies and software engineering tools into existing structures Create custom software components and analytics applications Install and update disaster recovery procedures Recommend ways to improve data reliability, efficiency and quality Collaborate with data architects, modelers and IT team members on project goals Build high-performance algorithms, prototypes, predictive models and proof of concepts ","RequirementsSkills:  2-3 years of working experience as a Data Engineering or as a developer Bachelor’s or Master’s degree in Computer Science Proficient in Java and in one of the scripting languages such as Python, JavaScript, Ruby or PHP Proficient with No-SQL databases like HBase or MongoDB Proficient in ETL processes and programming models on Apache Beam Proficient in cloud computing systems management services such as Stackdriver Familiar with cloud services such as MS Azure, Google Cloud Platform or AWS Familiarity with Machine Learning and Statistical techniques for data mining will be beneficial Familiarity with Google Analytics and Google Tag Manager will be an advantage Proficient in oral and written communication in English, as well as effective interpersonal skills ",Data Engineer,"$4,000to$8,000",Monthly,Professional,https://www.mycareersfuture.sg/job/data-engineer-woodpecker-asia-tech-33b8c0dff334c40cbd610e489ebf4e98
82,TITANSOFT PTE. LTD.,90 EU TONG SEN STREET 059811,Permanent,Professional Services,"Roles & ResponsibilitiesIf you believe data makes the world go round, we believe we have found the one we are looking for. Our data and research team are the ultimate managers of data. Others see meaningless figures but they see value. Our team are part of mathematician, part of computer scientist, and part of interpreters. Of data. What a Data Engineer does in Titansoft Manage data warehouse with plans for a business vertical or a group of business verticals Generate and manage all allocated data sets including ensuring its quality based on requirements Work with our Data Infrastructure team to triage and resolve infrastructure issues Manage the delivery of high impact dashboards and data visualisation diagrams","RequirementsWhat we are looking for in a Data Engineer Qualifications BA/BS in Computer Science, Electronics or Electrical Engineering, Information Technology or other qualified achievement Experience Hands-on experience in SQL or similar languages and development experience in at least one scripting language (Python preferred) Understand data architecture, data modeling and schema design What makes a (Super!) Data Engineer in Titansoft Patience to clean up huge amounts of data Passion to research domain knowledge Experience with large data sets, Hadoop, and data visualisation tools Interest in cloud computing and service (GCP, AWS, Azure) Interest in AI/ Machine-Learning product",Data Engineer,"$3,000to$8,000",Monthly,Executive,https://www.mycareersfuture.sg/job/data-engineer-titansoft-348181dcee42ac1c3b45c394fa314a43
83,BLUE STAR INFOSTACK SOLUTIONS PTE. LTD.,"ROBINSON SQUARE, 144 ROBINSON ROAD 068908",Contract,Information Technology,"Roles & ResponsibilitiesData Engineer  – Location Singapore    6+ years Experience in ETL / BI Technologies Hands on experience of writing complex SQL queries Experience with Amazon Kinesis, Hadoop, DynamoDB, Hive, and/or Spark a plus Understanding of data warehousing & databases is critical Ability to incorporate a variety of data sources in an analysis (HDFS, file, database, JSON, HTML, etc) Experience with data visualization tools (Tableau) Experience in requirement analysis, system design, development and testing Able to perform independent code reviews and execute unit tests on modules developed by self & other junior team members on the project. Expertise in coding, system testing, implementation and maintenance, performance tuning, go-live support and post-production support. Ensure the data is secure by creating and updating profiles, rules and Business reports timely. Strong communication and collaboration skills to understand customer needs and deliver solutions in alignment with business needs   ","Requirements 6+ years Experience in ETL / BI Technologies Hands on experience of writing complex SQL queries Experience with Amazon Kinesis, Hadoop, DynamoDB, Hive, and/or Spark a plus Understanding of data warehousing & databases is critical Ability to incorporate a variety of data sources in an analysis (HDFS, file, database, JSON, HTML, etc) Experience with data visualization tools (Tableau) Experience in requirement analysis, system design, development and testing Able to perform independent code reviews and execute unit tests on modules developed by self & other junior team members on the project. Expertise in coding, system testing, implementation and maintenance, performance tuning, go-live support and post-production support. Ensure the data is secure by creating and updating profiles, rules and Business reports timely. Strong communication and collaboration skills to understand customer needs and deliver solutions in alignment with business needs   ",Data Engineer,"$3,500to$6,000",Monthly,Executive,https://www.mycareersfuture.sg/job/data-engineer-blue-star-infostack-solutions-5bf603c975eef55f408defd7b6da76dd
84,CIMB BANK BERHAD,"SINGAPORE LAND TOWER, 50 RAFFLES PLACE 048623",Permanent,Information Technology,"Roles & Responsibilities Design, implement, and optimize the data pipelines from various channels using the latest technologies. Deploy dimensional data model to support functional and analytical business requirements. Partner with data analysts, business subject-matter experts and cross-functional technology teams to deliver end-to-end analytics solutions. Determine reporting needs to be integrated with business intelligence visualization tools and manage reports distribution via content management tools. Research, evaluate, and recommend technical solutions for data collection, processing, and reporting. Data cleansing, scraping unstructured data and converting them into structured/usable data. ","Requirements Bachelor Degree / Diploma in Computer Science, Information System or related studies. Minimum 5 years of experience in a data processing role. Consistent track record of designing and implementing scalable, performant data pipelines, data services, and data products. Proven ability to work well independently and within a fast-paced, collaborative environment. Excellent debugging, critical thinking and communication skills. Programming experience in building high quality data applications. Skills such as Java, Python or ASP.NET are preferred. Proficient in SAS, MS Excel and SQL Programming. Strong aptitude for learning new technologies related to Data Management and Data Science.  Please send detailed resume, including salary expectation and contact number to sg.enquiries@cimb.com. We regret that only shortlisted candidates will be notified.","Manager (Data Engineer), Decision Science, Credit Cards & Personal Financing","$5,000to$6,000",Monthly,Manager,https://www.mycareersfuture.sg/job/manager-decision-science-credit-cards-personal-financing-cimb-bank-berhad-33c003e1efee0089e56ee31524c47877
85,INFOGAIN PTE. LTD.,"ROBINSON SQUARE, 144 ROBINSON ROAD 068908",Contract,Information Technology,"Roles & ResponsibilitiesData Engineer  – Location Singapore  6+ years Experience in ETL / BI Technologies Hands on experience of writing complex SQL queries Experience with Amazon Kinesis, Hadoop, DynamoDB, Hive, and/or Spark a plus Understanding of data warehousing & databases is critical Ability to incorporate a variety of data sources in an analysis (HDFS, file, database, JSON, HTML, etc) Experience with data visualization tools (Tableau) Experience in requirement analysis, system design, development and testing Able to perform independent code reviews and execute unit tests on modules developed by self & other junior team members on the project. Expertise in coding, system testing, implementation and maintenance, performance tuning, go-live support and post-production support. Ensure the data is secure by creating and updating profiles, rules and Business reports timely. Strong communication and collaboration skills to understand customer needs and deliver solutions in alignment with business needs   ","Requirements  – Location Singapore    6+ years Experience in ETL / BI Technologies Hands on experience of writing complex SQL queries Experience with Amazon Kinesis, Hadoop, DynamoDB, Hive, and/or Spark a plus Understanding of data warehousing & databases is critical Ability to incorporate a variety of data sources in an analysis (HDFS, file, database, JSON, HTML, etc) Experience with data visualization tools (Tableau) Experience in requirement analysis, system design, development and testing Able to perform independent code reviews and execute unit tests on modules developed by self & other junior team members on the project. Expertise in coding, system testing, implementation and maintenance, performance tuning, go-live support and post-production support. Ensure the data is secure by creating and updating profiles, rules and Business reports timely. Strong communication and collaboration skills to understand customer needs and deliver solutions in alignment with business needs  ",Data Engineer,"$3,500to$6,000",Monthly,Executive,https://www.mycareersfuture.sg/job/data-engineer-infogain-4ddd284c9d10ef1c1072cc6a0c72aa15
86,DATASPARK PTE. LTD.,"COMCENTRE, 31 EXETER ROAD 239732",Permanent,Information Technology,"Roles & ResponsibilitiesResponsibilities  design and implement scalable and robust software platform for ingesting and transforming telco network datasets in (near) real-time using a variety of open-source and proprietary Big Data technologies recommend and implement ways to improve data reliability, efficiency and quality collaborate with product management, sales and marketing, and solution delivery teams to support the objectives that customer requirements are well managed and reflected in product releases support the deployment of DataSpark software within clients' IT environment working closely with stakeholders to ensure high standards of data governance during implementation serve as technical subject matter expert in latest big data technologies ","RequirementsRequirements  7+ years of superior software development experience building commercial large-scale software systems and database systems Excellence in algorithms, data structure, discrete math, data base and data warehousing Expert knowledge in data management technologies and software engineering tools to efficiently process large volume of data Demonstrated clear and thorough logical and analytical thinking, as well as problem solving skills Experience of data warehouses in excess of 10TB Experience of Web UI, middle tier, and data back end development Production coding experience in choice of programming languages and development frameworks Proven professional experience in processing large-scale commercial data. Experience with telco data a plus. Superior and proactive communications skills, including verbal, written, and presentation. A proven team player and contributor. Self-directed, ability to work independently and research innovative solutions to business problems Aptitude of working on multiple projects in parallel Attention to details and data accuracy MS or BS degree in Computer Science/Engineering, Statistics, Mathematics, or equivalent is required for this position. ",Data Engineer,"$3,500to$10,000",Monthly,"Executive, Senior Executive",https://www.mycareersfuture.sg/job/data-engineer-dataspark-e2b15cf3851973954e5d49b8515e1a38
87,DATASPARK PTE. LTD.,"COMCENTRE, 31 EXETER ROAD 239732",Permanent,Information Technology,"Roles & ResponsibilitiesResponsibilities  design and implement scalable and robust software platform for ingesting and transforming telco network datasets in (near) real-time using a variety of open-source and proprietary Big Data technologies recommend and implement ways to improve data reliability, efficiency and quality collaborate with product management, sales and marketing, and solution delivery teams to support the objectives that customer requirements are well managed and reflected in product releases support the deployment of DataSpark software within clients' IT environment working closely with stakeholders to ensure high standards of data governance during implementation serve as technical subject matter expert in latest big data technologies ","RequirementsRequirements  10+ years of superior software development experience building commercial large-scale software systems and database systems Excellence in algorithms, data structure, discrete math, data base and data warehousing Expert knowledge in data management technologies and software engineering tools to efficiently process large volume of data Demonstrated clear and thorough logical and analytical thinking, as well as problem solving skills Experience of data warehouses in excess of 10TB Experience of Web UI, middle tier, and data back end development Production coding experience in choice of programming languages and development frameworks Proven professional experience in processing large-scale commercial data. Experience with telco data a plus. Superior and proactive communications skills, including verbal, written, and presentation. A proven team player and contributor. Self-directed, ability to work independently and research innovative solutions to business problems Aptitude of working on multiple projects in parallel Attention to details and data accuracy MS or BS degree in Computer Science/Engineering, Statistics, Mathematics, or equivalent is required for this position. ",Lead Data Engineer,"$3,500to$10,000",Monthly,"Executive, Senior Executive",https://www.mycareersfuture.sg/job/lead-data-engineer-dataspark-4abea20ccd88d913146ec6ecbd387a27
88,DATASPARK PTE. LTD.,"COMCENTRE, 31 EXETER ROAD 239732",Permanent,Information Technology,"Roles & ResponsibilitiesResponsibilities  design and implement scalable and robust software platform for ingesting and transforming telco network datasets in (near) real-time using a variety of open-source and proprietary Big Data technologies recommend and implement ways to improve data reliability, efficiency and quality collaborate with product management, sales and marketing, and solution delivery teams to support the objectives that customer requirements are well managed and reflected in product releases support the deployment of DataSpark software within clients' IT environment working closely with stakeholders to ensure high standards of data governance during implementation serve as technical subject matter expert in latest big data technologies ","RequirementsRequirements  7+ years of superior software development experience building commercial large-scale software systems and database systems Excellence in algorithms, data structure, discrete math, data base and data warehousing Expert knowledge in data management technologies and software engineering tools to efficiently process large volume of data Demonstrated clear and thorough logical and analytical thinking, as well as problem solving skills Experience of data warehouses in excess of 10TB Experience of Web UI, middle tier, and data back end development Production coding experience in choice of programming languages and development frameworks Proven professional experience in processing large-scale commercial data. Experience with telco data a plus. Superior and proactive communications skills, including verbal, written, and presentation. A proven team player and contributor. Self-directed, ability to work independently and research innovative solutions to business problems Aptitude of working on multiple projects in parallel Attention to details and data accuracy MS or BS degree in Computer Science/Engineering, Statistics, Mathematics, or equivalent is required for this position. ",Senior Data Engineer,"$3,500to$9,000",Monthly,"Executive, Senior Executive",https://www.mycareersfuture.sg/job/senior-data-engineer-dataspark-afaa4ad5870b05c31d2738918ce5177c
89,ALPHATECH BUSINESS SOLUTIONS PTE. LTD.,Unknown,Permanent,Information Technology,Roles & Responsibilities Evaluate and recommend solutions via data analysis regarding issues related to the improvement of product qua;oty and resolving of customer feedback Apply software and programming abilities to manage and analyse data from a variety of sources ,"Requirements   Must know JAVA8 and SPARK Experience in distributed data architecture Have working knowledge of SQL, Python, Airflow Scala, Hadoop, SPARK Good to know CI/CD Experience (Jenkins Github), AWS, Kubernetes, Docker Preferred to have banking domain experience ",Data Quality Engineer,"$6,000to$7,800",Monthly,Senior Executive,https://www.mycareersfuture.sg/job/data-quality-engineer-alphatech-business-solutions-397e27060c6ed0fcabb3f578caac8432
90,A*STAR RESEARCH ENTITIES,Unknown,"Contract, Full Time",Sciences / Laboratory / R&D,Roles & ResponsibilitiesHe/She will mainly work on data preprocessing and preliminary analysis for dengue risk modeling research in this project. He/She will also be responsible for database construction and maintenance in this project and assist research scientist to fulfil the research tasks and deliverables.,"Requirements Master/Bachelor Degree in mathematics, engineering and electronics Good programming experiences and capabilities in python, c sharp, R and Java Experience in data analysis, parallel computing, optimization, agent-based simulation, and/or visualization is an added advantage ","Research Engineer (Big Urban Data), IHPC","$2,500to$5,000",Monthly,Non-executive,https://www.mycareersfuture.sg/job/research-engineer-ihpc-astar-research-entities-8b98f5f89638750fc0375acb68fd332e
91,Company Undisclosed,Unknown,"Contract, Full Time",Information Technology,"Roles & Responsibilities- Integration experience with backend product and booking engines - Creating complex applications, transforming user experience and enterprise - Working with some of the latest tools and techniques  - Hands-on coding, initially solely responsible developing services that construct applications front to back ie from UX to data acquisition and repositories  - Working in highly collaborative teams and building quality code - Working with senior business and technical colleagues to rapidly deliver solutions - Collaborating with and working under the direction of senior technical colleagues - Contributing to the technical design and architecture, particularly in service / microservice decomposition  - Knowledge in lots of different domains, programming languages and client environments - Furnish the business domain deeply and working closely with business stakeholders","Requirements- Degree in Information System or Computer Science related  - Must have knowledge in Protegrity and/or Blue Talon software  - 3 to 4 years of experience in banking environment  - Preferably trained in Agile methodology  - Proficient in Jira  - 3+ years of hands on experience in distributed data architectures is a must - 3+ years of Core Java or Scala or Python is a must  - Working knowledge of Apache Spark is a must  - CI/CD experience (Jenkins, Github) is a must  - Working knowledge of Bash scripts is a must  - AWS experience is nice to have  - Kubernetes or Docker experience is nice to have - Strong communication skills  - Previous work experience in Big Data environment is a plus - Understand Agile and full data warehouse flow - Understand database languages for analysing data out of data warehouse   Licence No: 12C6060",Data Security Engineer (Ref 22641),"$3,000to$6,000",Monthly,Executive,https://www.mycareersfuture.sg/job/data-security-engineer-e295250f4414c6bd07151ddc2e6ea469
92,DAIMLER SOUTH EAST ASIA PTE. LTD.,"WESTGATE TOWER, 1 GATEWAY DRIVE 608531",Full Time,Information Technology,"Roles & Responsibilities• Responsible for managing data sources, Data Ingest and Data Preparation processes including Metadata Management • Responsible for design of data models and to ensure they are in alignment with headquarter and global data models and scalable to other regions • Participate in Data Requirements Gathering and Analysis meetings with Team Members, Internal Customers, and Stakeholders. • Develop, maintain, test, and troubleshoot data solutions, including Database Development, ETL / Data Migration Development, and Big Data Development.   1. IT Solutions Delivery • Analyse complex, high-volume, high-dimensionality data from varying sources using a variety of ETL and data analysis techniques. • Responsible for managing data sources, Data Ingest and Data Preparation processes including Metadata Management • Responsible for design of data models and to ensure they are in alignment with headquarter and global data models and scalable to other regions • Participate in Data Requirements Gathering and Analysis meetings with Team Members, Internal Customers, and Stakeholders. • Develop, maintain, test, and troubleshoot data solutions, including Database Development, ETL / Data Migration Development, and Big Data Development. • Understanding and support of Regional overseas Data Lake and analytics engine and platform • Assist with enabling “Next best action / Offer and data insights” solutions that are scalable and transferable for regional markets • Investigation and understanding data landscape for the respective markets • Ability to recognize/analyse highly complex processes, interdependencies and gaps and to develop new approaches to solutions • Supports and troubleshoots the data movement processes and the data warehouse environment • Promote synergies and reuse within and across projects and platforms in order to maximize rapid yet responsible delivery   2. IT Strategy and Strategic Supplier Management (SSM) • Support in lean implementation and managing run costs downwards. • Support in sourcing and supplier management, liaise with ITx on SSM • Sharing and collaboration of solution across all regions • Ensure that standardized solutions are delivered • Ensure consistency of the overall Overseas IT landscape • Conduct post-implementation assessment to ensure that required value was delivered   3. Coordination & Communication • Management reporting and provide training on the platform architecture and strategy topics • Build-up and strengthen relationship with all stakeholders in the region, headquarter and the markets to effectively perform the necessary tasks within the function • Ability to explain complex processes clearly and precisely to different target groups and to explain the specific benefits of solution approaches  ","Requirements• Bachelor’s Degree in Business or IT Qualification • 2+ years’ relevant experience, minimum 5 years of working experience • Specific knowledge: People Management knowledge; IT knowledge; Process knowledge       • Experience with Azure Data Lake, Azure Data Lake Analytics, Azure Data factory, HDInsights. • Experience with design and implementation of HDInsights with Spark, Kafka clusters. • Experience with SQL Server Integration Services, Reporting Services and Analytic Services. • Familiar with Microsoft Azure data storage, ingestion, computation services & APIs. • Understands the common data movement architectures (like ETL, ELT, etc.) • Strong experience with Metadata Management, establish sourcing and access patterns for enterprise reference data. • Experience with data extraction and manipulation, and ad-hoc query tools • Highly motivated independent worker with minimum guidance required; • Readiness to travel; (20% of travel) • Ability to work in an international and intercultural context • Excellent communication skills in both written and spoken. Multi-linguistic abilities would be an added advantage.",Assistant Manager - Data Engineer,"$6,000to$13,000",Monthly,Professional,https://www.mycareersfuture.sg/job/assistant-manager-data-engineer-daimler-south-east-asia-30160a980e2c75182c6e3bded6c6c8a1
93,FOX NETWORKS GROUP SINGAPORE  PTE. LTD.,"NEXUS @ONE-NORTH, 1 FUSIONOPOLIS LINK 138542",Permanent,Information Technology,"Roles & ResponsibilitiesThe Role: We are looking for a DevOps Engineer (Data, Analytics & Marketing Technology) that will work on the integration & improvement of our Analytics, Business Intelligence, and Customer Marketing platforms. The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them. You will also be responsible for integrating them with the architecture used across the company.   Reporting To:  Head of Product, FOX+ (Asia)   Key Responsibilities:   Selecting and integrating any Big Data tools and frameworks required to provide requested capabilities Working with the Business Intelligence team to develop & maintain solutions for data consolidation, surfacing, and visualisation of Analytics & Data Translating business requirements from Marketing, Content, Product & Business Intelligence teams into technical specifications for implementation by relevant engineering teams Managing implementation & integration of analysis & visualisation tools Owning the technical definition & best practice for Analytics & tracking implementation into a suite of applications incl. Mobile, web & leanback. Implementing ETL process Monitoring performance and advising any necessary infrastructure changes Defining data retention policies   ","RequirementsMust-haves  Minimum 3 years working knowledge with any scripting language (i.e. Javascript/Node, Python, Go) Minimum 3 years working knowledge of any RDS such as PostgreSQL, MySQL. Understands various network transport protocols, and data file formats. Experience with implementing Client SDKs in a mobile environment. Experience with Implementing Database infrastructures in AWS using RDS, elasticache, redshift, and aurora. Proficient understanding of distributed computing principles Experience with the Implementation & Management of Amazon Redshift Working knowledge of ETL in a Data Warehousing Context Experience with the concept of analytics aggregation, or operation of tools such as Segment.com Experience with at least one of these tools: PowerBI, Tableau, Google Analytics, or New Relic Insights. Experience with the implementation & Management of Marketing platforms, e.g. Braze, Adobe, Oracle    Ideal & Preferred  Certification in any enterprise RDS programmes. Experience with NoSQL databases such as HBase, Cassandra, or MongoDB Working knowledge of AWS Lambda. Proficiency with Implementing and Maintaining Hadoop v2, MapReduce, or HDFS Experience in Spark, Pig, Hive, or Impala Experience with various messaging systems, such as Kafka or RabbitMQ Experience with building stream-processing systems, using solutions such as Storm or Spark-Streaming Experience with Big Data ML toolkits, such as Mahout, SparkML, or H2O Experience with Cloudera/MapR/Hortonworks ","Data, Analytics & Marketing Technology Engineer",Salary undisclosed,Unknown,Manager,https://www.mycareersfuture.sg/job/data-analytics-marketing-technology-engineer-fox-networks-group-singapore-e7d8281e9c60c1456b8efe30459eaf61
94,Company Undisclosed,Unknown,Full Time,"Engineering, Information Technology, Repair and Maintenance, Telecommunications","Roles & Responsibilities knowledge/familiar in Network device/cabling /server/cabinet/rackesponsible for designing, setting up, and managing information/network systems monitoring systems operations and administering IT solutions to ensure servers, hard drives, and other data center equipment function efficiently. carry out performance-tuning operations on data center storage systems to ensure high level of data quality, availability, and security. ",Requirements    at least diploma with min. 2years experience in Data Center  3 rotating shifts ,Data Center Engineer,"$3,000to$4,500",Monthly,"Middle Management, Fresh/entry level",https://www.mycareersfuture.sg/job/data-center-engineer-cbc6715f9ec705687a0e5ebab573d710
95,LAZADA SOUTH EAST ASIA PTE. LTD.,"AXA TOWER, 8 SHENTON WAY 068811",Permanent,Information Technology,Roles & ResponsibilitiesWe are looking for a Data Engineer that will be responsible to gather public information from the different players in the e-Commerce industry and consolidate them into a dashboard to get insights. You will be conducting strategic analysis to improve the products that are promoted on Lazada. What you will do:  You will work with Big Data information - structuring different inputs into a dashboard that is understandable for local category teams to action on You will work with APIs and bots technology You will be the middleman between Business requirements and implementation You will improve the Lazada platform to get information from different websites and social networks ,Requirements Bachelor’s Degree in Computer Science/Information Engineering or relevant technical field experience Minimum of 3 years of related professional work experience A great communicator Love code and open to learn new technologies Understand how Big Data databases works and how to get the information from it in a efficient way A decision maker. You will work in a environment with many stakeholders and you will have to understand and make decision in based of tradeoffs  ,"Manager, Data Engineer","$5,000to$8,000",Monthly,Manager,https://www.mycareersfuture.sg/job/manager-data-engineer-lazada-south-east-asia-422f4112d8c3919982385233d71ff070
96,SINGAPORE AIRLINES LIMITED,"AIRLINE HOUSE, 25 AIRLINE ROAD 819829",Permanent,Information Technology,"Roles & ResponsibilitiesSIA has multiple positions for junior and senior data scientists to drive our business analytics, data science and AI initiatives. Responsibilities include the following:  Member of an in-house data analytics and AI development team that works on machine learning (including NLP, image recognition, recommender system, deep learning), experimental design, and optimization. Oversee the technical work and provide datasets to external technology partners to deliver products/services in data analytics, data science and AI. Support business users in the assessment/validation of partner-supplied prediction models and in their deployment to production. Help business units create Tableau dashboards with relevant datasets. Extract insights through data visualization. Work closely with application development teams to operationalize and integrate analytics/machine learning capabilities into production systems using RESTful-API microservices. ","Requirements BS in Computer Science, Mathematics, Statistics, Physics or related discipline is required. Advanced degree related to analytics, machine learning or AI is preferred. Intermediate or advanced programming skills in Python. Conversant with algorithm design, data structure and SQL. Functional/object-oriented software development experience using Java or Scala is a plus. At least 2 years of relevant industry experience in two or more of the following areas: 	 Hands-on skills in shallow machine learning, AI, or information retrieval. Experience in GPU-accelerated deep learning frameworks (such as Keras, TensorFlow, PyTorch) is a plus. Knowledge and working experience in workflow, map-reduce or stream processing systems such as Spark and Kafka. Knowledge in statistics, especially Bayesian statistics and inference. Knowledge and working experience with data visualization tools like Tableau, PowerBI and Qlik.   Experience with Agile/Scrum/Kanban methodologies is a plus. Hands-on experience with AWS, GCP or similar public cloud environment is a plus. Excellent interpersonal & communication skills to work with non-technical business users. Proven ability as a problem-solver.  ",Data Sciences & Analytics Engineer,"$4,000to$8,000",Monthly,"Professional, Executive",https://www.mycareersfuture.sg/job/data-sciences-analytics-engineer-singapore-airlines-bd7e3e2c924ad2085f77bfbcd9721c05
97,ITCAN PTE. LIMITED,"PRUDENTIAL TOWER, 30 CECIL STREET 049712",Full Time,Information Technology,"Roles & Responsibilities1. Designs and creates information store / data warehouse and all related extraction, transformation and load of data functions 2. Experts at taking a big-picture view of a company's data situation 3. Proficient in able to read, analyze and digest what a business wants to accomplish with its data, understand data models and translate them into the best possible design of Databases and ETL processes 4. Executes basic and advanced transformation activities such as normalization, cleansing, aggregation, summarizing, and integration 5. Designs automation processes to control data access, transformation and movement; ensures source system data availability and update accessibility, data integrity, restorability and appropriately handles errors in a timely manner 6. Develops logical and physical data flow models for ETL applications. Translates data access, transformation and movement requirements into functional requirements and mapping designs 7. Plans and conducts ETL unit and development tests; monitors results and takes corrective action, when necessary 8. Plans, coordinates and implements security measures to safeguard data in information store / datawarehouse against accidental or unauthorized damage, modification or disclosure 9. Analysis of business requirements document created by Data Architect team 10. Create SSIS packages based on mapping document 11. Unit test & optimize SSIS package according to business needs 12. Document technical details of ETL project in Excel & word documents 13. Code maintenance in TFS (Branching/Merging) 14. Contact Data Architects for any queries/confusions/design changes for IS","Requirements. Bachelor’s degree or equivalent in relevant discipline. 2. Minimum 5 yrs. exp. working on MSBI Stack (SSAS, SSIS, SSRS) 3. Minimum 5 yrs. exp. working on SQL Server, DB2, Oracle databases 4. Working knowledge of modeling tools (Erwin, Embarcadero, EA Sparx etc.) in addition to hands-on experience with business intelligence tools 5. Mandatory Technical expertise in:  MS SQL Server 2008 R2 - FastTrack Data Warehouse Databases - DB2, SQL Server, Oracle (Must be able to write complex queries against these DBs) SQL Server Integration Services (Writing efficient SSIS packages) Any data replication tool (preferred: MS SQL Server, DT Share) Writing complex & efficient Stored Procedures, Views, Functions in MS SQL Server Optimizing complex SQL queries for optimum performance Setting up schedule jobs using Windows Scheduler or SQL Agent Any Data Quality Tools - Preferred: TIBCO Trillium, MS DQS Any Master Data Management Tools - Preferred: TIBCO MDM, MS MDS  6. Technical expertise in the following areas are a plus:  ETL experience with SAS Enterprise Guide ",Data Engineer - MSBI Stack,"$5,000to$7,000",Monthly,Professional,https://www.mycareersfuture.sg/job/data-engineer-msbi-stack-itcan-93351a0bf0e614e714dedb5d1250e2c0
98,HCL SINGAPORE PTE. LTD.,"AXA TOWER, 8 SHENTON WAY 068811",Permanent,Information Technology,"Roles & ResponsibilitiesThe Data center hands and feet engineer will Perform day to day activities, users, visitors and subscriber’s co-ordination. Co-ordinate with visitors visiting the DC for activities Will co-ordinate with respective teams with in a project Will be responsible for handling issues pertaining to operations and production Will be responsible to manage incidents during the shift Will be part of DC hands and feet operation team to work for customer on different DC member sites (Singapore) in the project and performing equipment Installation and Decommission, troubleshooting. Travel to different sites for INC/Planned activities Will be installing and decommissioning equipment’s, servers racking and stacking in the Datacentre Will be performing DC Cabling, labelling, patching Daily DC server health checks, walkthroughs, reporting, printing, remedy and email queue management Must have DC Cabling and equipment hardware troubleshooting knowledge Should have basic understanding of DC facilities, environment and standards Receiving the material deliveries, maintaining the inventory store room Maintaining visitor records & access logs 24*7 environment, shift rotation Flexibility per team rotation requirement Professional in 24/7 operational team availability  ",RequirementsHands on with DC infrastructure environment Good Experience in DC 24*7 Operation process & procedures DC racking and stacking & structured cabling understanding GOOD to HAVE Any Technical certification DC Certification Technical / Professional Skills Experience working with external Vendors Good communication and interpersonal skills Non-Technical / Soft Skills Excellent team player flexibility in 24*7 environment Maximum availability for operational requirements  ,Data Centre Operations Engineer,"$4,500to$5,000",Monthly,Professional,https://www.mycareersfuture.sg/job/data-centre-operations-engineer-hcl-singapore-62dbbb34399b6d8a59ef894555a48b65
99,ITCAN PTE. LIMITED,"PRUDENTIAL TOWER, 30 CECIL STREET 049712","Contract, Full Time",Information Technology,"Roles & Responsibilities1.       To take care of the function of the external service of the server room, which includes floor plan and layout of the server room, electrical equipment access management including UPS, MDB and DB, and on-site service management of the server room; 2.       Responsible for the daily operation and maintenance of power supply and all the cabling of the server room, and ensure a stable operation environment through implementation and management of strict and standard daily operation and maintenance on site. 3.       Responsible for capacity management of the server room, including floor plan and rack layout, ensuring optimal estate space usage of the server room; 4.       Responsible for coming up with the standard operating procedures for server room service management, and implementing the work quality management internally according to the standard to ensure improvement of work quality, timeliness and satisfaction; 5.       Responsible for daily operation and maintenance information management of the server room, identifying and providing server room space, power analysis report regularly, resolving all issues both existing and future. 6.       Serve as the duty leader, responsible for all work and personnel during the shift, and ensure the quality of work.","Requirements One year and more working experience in server room operation and maintenance is preferred. Related knowledge of computer foundation, server room operation, infrastructure and strong and weak current.  Have data analysis and report generating skills  Willing to work base on shift  ",Data Center Engineer,"$2,500to$4,500",Monthly,Executive,https://www.mycareersfuture.sg/job/data-center-engineer-itcan-bc2dcbf70c1b0339a2c55059d0dc24ad
100,PROPERTYGURU PTE. LTD.,"TRIPLEONE SOMERSET, 111 SOMERSET ROAD 238164","Permanent, Full Time","Engineering, Information Technology","Roles & ResponsibilitiesOur websites attract more than 100 million monthly page-views which result in non-stop massive click-stream for behaviour data with around 130 million of users. As a result, PropertyGuru has the most comprehensive data for property supply and demand in Southeast Asia. Our Data Science team is empowered to build unique and compelling user experiences using machine learning and data.  Your work will be to design, develop, support and maintain our existing infrastructure and democratizing data access within and outside the company. Your work will support the following areas:  Real time streaming infrastructure: 	 Enable teams to move quickly and get accurate information to the right people with minimum delay would be the key focus of the data engineering team Advance our streaming platform which allows easy development of the streaming applications   Interactive Data Analytics: 	 Query the data and compute the aggregates on various dimensions to support the various decisions made on the data and machine learning products that are built around it   Infrastructure management: 	 Help manage multiple terabyte-scale clusters, easy-to-use systems to handle security and replication are in development   Data workflow management: 	 Use Airflow and Azkaban to schedule data related workflows supporting various analytical and machine learning workloads   Machine learning infrastructure: 	 Develop the end-to-end platform that will allow us to develop and deploy various machine learning models into the PropertyGuru websites and apps with ease Data engineers play a very big role in this platform development and has the potential to significantly cut down the development time of the machine learning models   ","Requirements Bachelor’s degree in IT or relevant field. Alternatively, lesser qualifications with strong experience in machine learning will also be considered  2+ years of industry experience in working with terabyte scale datasets Working knowledge of relational databases and query authoring (SQL) Experience with data workflow management tools such as Azkaban, Airflow Ability to write high performance quality code Experience in Python is a must. Other equivalent languages like C++, Java, Go, Scala is a plus. Experience with open source technologies like Kafka, Presto and Spark would be a plus  Awareness of various cloud-based solutions such as AWS Redshift, Google Big Query, Qubole is a plus  Visit us at: htps://www.propertygurugroup.com/",Data Engineer,"$5,000to$7,000",Monthly,"Professional, Executive",https://www.mycareersfuture.sg/job/data-engineer-propertyguru-82e462a13cadc477f93d57ad6812d1d1
101,PROPERTYGURU PTE. LTD.,"TRIPLEONE SOMERSET, 111 SOMERSET ROAD 238164","Permanent, Full Time","Engineering, Information Technology","Roles & ResponsibilitiesOur websites attract more than 100 million monthly page-views which result in non-stop massive click-stream for behaviour data with around 130 million of users. As a result, PropertyGuru has the most comprehensive data for property supply and demand in Southeast Asia. Our Data Science team is empowered to build unique and compelling user experiences using machine learning and data.  Your work will be to design, develop, support and maintain our existing infrastructure and democratizing data access within and outside the company. Your work will support the following areas:  Real time streaming infrastructure:     Enable teams to move quickly and get accurate information to the right people with minimum delay would be the key focus of the data engineering team Advance our streaming platform which allows easy development of the streaming applications   Interactive Data Analytics:     Query the data and compute the aggregates on various dimensions to support the various decisions made on the data and machine learning products that are built around it   Infrastructure management:     Help manage multiple terabyte-scale clusters, easy-to-use systems to handle security and replication are in development   Data workflow management:     Use Airflow and Azkaban to schedule data related workflows supporting various analytical and machine learning workloads   Machine learning infrastructure:     Develop the end-to-end platform that will allow us to develop and deploy various machine learning models into the PropertyGuru websites and apps with ease Data engineers play a very big role in this platform development and has the potential to significantly cut down the development time of the machine learning models   ","Requirements Bachelor’s degree in IT or relevant field. Alternatively, lesser qualifications with strong experience in machine learning will also be considered  2+ years of industry experience in working with terabyte scale datasets Working knowledge of relational databases and query authoring (SQL) Experience with data workflow management tools such as Azkaban, Airflow Ability to write high performance quality code Experience in Python is a must. Other equivalent languages like C++, Java, Go, Scala is a plus. Experience with open source technologies like Kafka, Presto and Spark would be a plus  Awareness of various cloud-based solutions such as AWS Redshift, Google Big Query, Qubole is a plus  Visit us at: htps://www.propertygurugroup.com/",Data Engineer,"$7,000to$9,000",Monthly,"Professional, Senior Executive",https://www.mycareersfuture.sg/job/data-engineer-propertyguru-763708ca5a581db4389a766ef71654a0
102,NEURONCREDIT PTE. LTD.,Unknown,Full Time,Information Technology,"Roles & ResponsibilitiesJob Responsibilities  The Senior Data Engineer role requires maintaining data platforms, data pipeline and ETL, implementing AI models, and integrate our internal services. You will work with data scientists and back-end engineers in the team.","RequirementsJob Requirements  B.Sc./M.Sc. in Computer Science or a related field At least 3-year experience as a Data Engineer or Backend Developer Hands-on development experience in python, Java and Scala Familiar with MySQL/Mariadb, MongoDB, and Redis. Hands-on experience of building applications and infrastructure. Hadoop/Spark eco-system or any other big data technologies is a plus Experience in Docker and K8s is a plus. Meticulous, responsible, a strong team player ",Data Engineer,"$5,000to$9,000",Monthly,Executive,https://www.mycareersfuture.sg/job/data-engineer-neuroncredit-0f8b1db000a42e246f0c42e728953f8f
103,ZUZU ROOMS PTE. LTD.,92 AMOY STREET 069911,Full Time,Engineering,"Roles & ResponsibilitiesJoin a visionary team with tremendous experience in the travel industry focused on delivering a product that will revolutionize the way hotels organize their business processes. If you are committed to the highest levels of achievement in cutting-edge engineering, we welcome you to help write the next chapter of our history. We're currently looking to hire an experienced Data Engineer  to join our team. The ideal candidate will handle a wide range of exciting tasks from prototyping new techniques and technologies, to implementing real world, customer-focused cloud-based solutions in markets that are ripe for picking As an early-stage start-up, the role will be very dynamic.  As such we need someone smart, quick-minded, ambitious, and flexible.  The role will grow as you grow!  ","RequirementsResponsibilities: We are looking for a Sr. Data Engineer  to work on cloud and SaaS technologies. You will be responsible for the following:   Work on the collecting, storing, processing, and analyzing of huge sets of data.   Choose optimal solutions leveraging existing big data frameworks, then implement the infrastructure and  ETL processes, maintain and monitor them   Integrate these solutions with the existing architecture   Integrate these solutions with the existing CI/CD pipeline and automation   Propose architecture changes to handle the creation and consumption of big data.   Define data retention policies   Maintain high quality code   Investigate and, if necessary, prototype technologies relating to the task     Qualifications:   5-7 years of software development and data engineering experience   Proficient understanding of Big Data principles   Experience with integrating data from multiple sources and building Data Warehouses/ Data Lakes   Experience building stream processing systems leveraging Apache Storm, AWS Kinesis or Spark Streaming   Experience with Spark   Experience building and managing Hadoop clusters and proficient MapReduce and HDFS   Experience with NoSQL databases like MongoDB, DynamoDB , HBase or Cassandra   Knowledge of ETL techniques and frameworks like Flume or AWS Glue   Experience with  task schedulers like Airflow   Experience with ML toolkits like SparkML or H2O   Good understanding of Serverless Architecture   Ability to design and develop reusable and robust components and services   Experience with CI/CD using Jenkins is a plus   Team player with Agile development experience   BS or MS in Computer Science or related technical field  ",Senior Data Engineer,"$6,000to$10,000",Monthly,Professional,https://www.mycareersfuture.sg/job/senior-data-engineer-zuzu-rooms-4e3584148bdf0237093b107030de7abd
104,HELIX LEISURE PTE. LTD.,"YELLOW PAGES BUILDING, 1 LORONG 2 TOA PAYOH 319637",Permanent,Information Technology,"Roles & ResponsibilitiesHelix Leisure is a leading global supplier to the Out of Home Entertainment industry – locations outside the home people visit for entertainment and recreation. Across our core brands – Embed (revenue management systems, e-commerce), Booking Boss (Tours, Attractions and Activities), LAI Games (arcade games), The Locker Network (operating electronic lockers) and Matahari Leisure (equipment manufacturing) we service over 2,500 locations around the globe. Helix operates full service offices in Singapore, Perth, Sydney, Dallas, Dubai and Jakarta. The group enables our customers to create rich experiences for their visitors and guests through both technology and service. As we embark on building the next generation platform for our software – a core consumer, supplier and distributor facing application, we are looking for highly motivated professionals who enjoy working in a fast paced, agile development environment. You will be working closely with product owners and UX designers to create and develop best-in-class data service solutions with the ability to use the latest in web development technology. Responsibilities:  Design, develop, test, deploy, maintain and improve software Manage individual project priorities, deadlines and deliverables ","RequirementsSkills & Qualifications:  BS degree in Computer Science preferred, similar technical field of study or equivalent practical experience will be considered Strong Core Java Development Experience Experience working with three or more from the following: web application development, Unix/Linux environments, distributed and parallel systems, machine learning, information retrieval, natural language processing, networking, developing large software systems, and/or security software development Working proficiency and communication skills in verbal and written English Strong TSQL knowledge, able to optimise queries and understand unoptimised query plans Real Time, Multithreaded experience Experience with ORM tools such as hibernate Experience building high-volume file processing systems Experience with payments/transactions Experience in an Agile development environment. DBA experience Effective teamwork and good communication skills with the ability to mentor peers and provide peer code-reviews Ability to work effectively with software engineers to enhance test plans and automated testing framework ","Software Engineer, Data Services","$5,000to$9,000",Monthly,Senior Executive,https://www.mycareersfuture.sg/job/software-engineer-data-services-helix-leisure-9c20bde69b3f7d47ff97b3ac298ac59f
105,SCHELLDEN GLOBAL PTE. LTD.,"INTERNATIONAL PLAZA, 10 ANSON ROAD 079903",Full Time,Information Technology,"Roles & ResponsibilitiesYou will be responsible for end to end development of Data Analytics use cases, company Data Lake platform and tools. The role must be hands on and will be willingness to work as DevOps model. What You’ll Do:  Massive data: You will source / examine, analyze, engineer data pipelines for gigabytes/terabytes of structured and unstructured data with our platform to create value for customers.  Pushing the limits: This role will be on the cutting edge of our Data / Machine Learning platform. As we push to solve more of our customer challenges, you will be prototyping new features, tools and ideas. Innovate at a very fast pace to maintain our competitive edge.  Linux hacking: You will be masterfully using the command line, including tools like vi/emacs and understanding beyond basics of grep, bash, awk, sed, etc to aggressively dive into data, systems, and compute platforms to get the results you are seeking.  Production deployment: You will be responsible for integration and deployment of the machine learning pipelines into production where your ideas can come to life.  Coordinate and work with cross functional teams, sometimes located at different geo locations.","RequirementsWhat you have done:  Commercial software engineering: You have 3+ years of professional software development experience with languages and systems such as Java, Python (PySpark), and version control (git), with good analytical & debugging skills.  Big data: You have extensive experience with data analytics and working knowledge of big data infrastructure such as Hadoop Eco System, HDFS, Spark, Google Cloud, Big Query, Data Flow (nice to have). You've routinely built data pipelines with gigabytes/terabytes of data and understand the challenges of manipulating such large datasets.  Data Modeling: Flair for data, schema, data model, PL/SQL, Star & snow flake schema, how to bring efficiency in data modeling for efficient querying data for analysis, understands criticality TDD and develops data validation techniques.  Real Time Systems: Understands evolution of databases for in-memory, NoSQL & indexing technologies along with experience on real-time & stream processing systems like kafka, Storm.  Project management: You demonstrate excellent project and time management skills, exposure to scrum or other agile practices in JIRA.  CS fundamentals: You have earned at least a B.S. / MS in Computer Science, or related degree AND you have a strong ethos of continuous learning.",senior big data engineer,"$4,000to$7,000",Monthly,Senior Management,https://www.mycareersfuture.sg/job/senior-big-data-engineer-schellden-global-6b9679d4563c270b58f3a9a89638564a
106,ADVANCE INTELLIGENCE PTE. LTD.,Unknown,Full Time,Information Technology,"Roles & ResponsibilitiesJob Responsibility You will work closely with data scientist team to build the training pipeline and improve the computer vision products such as OCR, facial recognition, facial comparison and liveness.","RequirementsJob Requirements  Minimum Bachelor’s Degree in Computer Science/Information Technology/Engineering. Good experience with Python is a must. Passion in computer vision field with good learning attitude. Team player, meticulous, and strong ownership for work. ",Junior Data Engineer,"$3,600to$5,500",Monthly,Junior Executive,https://www.mycareersfuture.sg/job/junior-data-engineer-advance-intelligence-8326f4dc128c50cfda4e7cd463ff9372
107,SINGAPORE POWER LIMITED,"SP GROUP BUILDING, 2 KALLANG SECTOR 349277",Permanent,Information Technology,"Roles & ResponsibilitiesRoles & Responsibilities Why Work for Us We Power the Nation. Make the most of your talents and develop products that can create impact on a national scale. We are an in-house software team, assembled to move with speed and deliver with quality.   We Build Reliable Solutions. For Customers, Company and Country. You will be part of the Digital Technology Team and together, you will innovate, create, and deploy digital products that will empower more than 3,800 employees within SP Group and improve the quality of life for the 1.5 million commercial, industrial and residential customers that SP Group serves. We build solutions that enable sustainable high quality lifestyles and help consumers save energy and cost, as well as supporting national goals for a sustainable livable city. Now, imagine the impact you can create.   SP Digital Technology aims to use cutting edge technologies to help SP Group to revolutionize future utility/energy industry by providing better services and more efficient energy solutions to our customers. Data charter consists of data engineering, business intelligence, data science/machine learning teams. We oversee and drive all data and AI initiatives for SP group. It includes the following · Build next generation data infrastructure to collect/process/analyze different data from consumers, assets, energy. · Discover the business problems/opportunities and design data-driven solutions to improve operation/business/customer experience. · Uncover the actionable insights for multiple stakeholders to drive business growth   The mission of the data team is to drive SP to become data-driven company and create data-driven products. As a data team member, you will be responsible for designing, developing and deploying data-driven solutions to create business value. We are looking for data engineers/senior data engineers to join the team. You will work together with data scientists, machine learning engineers to build data ingestion pipelines, design data-driven applications to deploy the machine learning models into production environment.   What You'll Do  Selecting and integrating any Big Data tools and frameworks required to provide requested  Implementing ETL process Research opportunities for data acquisition and new uses for existing data Develop data set processes for data modelling, mining and production Recommend ways to improve data reliability, efficiency and quality Collaborate with data architects, modellers and IT team members on project goals ","RequirementsRequirements What You'll Need We are looking for Passion and Proficiency  Good analytical skill with solid software engineering background Experience with both Java and Python Experience working with Hadoop cluster Experience with integration of data from multiple data sources Experience with various messaging systems, such as Kafka ",Data Engineer  /  Senior Data Engineer,"$4,000to$8,000",Monthly,Professional,https://www.mycareersfuture.sg/job/data-engineer-senior-data-engineer-singapore-power-4cd668c50a85ba02abd424a650a92136
108,CARGILL ASIA PACIFIC HOLDINGS PTE. LTD.,"CAPITAGREEN, 138 MARKET STREET 048946",Full Time,Engineering,"Roles & ResponsibilitiesThe Data Engineer within the regional delivery portfolio will work closely with business partners, Global IT, CBS and 3rd party partners to enable the delivery of process, data, and technology solutions.  You will work with the entire Global IT delivery eco-system to understand data, business requirements, create designs, and test solutions to ensure they meet business needs.   As a skilled professional in data and reporting, systems & process design, you will provide solid data and reporting design, system designs, and quality solutions to solve various business requirements.  You will expertly manage low to medium rigor projects or work streams thoughout the entire project lifecycle and  use strong communication skills to deliver effective presentations of the solution detailing data, process, and system design to audiences of all sizes.   To be successful you will leverage common solutions and processes by following Global IT design to deliver solutions and gain knowledge of the solution(s) and business processes for an assigned Regional Delivery Portfolio of either:    Trading ERP Operations Supply Chain (OSC) Functions / Customer Facing    You will have the ability to design solutions by mapping customer business problems and data to reusable end-to-end business application solutions, engage in business decision discussions related to agility, business value, data, and business processes.   You are an individual who is resourceful, confident under pressure,  has demonstrated competence in expectation management, and a passion for the business through professionalism and striving for excellence in all aspects of the business experience.   This position is expected follow  the Cargill Project Delivery Process and Requirements Analysis and Solution Design process framework.    50%        Analysis & Requirements Gathering     Elicit requirements using interviews, document analysis, requirements workshops, surveys, site visits, business process descriptions, use cases, scenarios, business analysis, task and workflow analysis. Elicit functional and non-functional (performance, availability, security, accessibility, cross-browser compliance, data) requirements using a methodology most appropriate for the context of each project, such as Joint Requirements Planning (JRP), Joint Application Development (JAD). Proactively communicate and collaborate with external and internal stakeholders to analyze information needs and functional requirements and deliver the following artifacts as needed: Business Requirements Document, Use Cases, data requirements, Screen, and Interface Designs. Facilitate requirements discussions with key stakeholders Communicate and clarify the requirements to the design and development resources. Assist in translating business requirements into functional design specifications Clarify and improve the business processes and data impacted by the technology changes that are part of assigned  projects. Particiate in the evaluation of system changes for downstream system and/or organizational impacts Plan for acceptance of solution (change management, communication, training needs) Provide subject matter expertise in assigned business process areas    20%        Solution Design    Create solution designs across process, data, and technology that meet business requirements and adhere to relevant standards and principles, leverage common tools and processes, and meet cost/delivery objectives. Consider in the design, the business implications of the application of technology to the current and future business environment. Perform reviews with key stakeholders throughout the design lifecycle to ensure alignment on solution designs and requirements. This may include but is not limited to the following activities: design reviews, testing execution, data validation, deployment verifications, customer satisfaction reviews, etc. Create supporting solution design documentation to ensure sustainment of the solution and business capability, support solution implementation as necessary. Develop requirements specifications according to standard templates, using natural language. Participate with developers and subject matter experts to establish the technical vision and analyze trade-offs between usability and performance needs. Be the liaison between the business, technology teams and support teams. Actively participate in discussions on solution options and business partner decision making to minimize the amount of project investment divergence from target architecture        15%       Business Partnership    Work with businesses to identify and confirm connections between business goals,  strategies,process,  data, and technology investments required to achieve them. Engage with and influence business units on their assumptions of how they will successfully execute their plans Create and maintain strong working relationships with technology teams, functional counterparts, vendors and business partners. Effectively engage stakeholders in change management activities    15%      Project Delivery Responsibilities    Manage workload and priorities to deliver agreed upon project milestones Provides input to staffing plans at the project-level to identify key / required skills Ensure traceability from business requirements through application testing and work with offshore development and QA teams clarifying requirements. Deliver presentations and training of solutions to stakeholders and end users Holds project team resources accountable to their deliverables and ensures project execution. Prepare high and detailed level estimations of effort in order to achieve a preferred solution. Provide guidance and oversight during requirements, design, build, and test phases. ","RequirementsMinimum Required Qualifications    Bachelor’s degree in Business Administration, Computer Science or Management Information Systems; OR equivalent experience 3 years experience in diverse operational, development and/or business roles 3 years experience collaborating with project  teams to define business requirements and deliver solutions that meet business goal 3 years experience in developing integrated solutions involving process, data, and technolog 1 – 3 years expereince using Visualization tools such as Tableau and Power BI 1 – 3 years background in User Experience and User Interface. Ability to travel up to 20%    Preferred Qualifications  3 years experience across various businesses and project types (including custom software development, COTS and SAAS implementation, O&M support). 2 years experience in Use Case diagramming, Business Process Modeling (BPM) and User Stories development Experience working in a business role including mergers and acquisitions Experience with in-house developed and package implementations. Knowledge of and experience with Change Management, industry certification (e.g ITIL, six sigma etc) Knowledge of data access framework, data processing pipeline tools , and or other ETL knowledge. Experience with numerical algorithms, data structures and statistics Experience dashboard creation experience from scratch.  Success Attributes  Strong analytical skills required, including a thorough understanding of how to interpret customer business needs and translate them into application and operational requirements. Ability to influence peers and leadership stakeholders Strong written and verbal communication skills, ability to communicate technical and business information effectively to both technical and non-technical people. Strong project management, planning and organizational skills Ability to quickly comprehend the functions and capabilities of new technologies Demonstrated ability in identifying and developing strategies to address change management issues Business fluency in English ",Data Engineer,"$6,500to$10,000",Monthly,Non-executive,https://www.mycareersfuture.sg/job/data-engineer-cargill-asia-pacific-holdings-26ae01f829b032de5bf383e067d5e8ba
109,MCITS TECHNOLOGIES PTE. LTD.,"TONG BUILDING, 302 ORCHARD ROAD 238862","Contract, Full Time",Information Technology,"Roles & ResponsibilitiesManaged Services Provide 1st and 2nd Level incident/problem management, including ticket logging, problem identification, troubleshooting, resolution and escalation within agreed Service Level Agreement (SLA). Perform health and status monitoring of Systems inlcuding WAN, LAN, Wireless LAN Network using centralise fault and performance monitoring tools. Provide proactive health and status monitoring of server/system, storage, database, IP Telephony, security devices and application using centralised fault and performance monitoring tools. Perform basic server administration tasks including but not limited to creation of user accounts or resetting of passwords. The Individuals must interact with multiple parties and coordinate the recovery of system or network with the relevant vendors or support. Managed and work with agreed Service Level Agreement (SLA) with customers. Participate in UAT to ensure that new equipment/job function(s) are adhered to defined standards and specification. Facility Management Ensure Data Centre is operationally 24x7. Responsible for physical security of the Data Centre as defined in the Corporate Security Policy and standards Ensure Data Centre air conditioning units, lights and temperature are functional and within normal operational standards. Work as part of a team providing coverage on a 7 days a week, 24 hours per day (24x7) basis including public holidays, on a 12 hour rotating shift basis.   Remote Hand Service Provide vendor escorting service when in StarHub secured facilities. Provide basic cabling support like loose cable verification, connecting pre-laid cable – patching, etc. Provide visual checks and verifications of equipment (power cycle, equipment indicators, inspection of equipment, hardware reset). Provide basic checks on environmental conditions in the Data Centre. Provide basic data media loading/unloading to Tape Library using an enterprise backup application. Provide hardware parts replacement of systems (interface card & installation, slotting/removal of blades/line cards, movement of equipment). Assist in the labelling of Tape Media. Keep inventories of the movement of tape media using Tape Management System (TMS). Batch Job Management Ensure Daily Batch jobs for Billing and Revenue Collection are executed correctly and on time using a Enterprise Batch Scheduling tool. Monitor the progress of jobs execution and escalation to Application Support when necessary. Provide 1st Level Incident Management/Troubleshooting and act as point of contact with Banks and Merchants to resolve issues Ensure batch processing is on schedule and monitor all system and batch job status. Respond to all user enquries regarding system functionalities or billing and payment processing.     Tape Backup and Management Ensure daily tape backup are executed as scheduled and completed successfully. Provide 1st Level Incident Management/troubleshooting and escalate to engineering support when necessary. Provide backup jobs status/progress monitoring.  ","RequirementsQualification and Skills Minimum Diploma, preferable in Information Technology discipline, or equivalent required. Data Center Certified Professional , Cisco Certified Network Administrator (CCNA) with minimum 3-5 years hands-on experience in DC operations and/or network administration preferred. Applicant with minimum of 2-3 years Data Centre experience and supporting multi-vendor environment desirable. Experience in automated batch scheduler and enterprise data backup application desirable. Must be detailed oriented, highly organised and able to multi-task in an efficient manner. Knowledge of Sun Solaris, HP UX, MS Windows and Microsoft Office mandatory. Familiair with hardware/software components and terminology. Experience in analysing hardware and software problems and making quick and accurate diagnosis. Minimum of 2 years experience in Managed Services and Smart-Hand Services advantageous. Must be willing to work in a rotating shift and additional hours to support the team as required, including public holidays. Strong ethics, integrity and genuine concern for the needs of others. Must be a team player. Experience in customer support and interaction with corporate customers. Ability to workin a team environment, and also able to work under pressure with minimum supervision. Good initiatives, proactive with good command of spoken and written English.",Data Center Operations Engineer,"$2,500to$4,000",Monthly,Executive,https://www.mycareersfuture.sg/job/data-center-operations-engineer-mcits-technologies-6ae8ced77930ec68b3876e9127e84d5a
110,SOFTENGER (SINGAPORE) PTE. LTD.,"SIM LIM TOWER, 10 JALAN BESAR 208787",Contract,Information Technology,"Roles & Responsibilities Candidate should have 2-3 year of experience working in DC which can be medium to large data center in Singapore, basic knowledge of DC components such as Server, Storage, Network, Cabling (UTP, Fiber) is required. Candidate should know the basic Operations of Data center running environment and process follow with in DC. Follow the Standard Instructions given by SA (via SNOW) or DC Management for activities such as rack & stack, cable patching, server health check, etc. Work with Designated Partner for racking and stacking of servers and all movement of servers will be managed by partner, Operator has to help of rack and stack ono initial stage. Monitor the Data center servers status in terms of physical health check. ( Monitoring only involves for first level of work ) Work with SA for console access and any Citi approved process needs to follow in Data center. Follow the standard operating procedure of Citi DC ops, which are standard across the region. Worked on Service now tickets / Request for any action required for server first level monitoring and engage right Team for next course of action. Hands and legs Support in case of escorting partner or any change worked required in DC. ","RequirementsThe work location is at Novena or Outram Park. No Shiftwork. Office hour with 5 day work week, needed to support 24/7 including weekend when required. With simple unix skill or any shell commands is a plus point. Able to speak Mandarin.",Data Center Engineer  /  Data Centre Operator,"$4,000to$5,000",Monthly,"Executive, Senior Executive",https://www.mycareersfuture.sg/job/data-center-engineer-data-centre-operator-softenger-b536a5b1004cf228970f77c6944bc64a
111,INFINEON TECHNOLOGIES ASIA PACIFIC PTE LTD,"INFINEON, 8 KALLANG SECTOR 349282",Permanent,Engineering,"Roles & Responsibilities Be a member of the Systems team that delivers embedded reference solutions / kits using Infineon semiconductors, microcontrollers Have particular focus to apply artificial intelligence and data technologies and to implement smart connected systems Be responsible for specification, set up and implementation of solution comprising of hardware / circuits and application software, towards target application Deliver on required components to complete a reference solution / demo / kit – including documentation Maintain close cooperation with Marketing, Application teams and Field Application Engineers ","Requirements University degree in Computer Science or Electrical Engineering No prior employment required if you are an active technical hobbyist Coding experience in Python  Experience with embedded system development using C-language Understanding of electronics / electrical circuits, can debug the system Experience in embedded Linux is preferred Microcontroller and standard peripherals know-how is preferred A team self-motivator, demonstrated ability to independently explore and acquire new technologies to enable his project Able to travel occasionally when required ",Application Engineer - Smart Systems with AI & Data Technology,"$3,000to$5,500",Monthly,"Fresh/entry level, Professional, Junior Executive",https://www.mycareersfuture.sg/job/application-engineer-smart-systems-ai-data-technology-infineon-technologies-asia-pacific-843285eddedf6ceef0541d567426f31a
112,PM ASIA PROJECT SERVICES PTE. LTD.,"THE SYNERGY, 1 INTERNATIONAL BUSINESS PARK 609917","Permanent, Contract",Engineering,"Roles & ResponsibilitiesOverview:  Lead the Electrical Design of large scale, high end industrial facilities.  Have knowledge and experience in designing Electrical systems for Data Centres, including Medium Voltage, Low Voltage Power Systems, Emergency Power Systems, ELV’s (CCTV, Interlocks, Fire Alarm, Power Monitoring System) Work in a multi-disciplinary design office environment for global clients. Display a personal commitment to safety, hold safety as a core value and provide safety leadership in the performance of all work activities Be quality focussed, producing well engineered designs to the highest standards in an ISO9000 Quality System environment   ","Requirements Degree in Electrical Engineering, Chartered Engineer preferred At least 6 years’ of experience of relevant experience  Experience within the Data Centre industry is required  ",Senior Electrical Engineer (Data Centre),"$6,500to$9,500",Monthly,Senior Executive,https://www.mycareersfuture.sg/job/senior-electrical-engineer-pm-asia-project-services-a71d75b1956dc4c3194672ba28c7c858
